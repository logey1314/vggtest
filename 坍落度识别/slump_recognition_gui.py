#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åè½åº¦è¯†åˆ«ç³»ç»Ÿ - å¯è§†åŒ–ç•Œé¢
åŠŸèƒ½ï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„æ··å‡åœŸåè½åº¦è‡ªåŠ¨è¯†åˆ«
æ”¯æŒå®æ—¶è¯†åˆ«ã€æ‰¹é‡å¤„ç†ã€ç»“æœåˆ†æç­‰åŠŸèƒ½
"""

# è§£å†³OpenMPå†²çªé—®é¢˜ - å¿…é¡»åœ¨å¯¼å…¥å…¶ä»–åº“ä¹‹å‰è®¾ç½®
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import tkinter as tk
from tkinter import ttk, filedialog, messagebox
import cv2
import numpy as np
import threading
import time
from PIL import Image, ImageTk, ImageDraw, ImageFont
import glob
import logging
from datetime import datetime
import json
import torch
import torch.nn.functional as F
from pathlib import Path
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from matplotlib.figure import Figure
import seaborn as sns

# å¯¼å…¥æ¨¡å‹
from models import create_model, get_available_models

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class SlumpRecognitionGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("åè½åº¦è¯†åˆ«ç³»ç»Ÿ - åŸºäºæ·±åº¦å­¦ä¹ çš„æ™ºèƒ½è¯†åˆ«")
        self.root.geometry("1400x1000")
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # å˜é‡åˆå§‹åŒ–
        self.current_model = None
        self.model_config = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # è¯¦ç»†çš„27ç±»åè½åº¦åˆ†ç±»æ ‡ç­¾
        self.class_names = [
            "ç±»åˆ«1_120-130", "ç±»åˆ«2_131-135", "ç±»åˆ«3_136-140", "ç±»åˆ«4_150-160",
            "ç±»åˆ«5_161-165", "ç±»åˆ«6_166-169", "ç±»åˆ«7_171-174", "ç±»åˆ«8_176-179",
            "ç±»åˆ«9_181-184", "ç±»åˆ«10_185-190", "ç±»åˆ«11_190-195", "ç±»åˆ«12_194-200",
            "ç±»åˆ«13_200-205", "ç±»åˆ«14_205-210", "ç±»åˆ«15_210-215", "ç±»åˆ«16_215-220",
            "ç±»åˆ«17_220-225", "ç±»åˆ«18_225-230", "ç±»åˆ«19_230-235", "ç±»åˆ«20_235-240",
            "ç±»åˆ«21_240-250", "ç±»åˆ«22_250-260", "ç±»åˆ«23_260-265", "ç±»åˆ«24_262-270",
            "ç±»åˆ«25_271-275", "ç±»åˆ«26_276-280", "ç±»åˆ«27_281-285"
        ]
        
        # æ‰¹é‡å¤„ç†ç›¸å…³å˜é‡
        self.input_folder = ""
        self.output_folder = ""
        self.is_processing = False
        
        # æ‰¹é‡å¤„ç†é€‰é¡¹
        self.save_images = tk.BooleanVar(value=False)
        self.save_results = tk.BooleanVar(value=True) 
        self.generate_report = tk.BooleanVar(value=True)
        
        # è§†é¢‘ç›¸å…³å˜é‡
        self.current_video = None
        self.recognition_video = None  # ç‹¬ç«‹çš„è¯†åˆ«è§†é¢‘å¯¹è±¡
        self.current_video_path = ""
        self.video_frame_count = 0
        self.current_frame_index = 0
        self.video_fps = 30
        self.is_video_playing = False
        self.video_thread = None
        self.is_recognizing = False
        self.recognition_thread = None
        self.video_lock = threading.Lock()  # è§†é¢‘è®¿é—®é”
        
        # å®æ—¶è¯†åˆ«ç›¸å…³å˜é‡
        self.realtime_recognition_enabled = tk.BooleanVar(value=False)
        self.recognition_interval = tk.IntVar(value=10)  # æ¯Nå¸§è¯†åˆ«ä¸€æ¬¡
        self.last_recognition_frame = -1
        self.recognition_error_count = 0
        self.max_recognition_errors = 5  # æœ€å¤§è¿ç»­é”™è¯¯æ¬¡æ•°
        
        # è¯†åˆ«ç»“æœ
        self.recognition_results = []
        self.batch_results = {}
        
        # æ˜¾ç¤ºç›¸å…³
        self.canvas_width = 400
        self.canvas_height = 300
        
        # æ¨¡å‹è®¾ç½®
        self.model_name = tk.StringVar(value="resnet18")
        self.confidence_threshold = tk.DoubleVar(value=0.7)
        self.batch_size = tk.IntVar(value=32)
        self.image_size = tk.IntVar(value=224)
        
        # é¢„å¤„ç†è®¾ç½® - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„[-1,1]å½’ä¸€åŒ–
        self.auto_enhance = tk.BooleanVar(value=True)
        self.normalize_enabled = tk.BooleanVar(value=True)  # å¯ç”¨[-1,1]å½’ä¸€åŒ–
        self.resize_enabled = tk.BooleanVar(value=True)
        
        self.setup_ui()
        self.load_config()
        self.init_ui_state()
        
        # ç»‘å®šçª—å£å…³é—­äº‹ä»¶
        self.root.protocol("WM_DELETE_WINDOW", self.on_closing)
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        if not os.path.exists("logs"):
            os.makedirs("logs")
            
        log_filename = f"logs/slump_recognition_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info("åè½åº¦è¯†åˆ«ç³»ç»Ÿå¯åŠ¨")
    
    def init_ui_state(self):
        """åˆå§‹åŒ–UIçŠ¶æ€"""
        # ç¦ç”¨éœ€è¦æ¨¡å‹å’Œè§†é¢‘çš„æ§ä»¶
        self.recognize_btn.config(state=tk.DISABLED)
        self.batch_start_btn.config(state=tk.DISABLED)
        self.play_btn.config(state=tk.DISABLED)
        self.realtime_check.config(state=tk.DISABLED)
        
        # è®¾ç½®åˆå§‹çŠ¶æ€æ–‡æœ¬
        self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šç­‰å¾…æ¨¡å‹å’Œè§†é¢‘åŠ è½½")
        self.model_status_var.set("æœªåŠ è½½æ¨¡å‹")
        self.batch_status_var.set("å‡†å¤‡å°±ç»ª")
        
        self.logger.info("UIçŠ¶æ€åˆå§‹åŒ–å®Œæˆ")
        
    def setup_ui(self):
        """è®¾ç½®ç”¨æˆ·ç•Œé¢"""
        # ä¸»æ¡†æ¶
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # æ ‡é¢˜
        title_label = ttk.Label(main_frame, text="åè½åº¦è¯†åˆ«ç³»ç»Ÿ - æ™ºèƒ½æ··å‡åœŸè´¨é‡æ£€æµ‹", 
                               font=("Arial", 16, "bold"))
        title_label.pack(pady=(0, 15))
        
        # åˆ›å»ºNotebookå¤šæ ‡ç­¾é¡µ
        self.notebook = ttk.Notebook(main_frame)
        self.notebook.pack(fill=tk.BOTH, expand=True)
        
        # æ¨¡å‹é…ç½®æ ‡ç­¾é¡µ
        self.model_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.model_frame, text="ğŸ¤– æ¨¡å‹é…ç½®")
        
        # å®æ—¶è¯†åˆ«æ ‡ç­¾é¡µ
        self.realtime_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.realtime_frame, text="ğŸ¯ å®æ—¶è¯†åˆ«")
        
        # æ‰¹é‡å¤„ç†æ ‡ç­¾é¡µ
        self.batch_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.batch_frame, text="ğŸ“ æ‰¹é‡å¤„ç†")
        
        # ç»“æœåˆ†ææ ‡ç­¾é¡µ
        self.analysis_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.analysis_frame, text="ğŸ“Š ç»“æœåˆ†æ")
        
        # è®¾ç½®å„ä¸ªæ ‡ç­¾é¡µ
        self.setup_model_config()
        self.setup_realtime_recognition()
        self.setup_batch_processing()
        self.setup_result_analysis()
        
    def setup_model_config(self):
        """è®¾ç½®æ¨¡å‹é…ç½®ç•Œé¢"""
        # æ¨¡å‹é€‰æ‹©åŒºåŸŸ
        model_selection_frame = ttk.LabelFrame(self.model_frame, text="æ¨¡å‹é€‰æ‹©ä¸é…ç½®", padding="10")
        model_selection_frame.pack(fill=tk.X, pady=(0, 10))
        
        # æ¨¡å‹ç±»å‹é€‰æ‹©
        ttk.Label(model_selection_frame, text="æ¨¡å‹ç±»å‹:").grid(row=0, column=0, sticky=tk.W, pady=5)
        model_combo = ttk.Combobox(model_selection_frame, textvariable=self.model_name, 
                                  values=['resnet18', 'resnet50', 'vgg16'], state="readonly", width=20)
        model_combo.grid(row=0, column=1, padx=(10, 20), pady=5)
        
        # æ¨¡å‹æƒé‡è·¯å¾„
        ttk.Label(model_selection_frame, text="æ¨¡å‹æƒé‡:").grid(row=0, column=2, sticky=tk.W, pady=5)
        self.weight_path_var = tk.StringVar(value="pretrained/resnet18/best_model.pth")
        ttk.Entry(model_selection_frame, textvariable=self.weight_path_var, width=30).grid(row=0, column=3, padx=(10, 5), pady=5)
        ttk.Button(model_selection_frame, text="æµè§ˆ", command=self.select_model_weights).grid(row=0, column=4, pady=5)
        
        # è®¾å¤‡é€‰æ‹©
        ttk.Label(model_selection_frame, text="è®¡ç®—è®¾å¤‡:").grid(row=1, column=0, sticky=tk.W, pady=5)
        device_label = ttk.Label(model_selection_frame, text=f"å½“å‰: {'GPU' if torch.cuda.is_available() else 'CPU'}")
        device_label.grid(row=1, column=1, sticky=tk.W, padx=(10, 0), pady=5)
        
        # åˆ†ç±»è®¾ç½®
        ttk.Label(model_selection_frame, text="åˆ†ç±»æ ‡ç­¾:").grid(row=1, column=2, sticky=tk.W, pady=5)
        self.class_names_var = tk.StringVar(value=", ".join(self.class_names))
        ttk.Entry(model_selection_frame, textvariable=self.class_names_var, width=30).grid(row=1, column=3, padx=(10, 5), pady=5)
        
        # æ¨¡å‹å‚æ•°è®¾ç½®
        param_frame = ttk.LabelFrame(self.model_frame, text="æ¨ç†å‚æ•°è®¾ç½®", padding="10")
        param_frame.pack(fill=tk.X, pady=(0, 10))
        
        # ç½®ä¿¡åº¦é˜ˆå€¼
        ttk.Label(param_frame, text="ç½®ä¿¡åº¦é˜ˆå€¼:").grid(row=0, column=0, sticky=tk.W, pady=5)
        confidence_scale = ttk.Scale(param_frame, from_=0.1, to=1.0, variable=self.confidence_threshold, 
                                    orient=tk.HORIZONTAL, length=200)
        confidence_scale.grid(row=0, column=1, padx=(10, 5), pady=5)
        ttk.Label(param_frame, textvariable=self.confidence_threshold).grid(row=0, column=2, pady=5)
        
        # æ‰¹å¤„ç†å¤§å°
        ttk.Label(param_frame, text="æ‰¹å¤„ç†å¤§å°:").grid(row=1, column=0, sticky=tk.W, pady=5)
        batch_scale = ttk.Scale(param_frame, from_=1, to=64, variable=self.batch_size, 
                               orient=tk.HORIZONTAL, length=200)
        batch_scale.grid(row=1, column=1, padx=(10, 5), pady=5)
        ttk.Label(param_frame, textvariable=self.batch_size).grid(row=1, column=2, pady=5)
        
        # å›¾åƒå°ºå¯¸
        ttk.Label(param_frame, text="è¾“å…¥å›¾åƒå°ºå¯¸:").grid(row=2, column=0, sticky=tk.W, pady=5)
        size_combo = ttk.Combobox(param_frame, textvariable=self.image_size, 
                                 values=[224, 256, 299, 384], state="readonly", width=10)
        size_combo.grid(row=2, column=1, sticky=tk.W, padx=(10, 0), pady=5)
        
        # é¢„å¤„ç†é€‰é¡¹
        preprocess_frame = ttk.LabelFrame(self.model_frame, text="å›¾åƒé¢„å¤„ç†", padding="10")
        preprocess_frame.pack(fill=tk.X, pady=(0, 10))
        
        ttk.Checkbutton(preprocess_frame, text="è‡ªåŠ¨å¢å¼º", variable=self.auto_enhance).grid(row=0, column=0, sticky=tk.W, padx=(0, 20), pady=5)
        ttk.Checkbutton(preprocess_frame, text="[-1,1]å½’ä¸€åŒ–", variable=self.normalize_enabled).grid(row=0, column=1, sticky=tk.W, padx=(0, 20), pady=5)
        ttk.Checkbutton(preprocess_frame, text="è‡ªåŠ¨è°ƒæ•´å°ºå¯¸", variable=self.resize_enabled).grid(row=0, column=2, sticky=tk.W, pady=5)
        
        # æ·»åŠ é¢„å¤„ç†è¯´æ˜
        ttk.Label(preprocess_frame, text="æ³¨æ„ï¼šå½’ä¸€åŒ–æ–¹å¼å·²åŒ¹é…è®­ç»ƒæ—¶çš„é¢„å¤„ç† (x/127.5-1.0)", 
                 font=("Arial", 8), foreground="blue").grid(row=1, column=0, columnspan=3, sticky=tk.W, pady=(5, 0))
        
        # æ¨¡å‹æ“ä½œæŒ‰é’®
        button_frame = ttk.Frame(self.model_frame)
        button_frame.pack(fill=tk.X, pady=(10, 0))
        
        self.load_model_btn = ttk.Button(button_frame, text="ğŸ”„ åŠ è½½æ¨¡å‹", command=self.load_model)
        self.load_model_btn.pack(side=tk.LEFT, padx=(0, 10))
        
        self.test_model_btn = ttk.Button(button_frame, text="ğŸ§ª æµ‹è¯•æ¨¡å‹", command=self.test_model, state=tk.DISABLED)
        self.test_model_btn.pack(side=tk.LEFT, padx=(0, 10))
        
        self.save_config_btn = ttk.Button(button_frame, text="ğŸ’¾ ä¿å­˜é…ç½®", command=self.save_config)
        self.save_config_btn.pack(side=tk.LEFT)
        
        # æ¨¡å‹çŠ¶æ€æ˜¾ç¤º
        status_frame = ttk.LabelFrame(self.model_frame, text="æ¨¡å‹çŠ¶æ€", padding="10")
        status_frame.pack(fill=tk.X, pady=(10, 0))
        
        self.model_status_var = tk.StringVar(value="æœªåŠ è½½æ¨¡å‹")
        ttk.Label(status_frame, textvariable=self.model_status_var, font=("Arial", 10)).pack(anchor=tk.W)
        
    def setup_realtime_recognition(self):
        """è®¾ç½®å®æ—¶è¯†åˆ«ç•Œé¢"""
        # æ–‡ä»¶é€‰æ‹©åŒºåŸŸ
        file_frame = ttk.LabelFrame(self.realtime_frame, text="è§†é¢‘æ–‡ä»¶é€‰æ‹©", padding="10")
        file_frame.pack(fill=tk.X, pady=(0, 10))
        
        # è§†é¢‘æ–‡ä»¶é€‰æ‹©
        ttk.Label(file_frame, text="é€‰æ‹©è§†é¢‘:").grid(row=0, column=0, sticky=tk.W, pady=5)
        self.video_path_var = tk.StringVar()
        ttk.Entry(file_frame, textvariable=self.video_path_var, width=60).grid(row=0, column=1, padx=(10, 5), pady=5)
        ttk.Button(file_frame, text="æµè§ˆ", command=self.select_video_file).grid(row=0, column=2, pady=5)
        
        # ä¸»æ˜¾ç¤ºåŒºåŸŸ
        display_frame = ttk.Frame(self.realtime_frame)
        display_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 10))
        
        # å·¦ä¾§ï¼šå›¾åƒæ˜¾ç¤º
        left_frame = ttk.LabelFrame(display_frame, text="å›¾åƒæ˜¾ç¤º", padding="10")
        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 5))
        
        # å›¾åƒç”»å¸ƒ
        self.image_canvas = tk.Canvas(left_frame, width=self.canvas_width, height=self.canvas_height, 
                                     bg='lightgray', relief=tk.SUNKEN, bd=2)
        self.image_canvas.pack(pady=(0, 10))
        
        # è§†é¢‘æ§åˆ¶å¯¼èˆª
        nav_frame = ttk.Frame(left_frame)
        nav_frame.pack(fill=tk.X)
        
        # è§†é¢‘æ§åˆ¶æŒ‰é’®
        self.play_btn = ttk.Button(nav_frame, text="â–¶ æ’­æ”¾", command=self.toggle_video_play, state=tk.DISABLED)
        self.play_btn.pack(side=tk.LEFT, padx=(0, 5))
        
        ttk.Button(nav_frame, text="â® å‰10å¸§", command=self.video_prev_10).pack(side=tk.LEFT, padx=(0, 2))
        ttk.Button(nav_frame, text="â­ å10å¸§", command=self.video_next_10).pack(side=tk.LEFT, padx=(2, 5))
        
        self.video_info_var = tk.StringVar(value="æœªé€‰æ‹©è§†é¢‘æ–‡ä»¶")
        ttk.Label(nav_frame, textvariable=self.video_info_var).pack(side=tk.RIGHT)
        
        # å³ä¾§ï¼šè¯†åˆ«ç»“æœ
        right_frame = ttk.LabelFrame(display_frame, text="è¯†åˆ«ç»“æœ", padding="10")
        right_frame.pack(side=tk.RIGHT, fill=tk.Y, padx=(5, 0))
        
        # è¯†åˆ«æ§åˆ¶åŒºåŸŸ
        recognition_control_frame = ttk.LabelFrame(right_frame, text="è¯†åˆ«æ§åˆ¶", padding="5")
        recognition_control_frame.pack(fill=tk.X, pady=(0, 10))
        
        # æ‰‹åŠ¨è¯†åˆ«æŒ‰é’®
        self.recognize_btn = ttk.Button(recognition_control_frame, text="ğŸ¯ è¯†åˆ«å½“å‰å¸§", command=self.recognize_current_image, 
                                       state=tk.DISABLED)
        self.recognize_btn.pack(fill=tk.X, pady=(0, 5))
        
        # å®æ—¶è¯†åˆ«å¼€å…³
        self.realtime_check = ttk.Checkbutton(recognition_control_frame, text="ğŸ”„ å®æ—¶è¯†åˆ«", 
                                             variable=self.realtime_recognition_enabled,
                                             command=self.on_realtime_recognition_toggle)
        self.realtime_check.pack(anchor=tk.W, pady=(0, 5))
        
        # è¯†åˆ«é—´éš”è®¾ç½®
        interval_frame = ttk.Frame(recognition_control_frame)
        interval_frame.pack(fill=tk.X, pady=(0, 5))
        ttk.Label(interval_frame, text="é—´éš”:").pack(side=tk.LEFT)
        interval_combo = ttk.Combobox(interval_frame, textvariable=self.recognition_interval, 
                                     values=[1, 5, 10, 15, 30], state="readonly", width=8)
        interval_combo.pack(side=tk.LEFT, padx=(5, 0))
        ttk.Label(interval_frame, text="å¸§").pack(side=tk.LEFT, padx=(2, 0))
        
        # å®æ—¶è¯†åˆ«çŠ¶æ€æ˜¾ç¤º
        self.realtime_status_var = tk.StringVar(value="å®æ—¶è¯†åˆ«ï¼šæœªå¯ç”¨")
        status_label = ttk.Label(recognition_control_frame, textvariable=self.realtime_status_var, 
                                font=("Arial", 8), foreground="gray")
        status_label.pack(anchor=tk.W)
        
        # ç»“æœæ˜¾ç¤º
        result_display_frame = ttk.Frame(right_frame)
        result_display_frame.pack(fill=tk.BOTH, expand=True)
        
        # é¢„æµ‹ç±»åˆ«
        ttk.Label(result_display_frame, text="é¢„æµ‹åè½åº¦:", font=("Arial", 10, "bold")).pack(anchor=tk.W, pady=(0, 5))
        self.prediction_var = tk.StringVar(value="--")
        prediction_label = ttk.Label(result_display_frame, textvariable=self.prediction_var, 
                                    font=("Arial", 14, "bold"), foreground="blue")
        prediction_label.pack(anchor=tk.W, pady=(0, 10))
        
        # ç½®ä¿¡åº¦
        ttk.Label(result_display_frame, text="ç½®ä¿¡åº¦:", font=("Arial", 10, "bold")).pack(anchor=tk.W, pady=(0, 5))
        self.confidence_var = tk.StringVar(value="--")
        confidence_label = ttk.Label(result_display_frame, textvariable=self.confidence_var, 
                                    font=("Arial", 12), foreground="green")
        confidence_label.pack(anchor=tk.W, pady=(0, 10))
        
        # è¯¦ç»†æ¦‚ç‡
        ttk.Label(result_display_frame, text="å„ç±»åˆ«æ¦‚ç‡:", font=("Arial", 10, "bold")).pack(anchor=tk.W, pady=(0, 5))
        
        # æ¦‚ç‡æ˜¾ç¤ºæ¡†æ¶ - ä½¿ç”¨æ»šåŠ¨æ¡†
        prob_container = ttk.Frame(result_display_frame)
        prob_container.pack(fill=tk.BOTH, expand=True, pady=(0, 10))
        
        # åˆ›å»ºå¸¦æ»šåŠ¨æ¡çš„æ¦‚ç‡æ˜¾ç¤ºåŒºåŸŸ
        self.prob_canvas = tk.Canvas(prob_container, height=150)
        prob_scrollbar = ttk.Scrollbar(prob_container, orient="vertical", command=self.prob_canvas.yview)
        self.prob_frame = ttk.Frame(self.prob_canvas)
        
        self.prob_frame.bind("<Configure>", lambda e: self.prob_canvas.configure(scrollregion=self.prob_canvas.bbox("all")))
        
        self.prob_canvas.create_window((0, 0), window=self.prob_frame, anchor="nw")
        self.prob_canvas.configure(yscrollcommand=prob_scrollbar.set)
        
        self.prob_canvas.pack(side="left", fill="both", expand=True)
        prob_scrollbar.pack(side="right", fill="y")
        
        # å¤„ç†æ—¶é—´
        ttk.Label(result_display_frame, text="å¤„ç†æ—¶é—´:", font=("Arial", 10, "bold")).pack(anchor=tk.W, pady=(0, 5))
        self.process_time_var = tk.StringVar(value="--")
        ttk.Label(result_display_frame, textvariable=self.process_time_var).pack(anchor=tk.W)
        
    def setup_batch_processing(self):
        """è®¾ç½®æ‰¹é‡å¤„ç†ç•Œé¢"""
        # æ‰¹é‡è®¾ç½®åŒºåŸŸ
        batch_settings_frame = ttk.LabelFrame(self.batch_frame, text="æ‰¹é‡è¯†åˆ«è®¾ç½®", padding="10")
        batch_settings_frame.pack(fill=tk.X, pady=(0, 10))
        
        # è¾“å…¥æ–‡ä»¶å¤¹
        ttk.Label(batch_settings_frame, text="è¾“å…¥æ–‡ä»¶å¤¹:").grid(row=0, column=0, sticky=tk.W, pady=5)
        self.batch_input_var = tk.StringVar()
        ttk.Entry(batch_settings_frame, textvariable=self.batch_input_var, width=50).grid(row=0, column=1, padx=(10, 5), pady=5)
        ttk.Button(batch_settings_frame, text="æµè§ˆ", command=self.select_batch_input).grid(row=0, column=2, pady=5)
        
        # è¾“å‡ºæ–‡ä»¶å¤¹
        ttk.Label(batch_settings_frame, text="è¾“å‡ºæ–‡ä»¶å¤¹:").grid(row=1, column=0, sticky=tk.W, pady=5)
        self.batch_output_var = tk.StringVar()
        ttk.Entry(batch_settings_frame, textvariable=self.batch_output_var, width=50).grid(row=1, column=1, padx=(10, 5), pady=5)
        ttk.Button(batch_settings_frame, text="æµè§ˆ", command=self.select_batch_output).grid(row=1, column=2, pady=5)
        
        # å¤„ç†é€‰é¡¹
        options_frame = ttk.LabelFrame(self.batch_frame, text="è¯†åˆ«é€‰é¡¹", padding="10")
        options_frame.pack(fill=tk.X, pady=(0, 10))
        
        # é‡‡æ ·é—´éš”
        ttk.Label(options_frame, text="é‡‡æ ·é—´éš”:").grid(row=0, column=0, sticky=tk.W, pady=5)
        self.sample_interval = tk.IntVar(value=1)
        interval_combo = ttk.Combobox(options_frame, textvariable=self.sample_interval, 
                                     values=[1, 5, 10, 20, 50], state="readonly", width=10)
        interval_combo.grid(row=0, column=1, sticky=tk.W, padx=(10, 20), pady=5)
        ttk.Label(options_frame, text="(æ¯éš”Nå¼ å›¾ç‰‡è¯†åˆ«ä¸€æ¬¡)").grid(row=0, column=2, sticky=tk.W, pady=5)
        
        # ç½®ä¿¡åº¦è¿‡æ»¤
        ttk.Label(options_frame, text="æœ€ä½ç½®ä¿¡åº¦:").grid(row=1, column=0, sticky=tk.W, pady=5)
        self.min_confidence = tk.DoubleVar(value=0.5)
        conf_scale = ttk.Scale(options_frame, from_=0.1, to=1.0, variable=self.min_confidence, 
                              orient=tk.HORIZONTAL, length=150)
        conf_scale.grid(row=1, column=1, padx=(10, 5), pady=5)
        ttk.Label(options_frame, textvariable=self.min_confidence).grid(row=1, column=2, sticky=tk.W, padx=(5, 0), pady=5)
        
        # ä¿å­˜é€‰é¡¹
        save_options_frame = ttk.LabelFrame(options_frame, text="ä¿å­˜é€‰é¡¹", padding="5")
        save_options_frame.grid(row=2, column=0, columnspan=3, sticky=tk.W+tk.E, pady=10)
        
        ttk.Checkbutton(save_options_frame, text="ä¿å­˜æ ‡æ³¨å›¾ç‰‡", variable=self.save_images).grid(row=0, column=0, sticky=tk.W, padx=10)
        ttk.Checkbutton(save_options_frame, text="ä¿å­˜è¯†åˆ«ç»“æœ", variable=self.save_results).grid(row=0, column=1, sticky=tk.W, padx=10)
        ttk.Checkbutton(save_options_frame, text="ç”Ÿæˆåˆ†ææŠ¥å‘Š", variable=self.generate_report).grid(row=0, column=2, sticky=tk.W, padx=10)
        
        # æ§åˆ¶æŒ‰é’®
        control_frame = ttk.Frame(self.batch_frame)
        control_frame.pack(fill=tk.X, pady=(0, 10))
        
        self.batch_start_btn = ttk.Button(control_frame, text="â–¶ å¼€å§‹æ‰¹é‡å¤„ç†", command=self.start_batch_processing, 
                                         state=tk.DISABLED)
        self.batch_start_btn.pack(side=tk.LEFT, padx=(0, 10))
        
        self.batch_stop_btn = ttk.Button(control_frame, text="â¹ åœæ­¢å¤„ç†", command=self.stop_batch_processing, 
                                        state=tk.DISABLED)
        self.batch_stop_btn.pack(side=tk.LEFT, padx=(0, 10))
        
        # è¿›åº¦æ˜¾ç¤º
        progress_frame = ttk.LabelFrame(self.batch_frame, text="å¤„ç†è¿›åº¦", padding="10")
        progress_frame.pack(fill=tk.X, pady=(0, 10))
        
        self.batch_progress_bar = ttk.Progressbar(progress_frame, maximum=100)
        self.batch_progress_bar.pack(fill=tk.X, pady=(0, 5))
        
        self.batch_status_var = tk.StringVar(value="å‡†å¤‡å°±ç»ª")
        ttk.Label(progress_frame, textvariable=self.batch_status_var).pack(anchor=tk.W)
        
        # ç»“æœæ˜¾ç¤º
        result_frame = ttk.LabelFrame(self.batch_frame, text="è¯†åˆ«ç»“æœ", padding="10")
        result_frame.pack(fill=tk.BOTH, expand=True)
        
        # åˆ›å»ºè¡¨æ ¼æ˜¾ç¤ºç»“æœ
        columns = ('æ–‡ä»¶å', 'é¢„æµ‹ç±»åˆ«', 'ç½®ä¿¡åº¦', 'å¤„ç†æ—¶é—´')
        self.result_tree = ttk.Treeview(result_frame, columns=columns, show='headings', height=15)
        
        # è®¾ç½®åˆ—æ ‡é¢˜å’Œå®½åº¦
        self.result_tree.heading('æ–‡ä»¶å', text='æ–‡ä»¶å')
        self.result_tree.heading('é¢„æµ‹ç±»åˆ«', text='é¢„æµ‹ç±»åˆ«')
        self.result_tree.heading('ç½®ä¿¡åº¦', text='ç½®ä¿¡åº¦')
        self.result_tree.heading('å¤„ç†æ—¶é—´', text='å¤„ç†æ—¶é—´(ms)')
        
        self.result_tree.column('æ–‡ä»¶å', width=200)
        self.result_tree.column('é¢„æµ‹ç±»åˆ«', width=150)
        self.result_tree.column('ç½®ä¿¡åº¦', width=100)
        self.result_tree.column('å¤„ç†æ—¶é—´', width=100)
        
        # æ·»åŠ æ»šåŠ¨æ¡
        result_scrollbar = ttk.Scrollbar(result_frame, orient=tk.VERTICAL, command=self.result_tree.yview)
        self.result_tree.configure(yscrollcommand=result_scrollbar.set)
        
        self.result_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        result_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # å³é”®èœå•
        self.result_menu = tk.Menu(self.result_tree, tearoff=0)
        self.result_menu.add_command(label="å¯¼å‡ºé€‰ä¸­ç»“æœ", command=self.export_selected_results)
        self.result_menu.add_command(label="å¯¼å‡ºå…¨éƒ¨ç»“æœ", command=self.export_all_results)
        self.result_menu.add_command(label="æ¸…é™¤ç»“æœ", command=self.clear_batch_results)
        
        self.result_tree.bind("<Button-3>", self.show_result_menu)
        
    def setup_result_analysis(self):
        """è®¾ç½®ç»“æœåˆ†æç•Œé¢"""
        # åˆ†ææ§åˆ¶åŒºåŸŸ
        analysis_control_frame = ttk.LabelFrame(self.analysis_frame, text="åˆ†ææ§åˆ¶", padding="10")
        analysis_control_frame.pack(fill=tk.X, pady=(0, 10))
        
        # æ•°æ®æºé€‰æ‹©
        ttk.Label(analysis_control_frame, text="æ•°æ®æº:").grid(row=0, column=0, sticky=tk.W, pady=5)
        self.data_source_var = tk.StringVar(value="å½“å‰ä¼šè¯")
        data_source_combo = ttk.Combobox(analysis_control_frame, textvariable=self.data_source_var, 
                                        values=["å½“å‰ä¼šè¯", "å¯¼å…¥ç»“æœæ–‡ä»¶"], state="readonly", width=20)
        data_source_combo.grid(row=0, column=1, padx=(10, 20), pady=5)
        
        ttk.Button(analysis_control_frame, text="å¯¼å…¥æ•°æ®", command=self.import_results).grid(row=0, column=2, pady=5)
        ttk.Button(analysis_control_frame, text="ç”ŸæˆæŠ¥å‘Š", command=self.generate_analysis_report).grid(row=0, column=3, padx=(10, 0), pady=5)
        
        # å›¾è¡¨æ˜¾ç¤ºåŒºåŸŸ
        chart_frame = ttk.LabelFrame(self.analysis_frame, text="ç»Ÿè®¡å›¾è¡¨", padding="10")
        chart_frame.pack(fill=tk.BOTH, expand=True)
        
        # åˆ›å»ºmatplotlibå›¾è¡¨
        self.fig = Figure(figsize=(12, 8), dpi=100)
        self.canvas_plot = FigureCanvasTkAgg(self.fig, chart_frame)
        self.canvas_plot.get_tk_widget().pack(fill=tk.BOTH, expand=True)
        
        # åˆå§‹åŒ–ç©ºå›¾è¡¨
        self.update_analysis_chart()
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = "config/recognition_config.json"
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                # åº”ç”¨é…ç½®
                self.model_name.set(config.get('model_name', 'resnet18'))
                self.confidence_threshold.set(config.get('confidence_threshold', 0.7))
                self.batch_size.set(config.get('batch_size', 32))
                self.image_size.set(config.get('image_size', 224))
                self.weight_path_var.set(config.get('weight_path', 'pretrained/resnet18/best_model.pth'))
                
                if 'class_names' in config:
                    self.class_names = config['class_names']
                    self.class_names_var.set(", ".join(self.class_names))
                
                self.logger.info("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
                
            except Exception as e:
                self.logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                messagebox.showerror("é”™è¯¯", f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        
    def save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        if not os.path.exists("config"):
            os.makedirs("config")
            
        config = {
            'model_name': self.model_name.get(),
            'weight_path': self.weight_path_var.get(),
            'confidence_threshold': self.confidence_threshold.get(),
            'batch_size': int(self.batch_size.get()),
            'image_size': int(self.image_size.get()),
            'class_names': [name.strip() for name in self.class_names_var.get().split(',')],
            'auto_enhance': self.auto_enhance.get(),
            'normalize_enabled': self.normalize_enabled.get(),
            'resize_enabled': self.resize_enabled.get()
        }
        
        try:
            with open("config/recognition_config.json", 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            
            self.logger.info("é…ç½®ä¿å­˜æˆåŠŸ")
            messagebox.showinfo("æˆåŠŸ", "é…ç½®ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"é…ç½®ä¿å­˜å¤±è´¥: {e}")
            messagebox.showerror("é”™è¯¯", f"é…ç½®ä¿å­˜å¤±è´¥: {e}")
        
    def select_model_weights(self):
        """é€‰æ‹©æ¨¡å‹æƒé‡æ–‡ä»¶"""
        filetypes = [
            ('PyTorchæ¨¡å‹', '*.pth'),
            ('æ‰€æœ‰æ–‡ä»¶', '*.*')
        ]
        
        weight_path = filedialog.askopenfilename(
            title="é€‰æ‹©æ¨¡å‹æƒé‡æ–‡ä»¶",
            filetypes=filetypes,
            initialdir="pretrained"
        )
        
        if weight_path:
            self.weight_path_var.set(weight_path)
            self.logger.info(f"é€‰æ‹©æƒé‡æ–‡ä»¶: {weight_path}")
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            # æ›´æ–°åˆ†ç±»æ ‡ç­¾
            self.class_names = [name.strip() for name in self.class_names_var.get().split(',')]
            num_classes = len(self.class_names)
            
            # åˆ›å»ºæ¨¡å‹é…ç½®
            self.model_config = {
                'name': self.model_name.get(),
                'pretrained': False,  # å› ä¸ºè¦åŠ è½½è‡ªå®šä¹‰æƒé‡
                'num_classes': num_classes,
                'dropout': 0.5
            }
            
            self.model_status_var.set("æ­£åœ¨åŠ è½½æ¨¡å‹...")
            self.root.update()
            
            # åˆ›å»ºæ¨¡å‹
            self.current_model = create_model(self.model_config)
            
            # åŠ è½½æƒé‡
            weight_path = self.weight_path_var.get()
            if not os.path.exists(weight_path):
                raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_path}")
            
            # åŠ è½½æƒé‡åˆ°æ¨¡å‹
            checkpoint = torch.load(weight_path, map_location=self.device, weights_only=True)
            
            # å¤„ç†ä¸åŒçš„æƒé‡æ–‡ä»¶æ ¼å¼
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                elif 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            self.current_model.load_state_dict(state_dict)
            self.current_model.to(self.device)
            self.current_model.eval()
            
            # æ›´æ–°UIçŠ¶æ€
            self.model_status_var.set(f"æ¨¡å‹åŠ è½½æˆåŠŸ - {self.model_name.get().upper()} ({num_classes}ç±»)")
            self.test_model_btn.config(state=tk.NORMAL)
            self.recognize_btn.config(state=tk.NORMAL)
            self.batch_start_btn.config(state=tk.NORMAL)
            
            # æ›´æ–°å®æ—¶è¯†åˆ«çŠ¶æ€
            if self.current_video:
                self.realtime_check.config(state=tk.NORMAL)
                self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šå‡†å¤‡å°±ç»ª")
            else:
                self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šç­‰å¾…è§†é¢‘åŠ è½½")
            
            self.logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_name.get()}, æƒé‡: {weight_path}")
            messagebox.showinfo("æˆåŠŸ", "æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            
        except Exception as e:
            error_msg = f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}"
            self.model_status_var.set("æ¨¡å‹åŠ è½½å¤±è´¥")
            self.logger.error(error_msg)
            messagebox.showerror("é”™è¯¯", error_msg)
        
    def test_model(self):
        """æµ‹è¯•æ¨¡å‹"""
        if self.current_model is None:
            messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆåŠ è½½æ¨¡å‹")
            return
        
        try:
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            test_input = torch.randn(1, 3, self.image_size.get(), self.image_size.get()).to(self.device)
            
            start_time = time.time()
            with torch.no_grad():
                output = self.current_model(test_input)
                probabilities = F.softmax(output, dim=1)
            
            inference_time = time.time() - start_time
            
            # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
            test_result = (
                f"æ¨¡å‹æµ‹è¯•æˆåŠŸï¼\n"
                f"è¾“å…¥å½¢çŠ¶: {list(test_input.shape)}\n"
                f"è¾“å‡ºå½¢çŠ¶: {list(output.shape)}\n"
                f"æ¨ç†æ—¶é—´: {inference_time:.3f}ç§’\n"
                f"è®¾å¤‡: {self.device}\n"
                f"åˆ†ç±»æ•°é‡: {len(self.class_names)}"
            )
            
            messagebox.showinfo("æ¨¡å‹æµ‹è¯•", test_result)
            self.logger.info(f"æ¨¡å‹æµ‹è¯•æˆåŠŸï¼Œæ¨ç†æ—¶é—´: {inference_time:.3f}ç§’")
            
        except Exception as e:
            error_msg = f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}"
            self.logger.error(error_msg)
            messagebox.showerror("é”™è¯¯", error_msg)
    def preprocess_image(self, image_path):
        """é¢„å¤„ç†å›¾åƒ"""
        try:
            # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼è¯»å–å›¾åƒï¼Œæ”¯æŒä¸­æ–‡è·¯å¾„
            image = self.safe_imread(image_path)
            if image is None:
                raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
            
            # è½¬æ¢ä¸ºRGB
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            original_size = image.shape[:2]
            
            # å›¾åƒå¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.auto_enhance.get():
                image = self.enhance_image(image)
            
            # è°ƒæ•´å°ºå¯¸
            if self.resize_enabled.get():
                target_size = self.image_size.get()
                image = cv2.resize(image, (target_size, target_size))
            
            # è½¬æ¢ä¸ºPIL Image
            pil_image = Image.fromarray(image)
            
            # è½¬æ¢ä¸ºå¼ é‡
            import torchvision.transforms as transforms
            
            # å…ˆè½¬æ¢ä¸ºå¼ é‡
            tensor = transforms.ToTensor()(pil_image)  # [0,1]èŒƒå›´
            
            # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„å½’ä¸€åŒ–æ–¹å¼ï¼š[0,1] â†’ [-1,1]
            if self.normalize_enabled.get():
                # åŒ¹é…è®­ç»ƒæ—¶çš„é¢„å¤„ç†ï¼šx /= 127.5; x -= 1.0
                # ToTensorå·²ç»å°†[0,255] â†’ [0,1]ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦[0,1] â†’ [-1,1]
                tensor = tensor * 2.0 - 1.0  # [0,1] â†’ [-1,1]
            
            tensor = tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            return tensor, pil_image, original_size
            
        except Exception as e:
            self.logger.error(f"å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            raise
    
    def safe_imread(self, image_path):
        """å®‰å…¨è¯»å–å›¾åƒï¼Œæ”¯æŒä¸­æ–‡è·¯å¾„"""
        try:
            # æ–¹æ³•1ï¼šä½¿ç”¨cv2.imdecodeå’Œnumpyæ¥æ”¯æŒä¸­æ–‡è·¯å¾„
            import numpy as np
            
            # æ ‡å‡†åŒ–è·¯å¾„
            image_path = os.path.normpath(image_path)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(image_path):
                self.logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(image_path)
            if file_size == 0:
                self.logger.error(f"æ–‡ä»¶å¤§å°ä¸º0: {image_path}")
                return None
            
            # ä½¿ç”¨numpyè¯»å–æ–‡ä»¶å†…å®¹ï¼Œç„¶åç”¨cv2è§£ç 
            with open(image_path, 'rb') as f:
                img_data = np.frombuffer(f.read(), np.uint8)
            
            # è§£ç å›¾åƒ
            image = cv2.imdecode(img_data, cv2.IMREAD_COLOR)
            
            if image is None:
                # å°è¯•ä½¿ç”¨PILä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
                self.logger.warning(f"OpenCVè¯»å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨PIL: {image_path}")
                return self.safe_imread_pil(image_path)
            
            return image
            
        except Exception as e:
            self.logger.error(f"å›¾åƒè¯»å–å¼‚å¸¸: {e}, è·¯å¾„: {image_path}")
            # å°è¯•ä½¿ç”¨PILä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
            return self.safe_imread_pil(image_path)
    
    def safe_imread_pil(self, image_path):
        """ä½¿ç”¨PILä½œä¸ºå¤‡é€‰çš„å›¾åƒè¯»å–æ–¹æ³•"""
        try:
            from PIL import Image
            import numpy as np
            
            # ä½¿ç”¨PILè¯»å–å›¾åƒ
            pil_image = Image.open(image_path)
            
            # ç¡®ä¿æ˜¯RGBæ¨¡å¼
            if pil_image.mode != 'RGB':
                pil_image = pil_image.convert('RGB')
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œæ³¨æ„PILæ˜¯RGBï¼ŒOpenCVæ˜¯BGR
            image_array = np.array(pil_image)
            
            # è½¬æ¢ä¸ºBGRæ ¼å¼ï¼ˆOpenCVæ ¼å¼ï¼‰
            image_bgr = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
            
            return image_bgr
            
        except Exception as e:
            self.logger.error(f"PILå›¾åƒè¯»å–ä¹Ÿå¤±è´¥: {e}, è·¯å¾„: {image_path}")
            return None
        
    def enhance_image(self, image):
        """å›¾åƒå¢å¼º"""
        try:
            # è½¬æ¢ä¸ºPIL Imageè¿›è¡Œå¢å¼º
            pil_img = Image.fromarray(image)
            
            # å¯¹æ¯”åº¦å¢å¼º
            enhancer = ImageEnhance.Contrast(pil_img)
            pil_img = enhancer.enhance(1.2)
            
            # é”åŒ–å¢å¼º
            enhancer = ImageEnhance.Sharpness(pil_img)
            pil_img = enhancer.enhance(1.1)
            
            # äº®åº¦è°ƒæ•´
            enhancer = ImageEnhance.Brightness(pil_img)
            pil_img = enhancer.enhance(1.05)
            
            return np.array(pil_img)
            
        except Exception as e:
            self.logger.warning(f"å›¾åƒå¢å¼ºå¤±è´¥ï¼Œä½¿ç”¨åŸå›¾åƒ: {e}")
            return image
        
    
    def select_video_file(self):
        """é€‰æ‹©è§†é¢‘æ–‡ä»¶"""
        filetypes = [
            ('è§†é¢‘æ–‡ä»¶', '*.mp4 *.avi *.mov *.mkv *.wmv *.flv'),
            ('MP4', '*.mp4'),
            ('AVI', '*.avi'),
            ('æ‰€æœ‰æ–‡ä»¶', '*.*')
        ]
        
        video_path = filedialog.askopenfilename(
            title="é€‰æ‹©è§†é¢‘æ–‡ä»¶",
            filetypes=filetypes
        )
        
        if video_path:
            self.load_video(video_path)
    
    def load_video(self, video_path):
        """åŠ è½½è§†é¢‘æ–‡ä»¶"""
        try:
            # æ‰“å¼€è§†é¢‘
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                messagebox.showerror("é”™è¯¯", "æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
                return
            
            # é‡ç½®è¯†åˆ«çŠ¶æ€
            self.is_recognizing = False
            self.last_recognition_frame = -1
            self.recognition_error_count = 0
            if hasattr(self, 'recognition_thread') and self.recognition_thread and self.recognition_thread.is_alive():
                # ç­‰å¾…è¯†åˆ«çº¿ç¨‹ç»“æŸ
                self.recognition_thread.join(timeout=1.0)
            
            self.current_video = cap
            self.current_video_path = video_path
            self.video_path_var.set(video_path)
            
            # ä¸ºå®æ—¶è¯†åˆ«åˆ›å»ºç‹¬ç«‹çš„è§†é¢‘å¯¹è±¡
            try:
                self.recognition_video = cv2.VideoCapture(video_path)
                if not self.recognition_video.isOpened():
                    self.logger.warning("æ— æ³•ä¸ºè¯†åˆ«åˆ›å»ºç‹¬ç«‹è§†é¢‘å¯¹è±¡ï¼Œå°†ä½¿ç”¨å…±äº«å¯¹è±¡")
                    self.recognition_video = None
            except Exception as e:
                self.logger.warning(f"åˆ›å»ºè¯†åˆ«è§†é¢‘å¯¹è±¡å¤±è´¥: {e}")
                self.recognition_video = None
            
            # è·å–è§†é¢‘ä¿¡æ¯
            self.video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            self.video_fps = cap.get(cv2.CAP_PROP_FPS)
            self.current_frame_index = 0
            
            # æ˜¾ç¤ºç¬¬ä¸€å¸§
            ret, frame = cap.read()
            if ret:
                self.display_video_frame(frame)
                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # é‡ç½®åˆ°ç¬¬ä¸€å¸§
            
            # æ›´æ–°UIçŠ¶æ€
            self.play_btn.config(state=tk.NORMAL)
            self.recognize_btn.config(state=tk.NORMAL)
            self.recognize_btn.config(text="ğŸ¯ è¯†åˆ«å½“å‰å¸§")
            self.realtime_check.config(state=tk.NORMAL)
            self.video_info_var.set(f"è§†é¢‘: {os.path.basename(video_path)} ({self.video_frame_count}å¸§, {self.video_fps:.1f}fps)")
            
            # é‡ç½®å®æ—¶è¯†åˆ«çŠ¶æ€
            if self.current_model:
                self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šå‡†å¤‡å°±ç»ª")
            else:
                self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šç­‰å¾…æ¨¡å‹åŠ è½½")
            
            self.logger.info(f"åŠ è½½è§†é¢‘: {video_path}, å¸§æ•°: {self.video_frame_count}")
            
        except Exception as e:
            self.logger.error(f"è§†é¢‘åŠ è½½å¤±è´¥: {e}")
            messagebox.showerror("é”™è¯¯", f"è§†é¢‘åŠ è½½å¤±è´¥: {e}")
            
            # æ¸…ç†è§†é¢‘å¯¹è±¡
            if hasattr(self, 'current_video') and self.current_video:
                self.current_video.release()
                self.current_video = None
            if hasattr(self, 'recognition_video') and self.recognition_video:
                self.recognition_video.release()
                self.recognition_video = None
    
    def clear_video_state(self):
        """æ¸…é™¤è§†é¢‘çŠ¶æ€"""
        if self.current_video:
            self.current_video.release()
            self.current_video = None
        
        self.is_video_playing = False
        if self.video_thread and self.video_thread.is_alive():
            self.video_thread = None
        
        self.play_btn.config(state=tk.DISABLED, text="â–¶ æ’­æ”¾")
        self.current_video_path = ""
        self.video_path_var.set("")
        self.video_info_var.set("æœªé€‰æ‹©è§†é¢‘æ–‡ä»¶")
    
    def display_video_frame(self, frame):
        """æ˜¾ç¤ºè§†é¢‘å¸§"""
        try:
            # è½¬æ¢é¢œè‰²ç©ºé—´
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            
            # è°ƒæ•´å›¾ç‰‡å¤§å°é€‚åº”ç”»å¸ƒ
            img_width, img_height = pil_image.size
            scale_w = self.canvas_width / img_width
            scale_h = self.canvas_height / img_height
            scale = min(scale_w, scale_h)
            
            new_width = int(img_width * scale)
            new_height = int(img_height * scale)
            
            display_image = pil_image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            
            # è½¬æ¢ä¸ºTkinterå¯æ˜¾ç¤ºçš„æ ¼å¼
            self.current_image = ImageTk.PhotoImage(display_image)
            
            # æ¸…é™¤ç”»å¸ƒå¹¶æ˜¾ç¤º
            self.image_canvas.delete("all")
            x = (self.canvas_width - new_width) // 2
            y = (self.canvas_height - new_height) // 2
            self.image_canvas.create_image(x, y, anchor=tk.NW, image=self.current_image)
            
            # æ›´æ–°å¸§ä¿¡æ¯
            self.video_info_var.set(f"è§†é¢‘å¸§: {self.current_frame_index}/{self.video_frame_count} "
                                   f"({self.current_frame_index/self.video_fps:.1f}s)")
            
        except Exception as e:
            self.logger.error(f"è§†é¢‘å¸§æ˜¾ç¤ºå¤±è´¥: {e}")
    
    def toggle_video_play(self):
        """åˆ‡æ¢è§†é¢‘æ’­æ”¾çŠ¶æ€"""
        if not self.current_video:
            return
        
        if self.is_video_playing:
            self.stop_video()
        else:
            self.play_video()
    
    def play_video(self):
        """æ’­æ”¾è§†é¢‘"""
        if not self.current_video:
            return
        
        self.is_video_playing = True
        self.play_btn.config(text="â¸ æš‚åœ")
        
        # åœ¨æ–°çº¿ç¨‹ä¸­æ’­æ”¾è§†é¢‘
        self.video_thread = threading.Thread(target=self.video_play_worker)
        self.video_thread.daemon = True
        self.video_thread.start()
    
    def stop_video(self):
        """åœæ­¢è§†é¢‘æ’­æ”¾"""
        self.is_video_playing = False
        self.play_btn.config(text="â–¶ æ’­æ”¾")
        
        # æ›´æ–°å®æ—¶è¯†åˆ«çŠ¶æ€
        if self.realtime_recognition_enabled.get():
            self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šè§†é¢‘å·²æš‚åœ")
    
    def video_play_worker(self):
        """è§†é¢‘æ’­æ”¾å·¥ä½œçº¿ç¨‹"""
        if not self.current_video:
            return
        
        frame_delay = 1.0 / self.video_fps  # æ¯å¸§é—´éš”
        
        while self.is_video_playing and self.current_video:
            # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤è§†é¢‘æ’­æ”¾è¯»å–
            frame = None
            frame_success = False
            
            with self.video_lock:
                try:
                    ret, frame = self.current_video.read()
                    if not ret:
                        # è§†é¢‘ç»“æŸï¼Œé‡ç½®åˆ°å¼€å¤´
                        self.current_video.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        self.current_frame_index = 0
                        self.last_recognition_frame = -1  # é‡ç½®è¯†åˆ«å¸§è®¡æ•°
                        continue
                    
                    self.current_frame_index = int(self.current_video.get(cv2.CAP_PROP_POS_FRAMES)) - 1
                    frame_success = True
                    
                except Exception as e:
                    self.logger.error(f"è§†é¢‘æ’­æ”¾è¯»å–å¸§å¤±è´¥: {e}")
                    continue
            
            if frame_success and frame is not None:
                # åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°ç•Œé¢
                self.root.after(0, lambda f=frame: self.display_video_frame(f))
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å®æ—¶è¯†åˆ«
                if self.should_recognize_current_frame():
                    self.root.after(0, self.trigger_realtime_recognition)
            
            time.sleep(frame_delay)
        
        # æ’­æ”¾ç»“æŸ
        self.root.after(0, self.stop_video)
    
    def should_recognize_current_frame(self):
        """æ£€æŸ¥å½“å‰å¸§æ˜¯å¦éœ€è¦å®æ—¶è¯†åˆ«"""
        # æ£€æŸ¥å®æ—¶è¯†åˆ«æ˜¯å¦å¯ç”¨
        if not self.realtime_recognition_enabled.get():
            self.root.after(0, lambda: self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šæœªå¯ç”¨"))
            return False
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if not self.current_model:
            self.root.after(0, lambda: self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šç­‰å¾…æ¨¡å‹åŠ è½½"))
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨æ‰‹åŠ¨è¯†åˆ«ä¸­
        if self.is_recognizing:
            return False
        
        # æ£€æŸ¥å¸§é—´éš”
        interval = self.recognition_interval.get()
        if self.current_frame_index - self.last_recognition_frame >= interval:
            self.root.after(0, lambda: self.realtime_status_var.set(f"å®æ—¶è¯†åˆ«ï¼šæ¿€æ´»ä¸­ (æ¯{interval}å¸§)"))
            return True
        else:
            remaining = interval - (self.current_frame_index - self.last_recognition_frame)
            self.root.after(0, lambda: self.realtime_status_var.set(f"å®æ—¶è¯†åˆ«ï¼šç­‰å¾…ä¸­ ({remaining}å¸§åè¯†åˆ«)"))
            return False
    
    def on_realtime_recognition_toggle(self):
        """å®æ—¶è¯†åˆ«å¼€å…³å›è°ƒ"""
        if self.realtime_recognition_enabled.get():
            if not self.current_model:
                messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆåŠ è½½æ¨¡å‹åå†å¯ç”¨å®æ—¶è¯†åˆ«")
                self.realtime_recognition_enabled.set(False)
                return
            
            if not self.current_video:
                messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆé€‰æ‹©è§†é¢‘æ–‡ä»¶")
                self.realtime_recognition_enabled.set(False)
                return
                
            self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šå·²å¯ç”¨")
            self.logger.info(f"å®æ—¶è¯†åˆ«å·²å¯ç”¨ï¼Œè¯†åˆ«é—´éš”: {self.recognition_interval.get()}å¸§")
        else:
            self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šå·²å…³é—­")
            self.logger.info("å®æ—¶è¯†åˆ«å·²å…³é—­")
    
    def trigger_realtime_recognition(self):
        """è§¦å‘å®æ—¶è¯†åˆ«ï¼ˆä¸»çº¿ç¨‹å®‰å…¨ï¼‰"""
        try:
            if not self.should_recognize_current_frame():
                return
                
            # æ›´æ–°æœ€åè¯†åˆ«å¸§
            self.last_recognition_frame = self.current_frame_index
            
            # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œè¯†åˆ«ï¼ˆé¿å…é˜»å¡æ’­æ”¾ï¼‰
            recognition_thread = threading.Thread(
                target=self.realtime_recognition_worker, 
                args=(self.current_frame_index,)
            )
            recognition_thread.daemon = True
            recognition_thread.start()
            
        except Exception as e:
            self.logger.error(f"è§¦å‘å®æ—¶è¯†åˆ«å¤±è´¥: {e}")
    
    def realtime_recognition_worker(self, frame_index):
        """å®æ—¶è¯†åˆ«å·¥ä½œçº¿ç¨‹"""
        try:
            start_time = time.time()
            
            # ä¼˜å…ˆä½¿ç”¨ç‹¬ç«‹çš„è¯†åˆ«è§†é¢‘å¯¹è±¡
            video_obj = self.recognition_video if self.recognition_video else self.current_video
            
            if not video_obj:
                self.logger.error("æ²¡æœ‰å¯ç”¨çš„è§†é¢‘å¯¹è±¡è¿›è¡Œè¯†åˆ«")
                return
            
            # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤è§†é¢‘è¯»å–
            frame = None
            with self.video_lock:
                try:
                    video_obj.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
                    ret, frame = video_obj.read()
                    if not ret:
                        self.logger.warning(f"æ— æ³•è¯»å–å¸§ {frame_index}")
                        return
                except Exception as e:
                    self.logger.error(f"è¯»å–è§†é¢‘å¸§å¤±è´¥: {e}")
                    return
            
            # å°†è§†é¢‘å¸§é¢„å¤„ç†ä¸ºæ¨¡å‹è¾“å…¥
            tensor = self.preprocess_video_frame(frame)
            tensor = tensor.to(self.device)
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                output = self.current_model(tensor)
                probabilities = F.softmax(output, dim=1)
                predicted_class = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_class].item()
            
            processing_time = time.time() - start_time
            
            # å‡†å¤‡ç»“æœæ•°æ®
            predicted_label = self.class_names[predicted_class]
            result = {
                'video_path': self.current_video_path,
                'frame_index': frame_index,
                'timestamp_seconds': frame_index / self.video_fps,
                'predicted_class': predicted_class,
                'predicted_label': predicted_label,
                'confidence': confidence,
                'probabilities': probabilities[0].cpu().numpy().tolist(),
                'processing_time': processing_time,
                'timestamp': datetime.now().isoformat(),
                'recognition_type': 'realtime'
            }
            
            # åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°UIï¼ˆä¸é˜»å¡æ’­æ”¾ï¼‰
            self.root.after(0, lambda: self.update_realtime_recognition_results(result))
            
        except Exception as e:
            self.logger.error(f"å®æ—¶è¯†åˆ«å¤±è´¥: {e}")
            self.root.after(0, self.handle_recognition_error)
    
    def update_realtime_recognition_results(self, result):
        """æ›´æ–°å®æ—¶è¯†åˆ«ç»“æœæ˜¾ç¤ºï¼ˆä¸»çº¿ç¨‹å®‰å…¨ï¼‰"""
        try:
            # åªæœ‰å½“å‰æ’­æ”¾çš„å¸§æ‰æ›´æ–°æ˜¾ç¤ºï¼ˆé¿å…å»¶è¿Ÿæ˜¾ç¤ºï¼‰
            if abs(result['frame_index'] - self.current_frame_index) <= 5:
                # æ›´æ–°ç»“æœæ˜¾ç¤º
                self.prediction_var.set(result['predicted_label'])
                self.confidence_var.set(f"{result['confidence']:.2%}")
                self.process_time_var.set(f"{result['processing_time']:.3f} ç§’")
                
                # é‡å»ºæ¦‚ç‡å¼ é‡ä»¥æ›´æ–°æ¦‚ç‡æ˜¾ç¤º
                probabilities_tensor = torch.tensor(result['probabilities'])
                self.update_probability_display(probabilities_tensor)
            
            # ä¿å­˜ç»“æœï¼ˆå§‹ç»ˆä¿å­˜ï¼‰
            self.recognition_results.append(result)
            
            # é‡ç½®é”™è¯¯è®¡æ•°
            self.recognition_error_count = 0
            
            # è®°å½•æ—¥å¿—
            self.logger.info(f"å®æ—¶è¯†åˆ«: å¸§{result['frame_index']} â†’ {result['predicted_label']} ({result['confidence']:.2%})")
            
        except Exception as e:
            self.logger.error(f"æ›´æ–°å®æ—¶è¯†åˆ«ç»“æœå¤±è´¥: {e}")
            self.handle_recognition_error()
    
    def handle_recognition_error(self):
        """å¤„ç†è¯†åˆ«é”™è¯¯"""
        self.recognition_error_count += 1
        
        if self.recognition_error_count >= self.max_recognition_errors:
            # è¿ç»­é”™è¯¯è¿‡å¤šï¼Œè‡ªåŠ¨å…³é—­å®æ—¶è¯†åˆ«
            self.realtime_recognition_enabled.set(False)
            self.realtime_status_var.set("å®æ—¶è¯†åˆ«ï¼šé”™è¯¯è¿‡å¤šå·²è‡ªåŠ¨å…³é—­")
            self.logger.warning(f"å®æ—¶è¯†åˆ«å› è¿ç»­{self.recognition_error_count}æ¬¡é”™è¯¯å·²è‡ªåŠ¨å…³é—­")
            messagebox.showwarning("è­¦å‘Š", f"å®æ—¶è¯†åˆ«å› è¿ç»­é”™è¯¯å·²è‡ªåŠ¨å…³é—­ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æˆ–è§†é¢‘æ–‡ä»¶")
            self.recognition_error_count = 0
        else:
            self.realtime_status_var.set(f"å®æ—¶è¯†åˆ«ï¼šé”™è¯¯ ({self.recognition_error_count}/{self.max_recognition_errors})")
    
    def on_closing(self):
        """ç¨‹åºå…³é—­æ—¶çš„æ¸…ç†å·¥ä½œ"""
        try:
            self.logger.info("ç¨‹åºæ­£åœ¨å…³é—­ï¼Œæ¸…ç†èµ„æº...")
            
            # åœæ­¢è§†é¢‘æ’­æ”¾
            self.is_video_playing = False
            
            # å…³é—­å®æ—¶è¯†åˆ«
            self.realtime_recognition_enabled.set(False)
            
            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            if hasattr(self, 'video_thread') and self.video_thread and self.video_thread.is_alive():
                self.video_thread.join(timeout=2.0)
            
            if hasattr(self, 'recognition_thread') and self.recognition_thread and self.recognition_thread.is_alive():
                self.recognition_thread.join(timeout=2.0)
            
            # é‡Šæ”¾è§†é¢‘èµ„æº
            if hasattr(self, 'current_video') and self.current_video:
                self.current_video.release()
                self.current_video = None
                
            if hasattr(self, 'recognition_video') and self.recognition_video:
                self.recognition_video.release()
                self.recognition_video = None
            
            self.logger.info("èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ç¨‹åºå…³é—­æ¸…ç†å¤±è´¥: {e}")
        finally:
            # å…³é—­ä¸»çª—å£
            self.root.destroy()
    
    def video_prev_10(self):
        """è§†é¢‘å‰è¿›10å¸§"""
        if not self.current_video:
            return
        
        target_frame = max(0, self.current_frame_index - 10)
        self.goto_video_frame(target_frame)
    
    def video_next_10(self):
        """è§†é¢‘åé€€10å¸§"""
        if not self.current_video:
            return
        
        target_frame = min(self.video_frame_count - 1, self.current_frame_index + 10)
        self.goto_video_frame(target_frame)
    
    def goto_video_frame(self, frame_index):
        """è·³è½¬åˆ°æŒ‡å®šå¸§"""
        if not self.current_video:
            return
        
        try:
            # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤è§†é¢‘è·³è½¬
            with self.video_lock:
                self.current_video.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
                ret, frame = self.current_video.read()
                
                if ret:
                    self.current_frame_index = frame_index
                    self.display_video_frame(frame)
                else:
                    self.logger.warning(f"æ— æ³•è·³è½¬åˆ°å¸§ {frame_index}")
                
        except Exception as e:
            self.logger.error(f"è§†é¢‘å¸§è·³è½¬å¤±è´¥: {e}")
        
    def select_image_folder(self):
        """é€‰æ‹©å›¾ç‰‡æ–‡ä»¶å¤¹"""
        folder_path = filedialog.askdirectory(title="é€‰æ‹©å›¾ç‰‡æ–‡ä»¶å¤¹")
        
        if folder_path:
            self.folder_path_var.set(folder_path)
            self.input_folder = folder_path
            
        # æ‰«æå›¾ç‰‡æ–‡ä»¶ï¼Œè¿‡æ»¤æ‰æ— æ³•è¯»å–çš„æ–‡ä»¶
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']
        all_files = []
        
        for ext in image_extensions:
            all_files.extend(glob.glob(os.path.join(folder_path, ext)))
            all_files.extend(glob.glob(os.path.join(folder_path, ext.upper())))
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦å¯è¯»
        self.image_files = []
        invalid_files = []
        
        for file_path in all_files:
            if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                # ç®€å•éªŒè¯ï¼šæ£€æŸ¥æ–‡ä»¶æ‰©å±•åå’Œå¤§å°
                self.image_files.append(file_path)
            else:
                invalid_files.append(file_path)
        
        if invalid_files:
            self.logger.warning(f"å‘ç° {len(invalid_files)} ä¸ªæ— æ•ˆæ–‡ä»¶ï¼Œå·²è·³è¿‡")
        
        self.image_files.sort()
        
        if self.image_files:
            self.current_index = 0
            self.display_current_image()
            self.image_info_var.set(f"æ–‡ä»¶å¤¹: {len(self.image_files)} å¼ å›¾ç‰‡")
            self.logger.info(f"åŠ è½½æ–‡ä»¶å¤¹: {folder_path}, æ‰¾åˆ° {len(self.image_files)} å¼ å›¾ç‰‡")
        else:
            messagebox.showwarning("è­¦å‘Š", "è¯¥æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        
    def display_image(self, image_path):
        """æ˜¾ç¤ºå›¾ç‰‡"""
        try:
            # ä½¿ç”¨å®‰å…¨çš„æ–¹å¼è¯»å–å›¾ç‰‡
            image = self.safe_imread(image_path)
            if image is None:
                self.logger.error(f"æ— æ³•æ˜¾ç¤ºå›¾ç‰‡: {image_path}")
                # åœ¨ç”»å¸ƒä¸Šæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                self.image_canvas.delete("all")
                self.image_canvas.create_text(
                    self.canvas_width//2, self.canvas_height//2,
                    text="å›¾ç‰‡è¯»å–å¤±è´¥\nè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå®Œæ•´æ€§",
                    fill="red", font=("Arial", 12), anchor="center"
                )
                return
            
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(image)
            
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            img_width, img_height = pil_image.size
            scale_w = self.canvas_width / img_width
            scale_h = self.canvas_height / img_height
            scale = min(scale_w, scale_h)
            
            new_width = int(img_width * scale)
            new_height = int(img_height * scale)
            
            # è°ƒæ•´å›¾ç‰‡å¤§å°
            display_image = pil_image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            
            # è½¬æ¢ä¸ºTkinterå¯æ˜¾ç¤ºçš„æ ¼å¼
            self.current_image = ImageTk.PhotoImage(display_image)
            
            # æ¸…é™¤ç”»å¸ƒå¹¶æ˜¾ç¤ºå›¾ç‰‡
            self.image_canvas.delete("all")
            
            # è®¡ç®—å±…ä¸­ä½ç½®
            x = (self.canvas_width - new_width) // 2
            y = (self.canvas_height - new_height) // 2
            
            self.image_canvas.create_image(x, y, anchor=tk.NW, image=self.current_image)
            
        except Exception as e:
            self.logger.error(f"å›¾ç‰‡æ˜¾ç¤ºå¤±è´¥: {e}")
    
    def display_current_image(self):
        """æ˜¾ç¤ºå½“å‰ç´¢å¼•çš„å›¾ç‰‡"""
        if self.image_files and 0 <= self.current_index < len(self.image_files):
            image_path = self.image_files[self.current_index]
            self.current_image_path = image_path
            self.display_image(image_path)
            
            # æ›´æ–°ä¿¡æ¯æ˜¾ç¤º
            filename = os.path.basename(image_path)
            self.image_info_var.set(f"{self.current_index + 1}/{len(self.image_files)}: {filename}")
        
    def prev_image(self):
        """ä¸Šä¸€å¼ å›¾ç‰‡"""
        if self.image_files and self.current_index > 0:
            self.current_index -= 1
            self.display_current_image()
        
    def next_image(self):
        """ä¸‹ä¸€å¼ å›¾ç‰‡"""
        if self.image_files and self.current_index < len(self.image_files) - 1:
            self.current_index += 1
            self.display_current_image()
        
    def recognize_current_image(self):
        """è¯†åˆ«å½“å‰è§†é¢‘å¸§"""
        if self.current_model is None:
            messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆåŠ è½½æ¨¡å‹")
            return
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¯†åˆ«ä¸­
        if self.is_recognizing:
            messagebox.showinfo("æç¤º", "æ­£åœ¨è¯†åˆ«ä¸­ï¼Œè¯·ç¨å€™...")
            return
            
        if self.current_video:
            self.start_frame_recognition()
        else:
            messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆé€‰æ‹©è§†é¢‘æ–‡ä»¶")
            return
    
    def start_frame_recognition(self):
        """å¯åŠ¨å¸§è¯†åˆ«ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰"""
        if self.is_recognizing:
            return
            
        # è®¾ç½®è¯†åˆ«çŠ¶æ€
        self.is_recognizing = True
        self.recognize_btn.config(state=tk.DISABLED)
        self.recognize_btn.config(text="è¯†åˆ«ä¸­...")
        
        # è·å–å½“å‰å¸§æ•°æ®
        if not self.current_video:
            self.reset_recognition_ui()
            return
            
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œè¯†åˆ«
        self.recognition_thread = threading.Thread(target=self.frame_recognition_worker)
        self.recognition_thread.daemon = True
        self.recognition_thread.start()
    
    def frame_recognition_worker(self):
        """å¸§è¯†åˆ«å·¥ä½œçº¿ç¨‹"""
        try:
            start_time = time.time()
            
            # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤è§†é¢‘è¯»å–
            frame = None
            with self.video_lock:
                try:
                    self.current_video.set(cv2.CAP_PROP_POS_FRAMES, self.current_frame_index)
                    ret, frame = self.current_video.read()
                    if not ret:
                        self.root.after(0, lambda: messagebox.showerror("é”™è¯¯", "æ— æ³•è¯»å–è§†é¢‘å¸§"))
                        return
                except Exception as e:
                    self.root.after(0, lambda: messagebox.showerror("é”™è¯¯", f"è¯»å–è§†é¢‘å¸§å¤±è´¥: {e}"))
                    return
            
            # å°†è§†é¢‘å¸§é¢„å¤„ç†ä¸ºæ¨¡å‹è¾“å…¥
            tensor = self.preprocess_video_frame(frame)
            tensor = tensor.to(self.device)
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                output = self.current_model(tensor)
                probabilities = F.softmax(output, dim=1)
                predicted_class = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_class].item()
            
            processing_time = time.time() - start_time
            
            # å‡†å¤‡ç»“æœæ•°æ®
            predicted_label = self.class_names[predicted_class]
            result = {
                'video_path': self.current_video_path,
                'frame_index': self.current_frame_index,
                'timestamp_seconds': self.current_frame_index / self.video_fps,
                'predicted_class': predicted_class,
                'predicted_label': predicted_label,
                'confidence': confidence,
                'probabilities': probabilities[0].cpu().numpy().tolist(),
                'processing_time': processing_time,
                'timestamp': datetime.now().isoformat(),
                'recognition_type': 'manual'
            }
            
            # åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°UI
            self.root.after(0, lambda: self.update_recognition_results(result, processing_time))
            
        except Exception as e:
            self.logger.error(f"è§†é¢‘å¸§è¯†åˆ«å¤±è´¥: {e}")
            self.root.after(0, lambda: messagebox.showerror("é”™è¯¯", f"è¯†åˆ«å¤±è´¥: {e}"))
        finally:
            # é‡ç½®UIçŠ¶æ€
            self.root.after(0, self.reset_recognition_ui)
    
    def update_recognition_results(self, result, processing_time):
        """æ›´æ–°è¯†åˆ«ç»“æœæ˜¾ç¤ºï¼ˆåœ¨ä¸»çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰"""
        try:
            # æ›´æ–°ç»“æœæ˜¾ç¤º
            self.prediction_var.set(result['predicted_label'])
            self.confidence_var.set(f"{result['confidence']:.2%}")
            self.process_time_var.set(f"{processing_time:.3f} ç§’")
            
            # é‡å»ºæ¦‚ç‡å¼ é‡ä»¥æ›´æ–°æ¦‚ç‡æ˜¾ç¤º
            probabilities_tensor = torch.tensor(result['probabilities'])
            self.update_probability_display(probabilities_tensor)
            
            # ä¿å­˜ç»“æœ
            self.recognition_results.append(result)
            self.logger.info(f"è§†é¢‘å¸§è¯†åˆ«å®Œæˆ: å¸§{result['frame_index']} â†’ {result['predicted_label']} ({result['confidence']:.2%})")
            
        except Exception as e:
            self.logger.error(f"æ›´æ–°è¯†åˆ«ç»“æœå¤±è´¥: {e}")
    
    def reset_recognition_ui(self):
        """é‡ç½®è¯†åˆ«UIçŠ¶æ€"""
        self.is_recognizing = False
        self.recognize_btn.config(state=tk.NORMAL)
        self.recognize_btn.config(text="ğŸ¯ å¼€å§‹è¯†åˆ«")
    
    def preprocess_video_frame(self, frame):
        """é¢„å¤„ç†è§†é¢‘å¸§"""
        try:
            # è½¬æ¢ä¸ºRGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # å›¾åƒå¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.auto_enhance.get():
                frame_rgb = self.enhance_image(frame_rgb)
            
            # è°ƒæ•´å°ºå¯¸
            if self.resize_enabled.get():
                target_size = self.image_size.get()
                frame_rgb = cv2.resize(frame_rgb, (target_size, target_size))
            
            # è½¬æ¢ä¸ºPIL Image
            pil_image = Image.fromarray(frame_rgb)
            
            # è½¬æ¢ä¸ºå¼ é‡
            import torchvision.transforms as transforms
            
            # å…ˆè½¬æ¢ä¸ºå¼ é‡
            tensor = transforms.ToTensor()(pil_image)  # [0,1]èŒƒå›´
            
            # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„å½’ä¸€åŒ–æ–¹å¼ï¼š[0,1] â†’ [-1,1]
            if self.normalize_enabled.get():
                tensor = tensor * 2.0 - 1.0  # [0,1] â†’ [-1,1]
            
            tensor = tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            return tensor
            
        except Exception as e:
            self.logger.error(f"è§†é¢‘å¸§é¢„å¤„ç†å¤±è´¥: {e}")
            raise
    
    def update_probability_display(self, probabilities):
        """æ›´æ–°æ¦‚ç‡æ˜¾ç¤º"""
        # æ¸…é™¤ä¹‹å‰çš„æ˜¾ç¤º
        for widget in self.prob_frame.winfo_children():
            widget.destroy()
        
        # æ˜¾ç¤ºå„ç±»åˆ«æ¦‚ç‡ - æŒ‰æ¦‚ç‡æ’åºæ˜¾ç¤ºå‰10ä¸ª
        probs = probabilities.cpu().numpy()
        prob_pairs = list(zip(self.class_names, probs))
        prob_pairs.sort(key=lambda x: x[1], reverse=True)
        
        # æ˜¾ç¤ºå‰10ä¸ªæœ€é«˜æ¦‚ç‡çš„ç±»åˆ«
        top_n = min(10, len(prob_pairs))
        
        for i, (class_name, prob) in enumerate(prob_pairs[:top_n]):
            # åˆ›å»ºè¿›åº¦æ¡æ˜¾ç¤ºæ¦‚ç‡
            frame = ttk.Frame(self.prob_frame)
            frame.pack(fill=tk.X, pady=1)
            
            # ç®€åŒ–ç±»åˆ«åæ˜¾ç¤ºï¼ˆåªæ˜¾ç¤ºç±»åˆ«å·å’ŒèŒƒå›´ï¼‰
            short_name = class_name.replace("ç±»åˆ«", "").replace("_", " ")
            label = ttk.Label(frame, text=f"{short_name}:", width=12, font=("Arial", 8))
            label.pack(side=tk.LEFT)
            
            prob_var = tk.DoubleVar(value=prob * 100)
            prob_bar = ttk.Progressbar(frame, variable=prob_var, maximum=100, length=80)
            prob_bar.pack(side=tk.LEFT, padx=(5, 5))
            
            # æ ¹æ®æ¦‚ç‡å€¼è®¾ç½®é¢œè‰²
            if prob > 0.5:
                color = "green"
            elif prob > 0.1:
                color = "orange"
            else:
                color = "gray"
            
            prob_label = ttk.Label(frame, text=f"{prob:.1%}", foreground=color, font=("Arial", 8))
            prob_label.pack(side=tk.RIGHT)
        
        # å¦‚æœæœ‰æ›´å¤šç±»åˆ«ï¼Œæ˜¾ç¤ºæç¤º
        if len(prob_pairs) > top_n:
            more_frame = ttk.Frame(self.prob_frame)
            more_frame.pack(fill=tk.X, pady=2)
            ttk.Label(more_frame, text=f"... è¿˜æœ‰ {len(prob_pairs) - top_n} ä¸ªç±»åˆ«", 
                     font=("Arial", 8), foreground="gray").pack()
        
        # æ›´æ–°æ»šåŠ¨åŒºåŸŸ
        self.prob_canvas.update_idletasks()
        self.prob_canvas.configure(scrollregion=self.prob_canvas.bbox("all"))
        
    def select_batch_input(self):
        """é€‰æ‹©æ‰¹é‡å¤„ç†è¾“å…¥æ–‡ä»¶å¤¹"""
        folder_path = filedialog.askdirectory(title="é€‰æ‹©è¾“å…¥æ–‡ä»¶å¤¹")
        if folder_path:
            self.batch_input_var.set(folder_path)
            self.input_folder = folder_path
            
            # æ‰«ææ–‡ä»¶å¤¹ä¸­çš„å›¾ç‰‡æ•°é‡
            image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')
            image_count = 0
            
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    if file.lower().endswith(image_extensions):
                        image_count += 1
            
            self.batch_status_var.set(f"æ‰¾åˆ° {image_count} å¼ å›¾ç‰‡ï¼Œå‡†å¤‡è¯†åˆ«")
            self.logger.info(f"é€‰æ‹©è¾“å…¥æ–‡ä»¶å¤¹: {folder_path}, æ‰¾åˆ° {image_count} å¼ å›¾ç‰‡")

    def select_batch_output(self):
        """é€‰æ‹©æ‰¹é‡å¤„ç†è¾“å‡ºæ–‡ä»¶å¤¹"""
        folder_path = filedialog.askdirectory(title="é€‰æ‹©è¾“å‡ºæ–‡ä»¶å¤¹")
        if folder_path:
            self.batch_output_var.set(folder_path)
            self.output_folder = folder_path
            self.logger.info(f"é€‰æ‹©è¾“å‡ºæ–‡ä»¶å¤¹: {folder_path}")
        
    def log_message(self, message):
        """åœ¨æ—¥å¿—æ¡†ä¸­æ·»åŠ æ¶ˆæ¯"""
        timestamp = datetime.now().strftime('%H:%M:%S')
        log_entry = f"[{timestamp}] {message}"
        
        # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        self.logger.info(message)
        
        # æ›´æ–°çŠ¶æ€æ˜¾ç¤º
        self.batch_status_var.set(log_entry)
        self.root.update()
        
    def start_batch_processing(self):
        """å¼€å§‹æ‰¹é‡å¤„ç†"""
        if self.current_model is None:
            messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆåŠ è½½æ¨¡å‹")
            return
        
        if not self.input_folder:
            messagebox.showwarning("è­¦å‘Š", "è¯·é€‰æ‹©è¾“å…¥æ–‡ä»¶å¤¹")
            return
            
        # è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœç”¨æˆ·æœªé€‰æ‹©ï¼‰
        if not self.output_folder:
            self.output_folder = os.path.join(self.input_folder, "recognition_output")
            self.batch_output_var.set(self.output_folder)
        
        # æ¸…é™¤ä¹‹å‰çš„ç»“æœ
        self.clear_batch_results()
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œæ‰¹é‡å¤„ç†
        self.is_processing = True
        self.batch_start_btn.config(state=tk.DISABLED)
        self.batch_stop_btn.config(state=tk.NORMAL)
        
        self.batch_thread = threading.Thread(target=self.batch_processing_worker)
        self.batch_thread.daemon = True
        self.batch_thread.start()
        
    def batch_processing_worker(self):
        """æ‰¹é‡å¤„ç†å·¥ä½œçº¿ç¨‹"""
        try:
            # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
            image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')
            image_files = []
            
            for root, dirs, files in os.walk(self.input_folder):
                for file in files:
                    if file.lower().endswith(image_extensions):
                        file_path = os.path.join(root, file)
                        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                            image_files.append(file_path)
            
            # æŒ‰é‡‡æ ·é—´éš”ç­›é€‰æ–‡ä»¶
            sample_interval = self.sample_interval.get()
            if sample_interval > 1:
                image_files = image_files[::sample_interval]
            
            total_files = len(image_files)
            if total_files == 0:
                self.root.after(0, lambda: messagebox.showwarning("è­¦å‘Š", "è¾“å…¥æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶"))
                return
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
            os.makedirs(self.output_folder, exist_ok=True)
            
            # æ‰¹é‡å¤„ç†
            batch_results = []
            batch_size = int(self.batch_size.get())
            total_images = len(image_files)
            
            for i in range(0, total_images, batch_size):
                if not self.is_processing:  # æ£€æŸ¥æ˜¯å¦åœæ­¢
                    break
                
                batch_files = image_files[i:i + batch_size]
                batch_tensors = []
                batch_info = []
                
                # é¢„å¤„ç†æ‰¹æ¬¡å›¾åƒ
                for image_path in batch_files:
                    try:
                        tensor, pil_image, original_size = self.preprocess_image(image_path)
                        batch_tensors.append(tensor)
                        batch_info.append({
                            'path': image_path,
                            'size': original_size,
                            'filename': os.path.basename(image_path),
                            'start_time': time.time()
                        })
                    except Exception as e:
                        self.log_message(f"é¢„å¤„ç†å¤±è´¥: {os.path.basename(image_path)} - {e}")
                        continue
                
                if not batch_tensors:
                    continue
                
                # æ‰¹é‡æ¨ç†
                try:
                    batch_tensor = torch.cat(batch_tensors, dim=0).to(self.device)
                    
                    with torch.no_grad():
                        outputs = self.current_model(batch_tensor)
                        probabilities = F.softmax(outputs, dim=1)
                        predicted_classes = torch.argmax(probabilities, dim=1)
                        confidences = torch.max(probabilities, dim=1)[0]
                    
                    # å¤„ç†ç»“æœ
                    for j, info in enumerate(batch_info):
                        pred_class = predicted_classes[j].item()
                        confidence = confidences[j].item()
                        pred_label = self.class_names[pred_class]
                        
                        result = {
                            'image_path': info['path'],
                            'filename': info['filename'],
                            'predicted_class': pred_class,
                            'predicted_label': pred_label,
                            'confidence': confidence,
                            'probabilities': probabilities[j].cpu().numpy().tolist(),
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        batch_results.append(result)
                        
                        # æ›´æ–°è¿›åº¦
                        progress = ((i + j + 1) / total_images) * 100
                        self.root.after(0, lambda p=progress: self.batch_progress_bar.configure(value=p))
                        
                        status_msg = f"å¤„ç†ä¸­... {i + j + 1}/{total_images} - {pred_label} ({confidence:.2%})"
                        self.root.after(0, lambda msg=status_msg: self.batch_status_var.set(msg))
                        
                        self.log_message(f"å·²å¤„ç†: {info['filename']} â†’ {pred_label} ({confidence:.2%})")
                        
                        # è®¡ç®—å¤„ç†æ—¶é—´
                        processing_time = (time.time() - info['start_time']) * 1000  # æ¯«ç§’
                        
                        # æ›´æ–°GUIç»“æœè¡¨æ ¼ (éœ€è¦åœ¨ä¸»çº¿ç¨‹ä¸­æ‰§è¡Œ)
                        self.root.after(0, lambda r=result, pt=processing_time: self.add_result_to_tree(r, pt))
                        
                        # ä¿å­˜æ ‡æ³¨å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if self.save_images.get():
                            self.save_annotated_image(info['path'], pred_label, confidence, self.output_folder)
                    
                except Exception as e:
                    self.log_message(f"æ‰¹é‡æ¨ç†å¤±è´¥: {e}")
                    continue
            
            # ä¿å­˜ç»“æœ
            if batch_results and self.save_results.get():
                self.save_batch_results(batch_results, self.output_folder)
            
            # ç”ŸæˆæŠ¥å‘Š
            if batch_results and self.generate_report.get():
                self.generate_batch_report(batch_results, self.output_folder)
            
            self.batch_results.update({datetime.now().isoformat(): batch_results})
            self.log_message(f"æ‰¹é‡å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(batch_results)} å¼ å›¾ç‰‡")
            
            # æ›´æ–°åˆ†æå›¾è¡¨
            self.root.after(0, self.update_analysis_chart)
            
        except Exception as e:
            self.log_message(f"æ‰¹é‡å¤„ç†é”™è¯¯: {e}")
            self.logger.error(f"æ‰¹é‡å¤„ç†é”™è¯¯: {e}")
            
        finally:
            # é‡ç½®UIçŠ¶æ€
            self.root.after(0, self.reset_batch_ui)
    
    def add_batch_result_to_table(self, filename, predicted_label, confidence, processing_time):
        """æ·»åŠ æ‰¹é‡å¤„ç†ç»“æœåˆ°è¡¨æ ¼"""
        self.result_tree.insert('', 'end', values=(
            filename,
            predicted_label,
            f"{confidence:.2%}",
            f"{processing_time:.1f}"
        ))
        
        # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
        children = self.result_tree.get_children()
        if children:
            self.result_tree.see(children[-1])
    
    def clear_batch_results(self):
        """æ¸…é™¤æ‰¹é‡å¤„ç†ç»“æœ"""
        for item in self.result_tree.get_children():
            self.result_tree.delete(item)
        
        self.batch_status_var.set("å‡†å¤‡å°±ç»ª")
        self.batch_progress_bar.configure(value=0)
    
    def show_result_menu(self, event):
        """æ˜¾ç¤ºç»“æœå³é”®èœå•"""
        try:
            self.result_menu.tk_popup(event.x_root, event.y_root)
        finally:
            self.result_menu.grab_release()
    
    def export_selected_results(self):
        """å¯¼å‡ºé€‰ä¸­çš„ç»“æœ"""
        selected_items = self.result_tree.selection()
        if not selected_items:
            messagebox.showwarning("è­¦å‘Š", "è¯·å…ˆé€‰æ‹©è¦å¯¼å‡ºçš„ç»“æœ")
            return
        
        self.export_results(selected_items)
    
    def export_all_results(self):
        """å¯¼å‡ºå…¨éƒ¨ç»“æœ"""
        all_items = self.result_tree.get_children()
        if not all_items:
            messagebox.showwarning("è­¦å‘Š", "æ²¡æœ‰ç»“æœå¯å¯¼å‡º")
            return
        
        self.export_results(all_items)
    
    def export_results(self, items):
        """å¯¼å‡ºç»“æœåˆ°CSVæ–‡ä»¶"""
        try:
            import csv
            
            # é€‰æ‹©ä¿å­˜æ–‡ä»¶
            file_path = filedialog.asksaveasfilename(
                title="ä¿å­˜è¯†åˆ«ç»“æœ",
                defaultextension=".csv",
                filetypes=[("CSVæ–‡ä»¶", "*.csv"), ("æ‰€æœ‰æ–‡ä»¶", "*.*")]
            )
            
            if not file_path:
                return
            
            # æ”¶é›†æ•°æ®
            data = []
            for item in items:
                values = self.result_tree.item(item, 'values')
                data.append(values)
            
            # å†™å…¥CSV
            with open(file_path, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(['æ–‡ä»¶å', 'é¢„æµ‹ç±»åˆ«', 'ç½®ä¿¡åº¦', 'å¤„ç†æ—¶é—´(ms)'])
                writer.writerows(data)
            
            messagebox.showinfo("æˆåŠŸ", f"ç»“æœå·²å¯¼å‡ºåˆ°: {file_path}")
            self.logger.info(f"æ‰¹é‡ç»“æœå¯¼å‡º: {file_path}")
            
        except Exception as e:
            error_msg = f"å¯¼å‡ºå¤±è´¥: {e}"
            messagebox.showerror("é”™è¯¯", error_msg)
            self.logger.error(error_msg)
    
    def reset_batch_ui(self):
        """é‡ç½®æ‰¹é‡å¤„ç†UIçŠ¶æ€"""
        self.is_processing = False
        self.batch_start_btn.config(state=tk.NORMAL)
        self.batch_stop_btn.config(state=tk.DISABLED)
    
    def add_result_to_tree(self, result, processing_time):
        """å°†è¯†åˆ«ç»“æœæ·»åŠ åˆ°GUIè¡¨æ ¼ä¸­"""
        try:
            # æ·»åŠ åˆ°ç»“æœè¡¨æ ¼
            item_id = self.result_tree.insert('', 'end', values=(
                result['filename'],
                result['predicted_label'],
                f"{result['confidence']:.2%}",
                f"{processing_time:.1f}"
            ))
            
            # æ»šåŠ¨åˆ°æœ€æ–°ç»“æœ
            self.result_tree.see(item_id)
            
        except Exception as e:
            self.logger.warning(f"æ·»åŠ ç»“æœåˆ°è¡¨æ ¼å¤±è´¥: {e}")
    
    def save_annotated_image(self, image_path, predicted_label, confidence, output_folder):
        """ä¿å­˜æ ‡æ³¨å›¾ç‰‡"""
        try:
            # è¯»å–åŸå›¾
            image = cv2.imread(image_path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(image)
            
            # åˆ›å»ºç»˜å›¾å¯¹è±¡
            draw = ImageDraw.Draw(pil_image)
            
            # è®¾ç½®å­—ä½“ï¼ˆå°è¯•ä¸åŒå­—ä½“ï¼‰
            try:
                font = ImageFont.truetype("arial.ttf", 40)
            except:
                font = ImageFont.load_default()
            
            # ç»˜åˆ¶æ–‡æœ¬
            text = f"{predicted_label}\n{confidence:.1%}"
            
            # è®¡ç®—æ–‡æœ¬ä½ç½®
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            x = 10
            y = 10
            
            # ç»˜åˆ¶èƒŒæ™¯çŸ©å½¢
            draw.rectangle([x-5, y-5, x+text_width+10, y+text_height+10], 
                          fill=(0, 0, 0, 128), outline=(255, 255, 255))
            
            # ç»˜åˆ¶æ–‡æœ¬
            draw.text((x, y), text, font=font, fill=(255, 255, 255))
            
            # ä¿å­˜å›¾ç‰‡
            filename = os.path.basename(image_path)
            name, ext = os.path.splitext(filename)
            output_path = os.path.join(output_folder, f"{name}_annotated{ext}")
            
            pil_image.save(output_path)
            
        except Exception as e:
            self.logger.warning(f"ä¿å­˜æ ‡æ³¨å›¾ç‰‡å¤±è´¥: {e}")
    
    def save_batch_results(self, results, output_folder):
        """ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = os.path.join(output_folder, f"recognition_results_{timestamp}.json")
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            self.log_message(f"ç»“æœå·²ä¿å­˜: {results_file}")
            
        except Exception as e:
            self.log_message(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def generate_batch_report(self, results, output_folder):
        """ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Š"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = os.path.join(output_folder, f"recognition_report_{timestamp}.txt")
            
            # ç»Ÿè®¡åˆ†æ
            total_count = len(results)
            class_counts = {}
            confidence_sum = 0
            
            for result in results:
                label = result['predicted_label']
                confidence = result['confidence']
                
                class_counts[label] = class_counts.get(label, 0) + 1
                confidence_sum += confidence
            
            avg_confidence = confidence_sum / total_count if total_count > 0 else 0
            
            # ç”ŸæˆæŠ¥å‘Š
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("åè½åº¦è¯†åˆ«æ‰¹é‡å¤„ç†æŠ¥å‘Š\n")
                f.write("=" * 50 + "\n\n")
                f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ€»å›¾ç‰‡æ•°: {total_count}\n")
                f.write(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2%}\n\n")
                
                f.write("åˆ†ç±»ç»Ÿè®¡:\n")
                f.write("-" * 30 + "\n")
                for label, count in sorted(class_counts.items()):
                    percentage = (count / total_count) * 100
                    f.write(f"{label}: {count} å¼  ({percentage:.1f}%)\n")
                
                f.write("\nè¯¦ç»†ç»“æœ:\n")
                f.write("-" * 30 + "\n")
                for result in results:
                    f.write(f"{result['filename']}: {result['predicted_label']} ({result['confidence']:.2%})\n")
            
            self.log_message(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            
        except Exception as e:
            self.log_message(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    def reset_batch_ui(self):
        """é‡ç½®æ‰¹é‡å¤„ç†UIçŠ¶æ€"""
        self.is_processing = False
        self.batch_start_btn.config(state=tk.NORMAL)
        self.batch_stop_btn.config(state=tk.DISABLED)
        self.batch_status_var.set("å¤„ç†å®Œæˆ")
        
    def stop_batch_processing(self):
        """åœæ­¢æ‰¹é‡å¤„ç†"""
        self.is_processing = False
        self.log_message("æ­£åœ¨åœæ­¢æ‰¹é‡å¤„ç†...")
        self.batch_status_var.set("æ­£åœ¨åœæ­¢...")
        
    def import_results(self):
        """å¯¼å…¥åˆ†æç»“æœ"""
        filetypes = [
            ('JSONæ–‡ä»¶', '*.json'),
            ('æ‰€æœ‰æ–‡ä»¶', '*.*')
        ]
        
        file_path = filedialog.askopenfilename(
            title="å¯¼å…¥è¯†åˆ«ç»“æœæ–‡ä»¶",
            filetypes=filetypes
        )
        
        if file_path:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                
                # éªŒè¯æ•°æ®æ ¼å¼
                if isinstance(results, list) and results:
                    timestamp = datetime.now().isoformat()
                    self.batch_results[f"imported_{timestamp}"] = results
                    
                    self.update_analysis_chart()
                    messagebox.showinfo("æˆåŠŸ", f"æˆåŠŸå¯¼å…¥ {len(results)} æ¡è¯†åˆ«ç»“æœ")
                    self.logger.info(f"å¯¼å…¥ç»“æœæ–‡ä»¶: {file_path}")
                else:
                    messagebox.showerror("é”™è¯¯", "æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                    
            except Exception as e:
                error_msg = f"å¯¼å…¥æ–‡ä»¶å¤±è´¥: {e}"
                messagebox.showerror("é”™è¯¯", error_msg)
                self.logger.error(error_msg)
        
    def generate_analysis_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.batch_results and not self.recognition_results:
            messagebox.showwarning("è­¦å‘Š", "æ²¡æœ‰å¯åˆ†æçš„æ•°æ®")
            return
        
        try:
            # åˆå¹¶æ‰€æœ‰ç»“æœ
            all_results = []
            
            # æ·»åŠ æ‰¹é‡ç»“æœ
            for batch_results in self.batch_results.values():
                all_results.extend(batch_results)
            
            # æ·»åŠ å•å¼ è¯†åˆ«ç»“æœ
            all_results.extend(self.recognition_results)
            
            if not all_results:
                messagebox.showwarning("è­¦å‘Š", "æ²¡æœ‰å¯åˆ†æçš„æ•°æ®")
                return
            
            # é€‰æ‹©ä¿å­˜ä½ç½®
            file_path = filedialog.asksaveasfilename(
                title="ä¿å­˜åˆ†ææŠ¥å‘Š",
                defaultextension=".txt",
                filetypes=[("æ–‡æœ¬æ–‡ä»¶", "*.txt"), ("æ‰€æœ‰æ–‡ä»¶", "*.*")]
            )
            
            if not file_path:
                return
            
            # ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
            self.create_detailed_report(all_results, file_path)
            messagebox.showinfo("æˆåŠŸ", "åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ")
            
        except Exception as e:
            error_msg = f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}"
            messagebox.showerror("é”™è¯¯", error_msg)
            self.logger.error(error_msg)
    
    def create_detailed_report(self, results, file_path):
        """åˆ›å»ºè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        # ç»Ÿè®¡åˆ†æ
        total_count = len(results)
        class_counts = {}
        confidence_values = []
        
        for result in results:
            label = result['predicted_label']
            confidence = result['confidence']
            
            class_counts[label] = class_counts.get(label, 0) + 1
            confidence_values.append(confidence)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        avg_confidence = np.mean(confidence_values) if confidence_values else 0
        min_confidence = min(confidence_values) if confidence_values else 0
        max_confidence = max(confidence_values) if confidence_values else 0
        std_confidence = np.std(confidence_values) if confidence_values else 0
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        high_conf = sum(1 for c in confidence_values if c >= 0.9)
        medium_conf = sum(1 for c in confidence_values if 0.7 <= c < 0.9)
        low_conf = sum(1 for c in confidence_values if c < 0.7)
        
        # ç”ŸæˆæŠ¥å‘Š
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write("åè½åº¦è¯†åˆ«ç³»ç»Ÿ - è¯¦ç»†åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"åˆ†ææ ·æœ¬æ•°: {total_count}\n")
            f.write(f"æ¨¡å‹ç±»å‹: {self.model_name.get().upper()}\n")
            f.write(f"åˆ†ç±»æ ‡ç­¾: {', '.join(self.class_names)}\n\n")
            
            # åŸºæœ¬ç»Ÿè®¡
            f.write("ğŸ“Š åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n")
            f.write("-" * 40 + "\n")
            f.write(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f} ({avg_confidence:.1%})\n")
            f.write(f"æœ€é«˜ç½®ä¿¡åº¦: {max_confidence:.3f} ({max_confidence:.1%})\n")
            f.write(f"æœ€ä½ç½®ä¿¡åº¦: {min_confidence:.3f} ({min_confidence:.1%})\n")
            f.write(f"ç½®ä¿¡åº¦æ ‡å‡†å·®: {std_confidence:.3f}\n\n")
            
            # ç½®ä¿¡åº¦åˆ†å¸ƒ
            f.write("ğŸ¯ ç½®ä¿¡åº¦åˆ†å¸ƒ\n")
            f.write("-" * 40 + "\n")
            f.write(f"é«˜ç½®ä¿¡åº¦ (â‰¥90%): {high_conf} å¼  ({high_conf/total_count:.1%})\n")
            f.write(f"ä¸­ç­‰ç½®ä¿¡åº¦ (70%-90%): {medium_conf} å¼  ({medium_conf/total_count:.1%})\n")
            f.write(f"ä½ç½®ä¿¡åº¦ (<70%): {low_conf} å¼  ({low_conf/total_count:.1%})\n\n")
            
            # åˆ†ç±»ç»Ÿè®¡
            f.write("ğŸ“ˆ åˆ†ç±»åˆ†å¸ƒç»Ÿè®¡\n")
            f.write("-" * 40 + "\n")
            for label in sorted(class_counts.keys()):
                count = class_counts[label]
                percentage = (count / total_count) * 100
                f.write(f"{label:>12}: {count:>4} å¼  ({percentage:>5.1f}%)\n")
            
            f.write("\n")
            
            # è´¨é‡è¯„ä¼°
            f.write("ğŸ” è´¨é‡è¯„ä¼°\n")
            f.write("-" * 40 + "\n")
            
            if avg_confidence >= 0.9:
                quality = "ä¼˜ç§€"
            elif avg_confidence >= 0.8:
                quality = "è‰¯å¥½"
            elif avg_confidence >= 0.7:
                quality = "ä¸€èˆ¬"
            else:
                quality = "éœ€è¦æ”¹è¿›"
            
            f.write(f"æ•´ä½“è¯†åˆ«è´¨é‡: {quality}\n")
            
            if std_confidence < 0.1:
                consistency = "éå¸¸ç¨³å®š"
            elif std_confidence < 0.15:
                consistency = "ç¨³å®š"
            elif std_confidence < 0.2:
                consistency = "è¾ƒç¨³å®š"
            else:
                consistency = "ä¸ç¨³å®š"
            
            f.write(f"è¯†åˆ«ä¸€è‡´æ€§: {consistency}\n\n")
            
            # å»ºè®®
            f.write("ğŸ’¡ æ”¹è¿›å»ºè®®\n")
            f.write("-" * 40 + "\n")
            
            if low_conf > total_count * 0.1:
                f.write("- ä½ç½®ä¿¡åº¦æ ·æœ¬è¾ƒå¤šï¼Œå»ºè®®æ£€æŸ¥å›¾ç‰‡è´¨é‡æˆ–é‡æ–°è®­ç»ƒæ¨¡å‹\n")
            
            if std_confidence > 0.2:
                f.write("- è¯†åˆ«ç»“æœä¸å¤Ÿç¨³å®šï¼Œå»ºè®®å¢åŠ è®­ç»ƒæ•°æ®\n")
            
            # æ‰¾å‡ºæœ€ä¸å¹³è¡¡çš„åˆ†ç±»
            class_ratios = [count/total_count for count in class_counts.values()]
            if max(class_ratios) > 0.6:
                f.write("- åˆ†ç±»åˆ†å¸ƒä¸å¹³è¡¡ï¼Œå»ºè®®å¹³è¡¡å„ç±»åˆ«çš„è®­ç»ƒæ•°æ®\n")
            
            if avg_confidence >= 0.9:
                f.write("- æ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼Œå¯ä»¥è€ƒè™‘éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ\n")
            
            f.write("\nè¯¦ç»†è¯†åˆ«ç»“æœåˆ—è¡¨:\n")
            f.write("-" * 40 + "\n")
            
            # æŒ‰ç½®ä¿¡åº¦æ’åºæ˜¾ç¤º
            sorted_results = sorted(results, key=lambda x: x['confidence'], reverse=True)
            for i, result in enumerate(sorted_results[:50]):  # åªæ˜¾ç¤ºå‰50ä¸ª
                filename = os.path.basename(result['image_path']) if 'image_path' in result else result.get('filename', f'Image_{i+1}')
                f.write(f"{filename:>30}: {result['predicted_label']:>10} ({result['confidence']:>6.1%})\n")
            
            if len(sorted_results) > 50:
                f.write(f"\n... è¿˜æœ‰ {len(sorted_results) - 50} æ¡ç»“æœæœªæ˜¾ç¤º\n")
        
    def update_analysis_chart(self):
        """æ›´æ–°åˆ†æå›¾è¡¨"""
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = []
        
        # æ·»åŠ æ‰¹é‡ç»“æœ
        for batch_results in self.batch_results.values():
            all_results.extend(batch_results)
        
        # æ·»åŠ å•å¼ è¯†åˆ«ç»“æœ
        all_results.extend(self.recognition_results)
        
        if not all_results:
            # æ˜¾ç¤ºç©ºæ•°æ®æç¤º
            self.fig.clear()
            ax = self.fig.add_subplot(111)
            ax.text(0.5, 0.5, 'æš‚æ— æ•°æ®\nè¯·å…ˆè¿›è¡Œåè½åº¦è¯†åˆ«', 
                    horizontalalignment='center', verticalalignment='center',
                    transform=ax.transAxes, fontsize=16, color='gray')
            ax.set_title('è¯†åˆ«ç»“æœç»Ÿè®¡', fontsize=14, fontweight='bold')
            ax.axis('off')
            self.canvas_plot.draw()
            return
        
        # ç»Ÿè®¡æ•°æ®
        class_counts = {}
        confidence_values = []
        
        for result in all_results:
            label = result['predicted_label']
            confidence = result['confidence']
            
            class_counts[label] = class_counts.get(label, 0) + 1
            confidence_values.append(confidence)
        
        # æ¸…é™¤ä¹‹å‰çš„å›¾è¡¨
        self.fig.clear()
        
        # åˆ›å»º2x2å­å›¾
        gs = self.fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)
        
        # 1. åˆ†ç±»åˆ†å¸ƒé¥¼å›¾
        ax1 = self.fig.add_subplot(gs[0, 0])
        if class_counts:
            labels = list(class_counts.keys())
            sizes = list(class_counts.values())
            colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))
            
            wedges, texts, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.1f%%', 
                                              colors=colors, startangle=90)
            ax1.set_title('åˆ†ç±»åˆ†å¸ƒ', fontweight='bold')
            
            # ç¾åŒ–æ–‡æœ¬
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')
        
        # 2. åˆ†ç±»åˆ†å¸ƒæŸ±çŠ¶å›¾
        ax2 = self.fig.add_subplot(gs[0, 1])
        if class_counts:
            labels = list(class_counts.keys())
            values = list(class_counts.values())
            
            bars = ax2.bar(labels, values, color=plt.cm.Set2(np.linspace(0, 1, len(labels))))
            ax2.set_title('å„ç±»åˆ«æ•°é‡', fontweight='bold')
            ax2.set_ylabel('æ•°é‡')
            
            # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{value}', ha='center', va='bottom', fontweight='bold')
            
            plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
        
        # 3. ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾
        ax3 = self.fig.add_subplot(gs[1, 0])
        if confidence_values:
            ax3.hist(confidence_values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            ax3.axvline(np.mean(confidence_values), color='red', linestyle='--', 
                       label=f'å¹³å‡å€¼: {np.mean(confidence_values):.2f}')
            ax3.set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ', fontweight='bold')
            ax3.set_xlabel('ç½®ä¿¡åº¦')
            ax3.set_ylabel('é¢‘æ¬¡')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        # 4. ç½®ä¿¡åº¦ç­‰çº§åˆ†å¸ƒ
        ax4 = self.fig.add_subplot(gs[1, 1])
        if confidence_values:
            high_conf = sum(1 for c in confidence_values if c >= 0.9)
            medium_conf = sum(1 for c in confidence_values if 0.7 <= c < 0.9)
            low_conf = sum(1 for c in confidence_values if c < 0.7)
            
            categories = ['é«˜\n(â‰¥90%)', 'ä¸­\n(70%-90%)', 'ä½\n(<70%)']
            values = [high_conf, medium_conf, low_conf]
            colors = ['green', 'orange', 'red']
            
            bars = ax4.bar(categories, values, color=colors, alpha=0.7)
            ax4.set_title('ç½®ä¿¡åº¦ç­‰çº§åˆ†å¸ƒ', fontweight='bold')
            ax4.set_ylabel('æ•°é‡')
            
            # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼å’Œç™¾åˆ†æ¯”
            total = len(confidence_values)
            for bar, value in zip(bars, values):
                if value > 0:
                    height = bar.get_height()
                    percentage = (value / total) * 100
                    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                            f'{value}\n({percentage:.1f}%)', 
                            ha='center', va='bottom', fontweight='bold')
        
        # æ›´æ–°å›¾è¡¨
        self.canvas_plot.draw()

def main():
    """ä¸»å‡½æ•°"""
    root = tk.Tk()
    app = SlumpRecognitionGUI(root)
    root.mainloop()

if __name__ == "__main__":
    main()
