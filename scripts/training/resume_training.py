"""
æ¢å¤è®­ç»ƒè„šæœ¬
ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼Œé¿å…é‡æ–°å¼€å§‹è®­ç»ƒ
æ”¯æŒé€šè¿‡é…ç½®æ–‡ä»¶æ§åˆ¶ï¼Œå¯åœ¨PyCharmä¸­ç›´æ¥è¿è¡Œ
"""

import os
import sys
import yaml
import torch
import time
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.models.vgg import vgg16
from src.data.dataset import get_data_loaders
from src.utils.checkpoint_manager import CheckpointManager
from src.utils.visualization import generate_all_plots
from .training_utils import (
    validate_resume_config,
    setup_training_directories,
    backup_config_file,
    setup_training_logger,
    print_training_summary,
    check_training_prerequisites,
    get_training_status_summary
)


def load_config(config_path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return None
    except yaml.YAMLError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return None


def create_optimizer(model, optimizer_name, learning_rate):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    optimizer_map = {
        'adam': torch.optim.Adam,
        'sgd': lambda params, lr: torch.optim.SGD(params, lr=lr, momentum=0.9),
        'adamw': torch.optim.AdamW,
        'rmsprop': torch.optim.RMSprop
    }
    
    optimizer_class = optimizer_map.get(optimizer_name.lower())
    if optimizer_class:
        return optimizer_class(model.parameters(), lr=learning_rate)
    else:
        print(f"âš ï¸  æœªçŸ¥çš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_name}ï¼Œä½¿ç”¨é»˜è®¤Adam")
        return torch.optim.Adam(model.parameters(), lr=learning_rate)


def create_scheduler(optimizer, scheduler_type, scheduler_configs, epochs):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    if scheduler_type == 'None' or scheduler_type is None:
        return None
    
    # å¤„ç†CosineAnnealingLRçš„T_maxå‚æ•°
    cosine_config = scheduler_configs.get('CosineAnnealingLR', {}).copy()
    if cosine_config.get('T_max') == 'auto':
        cosine_config['T_max'] = epochs
    
    if 'eta_min' in cosine_config and isinstance(cosine_config['eta_min'], str):
        cosine_config['eta_min'] = float(cosine_config['eta_min'])
    
    scheduler_map = {
        'StepLR': lambda: torch.optim.lr_scheduler.StepLR(optimizer, **scheduler_configs['StepLR']),
        'MultiStepLR': lambda: torch.optim.lr_scheduler.MultiStepLR(optimizer, **scheduler_configs['MultiStepLR']),
        'CosineAnnealingLR': lambda: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, **cosine_config),
        'ReduceLROnPlateau': lambda: torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, **scheduler_configs['ReduceLROnPlateau'])
    }
    
    if scheduler_type in scheduler_map:
        try:
            return scheduler_map[scheduler_type]()
        except Exception as e:
            print(f"âš ï¸  åˆ›å»ºè°ƒåº¦å™¨å¤±è´¥: {e}ï¼Œä¸ä½¿ç”¨è°ƒåº¦å™¨")
            return None
    else:
        print(f"âš ï¸  æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}ï¼Œä¸ä½¿ç”¨è°ƒåº¦å™¨")
        return None


def create_loss_function(loss_config, train_lines, num_classes):
    """åˆ›å»ºæŸå¤±å‡½æ•°"""
    loss_name = loss_config['name']
    
    if loss_name == 'CrossEntropyLoss':
        return torch.nn.CrossEntropyLoss()
    elif loss_name == 'WeightedCrossEntropyLoss':
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_counts = [0] * num_classes
        for line in train_lines:
            class_id = int(line.split(';')[0])
            class_counts[class_id] += 1
        
        total_samples = sum(class_counts)
        class_weights = [total_samples / (num_classes * count) if count > 0 else 1.0 
                        for count in class_counts]
        weights = torch.FloatTensor(class_weights)
        return torch.nn.CrossEntropyLoss(weight=weights)
    else:
        print(f"âš ï¸  æœªçŸ¥çš„æŸå¤±å‡½æ•°: {loss_name}ï¼Œä½¿ç”¨é»˜è®¤CrossEntropyLoss")
        return torch.nn.CrossEntropyLoss()


def find_checkpoint_file(checkpoint_manager, resume_config, project_root):
    """æŸ¥æ‰¾æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    checkpoint_path = None
    
    # 1. ä¼˜å…ˆä½¿ç”¨æŒ‡å®šçš„æ£€æŸ¥ç‚¹è·¯å¾„
    if resume_config.get('checkpoint_path'):
        checkpoint_path = os.path.join(project_root, resume_config['checkpoint_path'])
        if not os.path.exists(checkpoint_path):
            print(f"âŒ æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            checkpoint_path = None
    
    # 2. ä½¿ç”¨æŒ‡å®šçš„è®­ç»ƒç›®å½•
    if not checkpoint_path and resume_config.get('train_dir'):
        checkpoint_path = checkpoint_manager.find_latest_checkpoint(resume_config['train_dir'])
    
    # 3. è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹
    if not checkpoint_path and resume_config.get('auto_find_latest', True):
        checkpoint_path = checkpoint_manager.find_latest_checkpoint()
    
    return checkpoint_path


def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, total_epochs):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_train_loss = 0.0
    train_correct = 0
    train_total = 0
    
    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{total_epochs} [Train]',
                     ncols=100, leave=False)
    
    for batch_idx, (img, label, _) in enumerate(train_pbar):
        img = img.to(device)
        label = label.to(device)
        
        optimizer.zero_grad()
        output = model(img)
        train_loss = criterion(output, label)
        train_loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        total_train_loss += train_loss.item()
        _, predicted = output.max(1)
        train_total += label.size(0)
        train_correct += predicted.eq(label).sum().item()
        
        # æ›´æ–°è¿›åº¦æ¡
        current_loss = total_train_loss / (batch_idx + 1)
        current_acc = 100. * train_correct / train_total
        train_pbar.set_postfix({
            'Loss': f'{current_loss:.4f}',
            'Acc': f'{current_acc:.2f}%'
        })
    
    avg_train_loss = total_train_loss / len(train_loader)
    train_acc = 100. * train_correct / train_total
    
    return avg_train_loss, train_acc


def validate_one_epoch(model, val_loader, criterion, device, epoch, total_epochs):
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    total_val_loss = 0.0
    val_correct = 0
    val_total = 0
    
    val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{total_epochs} [Val]',
                   ncols=100, leave=False)
    
    with torch.no_grad():
        for batch_idx, (img, label, _) in enumerate(val_pbar):
            img = img.to(device)
            label = label.to(device)
            
            output = model(img)
            val_loss = criterion(output, label)
            
            # ç»Ÿè®¡
            total_val_loss += val_loss.item()
            _, predicted = output.max(1)
            val_total += label.size(0)
            val_correct += predicted.eq(label).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            current_loss = total_val_loss / (batch_idx + 1)
            current_acc = 100. * val_correct / val_total
            val_pbar.set_postfix({
                'Loss': f'{current_loss:.4f}',
                'Acc': f'{current_acc:.2f}%'
            })
    
    avg_val_loss = total_val_loss / len(val_loader)
    val_acc = 100. * val_correct / val_total
    
    return avg_val_loss, val_acc


def main():
    """ä¸»å‡½æ•° - æ¢å¤è®­ç»ƒ"""
    print("ğŸ”„ VGG16 æ¢å¤è®­ç»ƒè„šæœ¬")
    print("="*80)
    
    # ==================== è·¯å¾„å’Œé…ç½®åŠ è½½ ====================
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(script_dir))  # ä¸Šä¸¤çº§ç›®å½•
    config_path = os.path.join(project_root, 'configs', 'training_config.yaml')
    
    print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {config_path}")
    
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    if config is None:
        print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # ==================== é…ç½®éªŒè¯ ====================
    valid, error_msg = validate_resume_config(config)
    if not valid:
        print(f"âŒ {error_msg}")
        print("ğŸ’¡ æç¤º: ä¿®æ”¹ configs/training_config.yaml ä¸­çš„ç›¸å…³é…ç½®")
        return
    
    print("âœ… æ¢å¤è®­ç»ƒæ¨¡å¼å·²å¯ç”¨")
    
    # ==================== å‰ç½®æ¡ä»¶æ£€æŸ¥ ====================
    prerequisites_ok, errors = check_training_prerequisites(project_root, config)
    if not prerequisites_ok:
        print("âŒ è®­ç»ƒå‰ç½®æ¡ä»¶æ£€æŸ¥å¤±è´¥:")
        for error in errors:
            print(f"   â€¢ {error}")
        return
    
    print("âœ… è®­ç»ƒå‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡")
    
    # ==================== æ£€æŸ¥ç‚¹ç®¡ç† ====================
    checkpoint_manager = CheckpointManager(project_root)
    checkpoint_manager.print_checkpoint_summary()
    
    resume_config = config.get('resume', {})
    checkpoint_path = find_checkpoint_file(checkpoint_manager, resume_config, project_root)
    
    if not checkpoint_path:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ£€æŸ¥ç‚¹æ–‡ä»¶")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. æ˜¯å¦å·²ç»è¿›è¡Œè¿‡è®­ç»ƒå¹¶ä¿å­˜äº†æ£€æŸ¥ç‚¹")
        print("   2. é…ç½®æ–‡ä»¶ä¸­çš„æ£€æŸ¥ç‚¹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   3. æ£€æŸ¥ç‚¹æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        return
    
    print(f"ğŸ¯ å°†ä½¿ç”¨æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # ==================== è®¾ç½®è®­ç»ƒç›®å½• ====================
    directories = setup_training_directories(project_root, config, "resume")
    
    # ==================== æ‰“å°è®­ç»ƒæ‘˜è¦ ====================
    print_training_summary(config, directories, checkpoint_path)
    
    # ==================== æå–é…ç½®å‚æ•° ====================
    EPOCHS = config['training']['epochs']
    LEARNING_RATE = config['training']['learning_rate']
    BATCH_SIZE = config['dataloader']['batch_size']
    NUM_WORKERS = config['dataloader']['num_workers']
    NUM_CLASSES = config['data']['num_classes']
    CLASS_NAMES = config['data']['class_names']

    # ==================== è®¾å¤‡å’Œæ¨¡å‹åˆå§‹åŒ– ====================
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    model = vgg16(
        pretrained=config['model']['pretrained'],
        num_classes=NUM_CLASSES,
        dropout=config['model']['dropout']
    )
    model.to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimizer(model, config['training']['optimizer'], LEARNING_RATE)

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_config = config['training']['scheduler']
    scheduler_type = scheduler_config['type']
    scheduler_configs = {
        'StepLR': scheduler_config.get('step_lr', {'step_size': 7, 'gamma': 0.5}),
        'MultiStepLR': scheduler_config.get('multi_step_lr', {'milestones': [10, 15], 'gamma': 0.1}),
        'CosineAnnealingLR': scheduler_config.get('cosine_annealing_lr', {'T_max': 'auto', 'eta_min': 0}),
        'ReduceLROnPlateau': scheduler_config.get('reduce_lr_on_plateau', {'mode': 'min', 'factor': 0.5, 'patience': 3})
    }
    scheduler = create_scheduler(optimizer, scheduler_type, scheduler_configs, EPOCHS)

    # ==================== åŠ è½½æ£€æŸ¥ç‚¹ ====================
    try:
        start_epoch, best_acc = checkpoint_manager.load_checkpoint(
            checkpoint_path, model, optimizer, scheduler
        )
        print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        print(f"   èµ·å§‹epoch: {start_epoch + 1}")
        print(f"   å½“å‰æœ€ä½³ç²¾åº¦: {best_acc:.2f}%")
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
        return

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­è®­ç»ƒ
    if start_epoch >= EPOCHS:
        print(f"âš ï¸  æ£€æŸ¥ç‚¹çš„epoch ({start_epoch}) å·²è¾¾åˆ°æˆ–è¶…è¿‡ç›®æ ‡è½®æ•° ({EPOCHS})")
        print("ğŸ’¡ å¦‚éœ€ç»§ç»­è®­ç»ƒï¼Œè¯·å¢åŠ é…ç½®æ–‡ä»¶ä¸­çš„ training.epochs å€¼")
        return

    # ==================== æ•°æ®åŠ è½½ ====================
    print(f"\nğŸ“Š å‡†å¤‡æ•°æ®åŠ è½½å™¨...")

    annotation_path = os.path.join(project_root, config['data']['annotation_file'])

    try:
        train_loader, val_loader, train_lines, val_lines = get_data_loaders(
            annotation_path=annotation_path,
            batch_size=BATCH_SIZE,
            num_workers=NUM_WORKERS,
            train_val_split=config['data']['train_val_split'],
            random_seed=config['data']['random_seed'],
            stratified_split=config['data']['stratified_split'],
            num_classes=NUM_CLASSES
        )
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_lines)}")
        print(f"   éªŒè¯æ ·æœ¬: {len(val_lines)}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # ==================== æŸå¤±å‡½æ•° ====================
    criterion = create_loss_function(
        config['training']['loss_function'],
        train_lines,
        NUM_CLASSES
    )
    criterion.to(device)

    # ==================== è®¾ç½®æ—¥å¿—å’Œå¤‡ä»½é…ç½® ====================
    logger = setup_training_logger(directories['log_dir'], directories['timestamp'], "resume_training")
    logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {checkpoint_path}")
    logger.info(f"èµ·å§‹epoch: {start_epoch + 1}")
    logger.info(f"ç›®æ ‡epoch: {EPOCHS}")
    logger.info(f"å½“å‰æœ€ä½³ç²¾åº¦: {best_acc:.2f}%")

    # å¤‡ä»½é…ç½®æ–‡ä»¶
    backup_config_file(config, directories['log_dir'])

    # ==================== æå–é…ç½®å‚æ•° ====================
    EPOCHS = config['training']['epochs']
    LEARNING_RATE = config['training']['learning_rate']
    BATCH_SIZE = config['dataloader']['batch_size']
    NUM_WORKERS = config['dataloader']['num_workers']
    NUM_CLASSES = config['data']['num_classes']
    CLASS_NAMES = config['data']['class_names']

    # ==================== è®¾å¤‡å’Œæ¨¡å‹åˆå§‹åŒ– ====================
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    model = vgg16(
        pretrained=config['model']['pretrained'],
        num_classes=NUM_CLASSES,
        dropout=config['model']['dropout']
    )
    model.to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimizer(model, config['training']['optimizer'], LEARNING_RATE)

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_config = config['training']['scheduler']
    scheduler_type = scheduler_config['type']
    scheduler_configs = {
        'StepLR': scheduler_config.get('step_lr', {'step_size': 7, 'gamma': 0.5}),
        'MultiStepLR': scheduler_config.get('multi_step_lr', {'milestones': [10, 15], 'gamma': 0.1}),
        'CosineAnnealingLR': scheduler_config.get('cosine_annealing_lr', {'T_max': 'auto', 'eta_min': 0}),
        'ReduceLROnPlateau': scheduler_config.get('reduce_lr_on_plateau', {'mode': 'min', 'factor': 0.5, 'patience': 3})
    }
    scheduler = create_scheduler(optimizer, scheduler_type, scheduler_configs, EPOCHS)

    # ==================== åŠ è½½æ£€æŸ¥ç‚¹ ====================
    try:
        start_epoch, best_acc = checkpoint_manager.load_checkpoint(
            checkpoint_path, model, optimizer, scheduler
        )
        print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        print(f"   èµ·å§‹epoch: {start_epoch + 1}")
        print(f"   å½“å‰æœ€ä½³ç²¾åº¦: {best_acc:.2f}%")
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
        return

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­è®­ç»ƒ
    if start_epoch >= EPOCHS:
        print(f"âš ï¸  æ£€æŸ¥ç‚¹çš„epoch ({start_epoch}) å·²è¾¾åˆ°æˆ–è¶…è¿‡ç›®æ ‡è½®æ•° ({EPOCHS})")
        print("ğŸ’¡ å¦‚éœ€ç»§ç»­è®­ç»ƒï¼Œè¯·å¢åŠ é…ç½®æ–‡ä»¶ä¸­çš„ training.epochs å€¼")
        return

    # ==================== æ•°æ®åŠ è½½ ====================
    print(f"\nğŸ“Š å‡†å¤‡æ•°æ®åŠ è½½å™¨...")

    annotation_path = os.path.join(project_root, config['data']['annotation_file'])

    try:
        train_loader, val_loader, train_lines, val_lines = get_data_loaders(
            annotation_path=annotation_path,
            batch_size=BATCH_SIZE,
            num_workers=NUM_WORKERS,
            train_val_split=config['data']['train_val_split'],
            random_seed=config['data']['random_seed'],
            stratified_split=config['data']['stratified_split'],
            num_classes=NUM_CLASSES
        )
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_lines)}")
        print(f"   éªŒè¯æ ·æœ¬: {len(val_lines)}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # ==================== æŸå¤±å‡½æ•° ====================
    criterion = create_loss_function(
        config['training']['loss_function'],
        train_lines,
        NUM_CLASSES
    )
    criterion.to(device)

    # ==================== è®¾ç½®æ—¥å¿—å’Œå¤‡ä»½é…ç½® ====================
    logger = setup_training_logger(directories['log_dir'], directories['timestamp'], "resume_training")
    logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {checkpoint_path}")
    logger.info(f"èµ·å§‹epoch: {start_epoch + 1}")
    logger.info(f"ç›®æ ‡epoch: {EPOCHS}")
    logger.info(f"å½“å‰æœ€ä½³ç²¾åº¦: {best_acc:.2f}%")

    # å¤‡ä»½é…ç½®æ–‡ä»¶
    backup_config_file(config, directories['log_dir'])

    print(f"\nğŸš€ å¼€å§‹æ¢å¤è®­ç»ƒ (ä»ç¬¬{start_epoch + 1}è½®åˆ°ç¬¬{EPOCHS}è½®)")
    print("="*80)

    # ==================== è®­ç»ƒå¾ªç¯ ====================
    # è®­ç»ƒå†å²è®°å½•ï¼ˆä»æ¢å¤ç‚¹å¼€å§‹é‡æ–°è®°å½•ï¼‰
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    start_time = time.time()

    # è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, EPOCHS):
        epoch_start_time = time.time()
        print(f"\nEpoch {epoch + 1}/{EPOCHS}")
        print("-" * 80)

        # ==================== è®­ç»ƒé˜¶æ®µ ====================
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device, epoch, EPOCHS
        )

        # ==================== éªŒè¯é˜¶æ®µ ====================
        val_loss, val_acc = validate_one_epoch(
            model, val_loader, criterion, device, epoch, EPOCHS
        )

        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None:
            if scheduler_type == 'ReduceLROnPlateau':
                scheduler.step(val_loss)
            else:
                scheduler.step()

        current_lr = optimizer.param_groups[0]['lr']

        # è®°å½•å†å²
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        is_best = val_acc > best_acc
        if is_best:
            best_acc = val_acc

        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_manager.save_checkpoint(
            epoch=epoch,
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            best_acc=best_acc,
            train_loss=train_loss,
            val_loss=val_loss,
            train_acc=train_acc,
            val_acc=val_acc,
            save_dir=directories['checkpoint_dir'],
            latest_dir=directories['latest_checkpoint_dir'],
            is_best=is_best,
            save_frequency=config['save']['save_frequency']
        )

        # è®¡ç®—epochæ—¶é—´
        epoch_time = time.time() - epoch_start_time
        elapsed_time = time.time() - start_time

        # æ‰“å°epochç»“æœ
        print(f"\nEpoch {epoch + 1}/{EPOCHS} å®Œæˆ:")
        print(f"  è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"  éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
        print(f"  æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")
        print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
        print(f"  æœ¬è½®ç”¨æ—¶: {epoch_time:.1f}ç§’")

        # è®°å½•åˆ°æ—¥å¿—
        logger.info(f"Epoch {epoch + 1}/{EPOCHS} - "
                   f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, "
                   f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, "
                   f"Best Acc: {best_acc:.2f}%, LR: {current_lr:.6f}")

        # æ¯5ä¸ªepochæ‰“å°çŠ¶æ€æ‘˜è¦
        if (epoch + 1) % 5 == 0:
            status_summary = get_training_status_summary(
                epoch + 1 - start_epoch, EPOCHS - start_epoch,
                train_acc, val_acc, best_acc, elapsed_time
            )
            print(status_summary)

    # ==================== è®­ç»ƒå®Œæˆ ====================
    total_time = time.time() - start_time
    print(f"\nğŸ¯ æ¢å¤è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f}å°æ—¶")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {directories['checkpoint_dir']}/")
    print("="*80)

    logger.info("ğŸ¯ æ¢å¤è®­ç»ƒå®Œæˆ")
    logger.info(f"æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")
    logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f}å°æ—¶")

    # ==================== è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ====================
    print(f"\nğŸ“Š å¼€å§‹ç”Ÿæˆè®­ç»ƒåˆ†æå›¾è¡¨...")
    try:
        # ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨
        evaluation_results = generate_all_plots(
            train_losses=train_losses,
            val_losses=val_losses,
            train_accuracies=train_accuracies,
            val_accuracies=val_accuracies,
            model=model,
            dataloader=val_loader,
            device=device,
            class_names=CLASS_NAMES,
            plot_dir=directories['plot_dir'],
            latest_plot_dir=directories['latest_plot_dir']
        )

        print(f"âœ… æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“ å›¾è¡¨ä¿å­˜ä½ç½®:")
        print(f"   ğŸ“Š æ—¶é—´æˆ³ç›®å½•: {directories['plot_dir']}")
        print(f"   ğŸ“Š æœ€æ–°ç›®å½•: {directories['latest_plot_dir']}")

        logger.info("ğŸ“Š å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
        logger.info(f"å›¾è¡¨ä¿å­˜ä½ç½®: {directories['plot_dir']}")
        logger.info(f"æœ€æ–°å›¾è¡¨ä½ç½®: {directories['latest_plot_dir']}")

    except Exception as e:
        print(f"âš ï¸  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
        logger.error(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

    print(f"\nğŸ‰ æ¢å¤è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆ!")
    print(f"ğŸ’¡ æç¤º:")
    print(f"   â€¢ æŸ¥çœ‹è®­ç»ƒå›¾è¡¨: {directories['latest_plot_dir']}")
    print(f"   â€¢ æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: {directories['log_dir']}")
    print(f"   â€¢ æ¨¡å‹æ–‡ä»¶ä½ç½®: {directories['latest_checkpoint_dir']}")
    print("="*80)


if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œé…ç½® - å¯ä»¥åœ¨ PyCharm ä¸­ç›´æ¥ç‚¹å‡»è¿è¡Œ
    æ¢å¤è®­ç»ƒåŠŸèƒ½é€šè¿‡é…ç½®æ–‡ä»¶æ§åˆ¶
    
    ä½¿ç”¨æ–¹æ³•:
    1. ä¿®æ”¹ configs/training_config.yaml ä¸­çš„é…ç½®:
       - training.epochs: è®¾ç½®ç›®æ ‡è½®æ•° (å¦‚100)
       - resume.enabled: è®¾ç½®ä¸º true
       - resume.auto_find_latest: è®¾ç½®ä¸º true (è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹)
    
    2. åœ¨ PyCharm ä¸­ç›´æ¥è¿è¡Œæ­¤è„šæœ¬
    
    3. è„šæœ¬ä¼šè‡ªåŠ¨:
       - æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
       - ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒçŠ¶æ€
       - ç»§ç»­è®­ç»ƒåˆ°ç›®æ ‡è½®æ•°
       - ç”Ÿæˆå®Œæ•´çš„åˆ†æå›¾è¡¨
    """
    main()
