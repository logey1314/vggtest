"""
è®­ç»ƒå†å²ç®¡ç†è„šæœ¬
ç”¨äºæŸ¥çœ‹ã€æ¯”è¾ƒå’Œç®¡ç†ä¸åŒè®­ç»ƒçš„ç»“æœ
"""

import os
import sys
import json
import datetime
from pathlib import Path

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))


def list_training_runs(project_root):
    """åˆ—å‡ºæ‰€æœ‰è®­ç»ƒè¿è¡Œè®°å½•"""
    checkpoint_base = os.path.join(project_root, 'models', 'checkpoints')
    evaluation_base = os.path.join(project_root, 'outputs', 'evaluation')
    log_base = os.path.join(project_root, 'outputs', 'logs')

    print("ğŸš€ è®­ç»ƒå†å²è®°å½•")
    print("="*80)
    
    # åˆ—å‡ºè®­ç»ƒæ£€æŸ¥ç‚¹
    if os.path.exists(checkpoint_base):
        train_dirs = [d for d in os.listdir(checkpoint_base) 
                     if d.startswith('train_') and os.path.isdir(os.path.join(checkpoint_base, d))]
        train_dirs.sort(reverse=True)  # æœ€æ–°çš„åœ¨å‰
        
        print(f"ğŸ“ è®­ç»ƒæ£€æŸ¥ç‚¹ç›®å½• ({len(train_dirs)} ä¸ª):")
        for i, train_dir in enumerate(train_dirs[:10]):  # åªæ˜¾ç¤ºæœ€è¿‘10ä¸ª
            train_path = os.path.join(checkpoint_base, train_dir)
            best_model_path = os.path.join(train_path, 'best_model.pth')
            
            # è·å–ç›®å½•åˆ›å»ºæ—¶é—´
            timestamp = train_dir.replace('train_', '')
            try:
                dt = datetime.datetime.strptime(timestamp, '%Y%m%d_%H%M%S')
                time_str = dt.strftime('%Y-%m-%d %H:%M:%S')
            except:
                time_str = timestamp
            
            status = "âœ…" if os.path.exists(best_model_path) else "âŒ"
            print(f"  {i+1:2d}. {train_dir} - {time_str} {status}")
    
    print()
    
    # åˆ—å‡ºè¯„ä¼°ç»“æœ
    if os.path.exists(evaluation_base):
        eval_dirs = [d for d in os.listdir(evaluation_base) 
                    if d.startswith('eval_') and os.path.isdir(os.path.join(evaluation_base, d))]
        eval_dirs.sort(reverse=True)  # æœ€æ–°çš„åœ¨å‰
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœç›®å½• ({len(eval_dirs)} ä¸ª):")
        for i, eval_dir in enumerate(eval_dirs[:10]):  # åªæ˜¾ç¤ºæœ€è¿‘10ä¸ª
            eval_path = os.path.join(evaluation_base, eval_dir)
            results_path = os.path.join(eval_path, 'reports', 'evaluation_results.json')
            
            # è·å–ç›®å½•åˆ›å»ºæ—¶é—´
            timestamp = eval_dir.replace('eval_', '')
            try:
                dt = datetime.datetime.strptime(timestamp, '%Y%m%d_%H%M%S')
                time_str = dt.strftime('%Y-%m-%d %H:%M:%S')
            except:
                time_str = timestamp
            
            # å°è¯•è¯»å–è¯„ä¼°ç»“æœ
            accuracy = "N/A"
            if os.path.exists(results_path):
                try:
                    with open(results_path, 'r', encoding='utf-8') as f:
                        results = json.load(f)
                        accuracy = f"{results['metrics']['accuracy']:.4f}"
                except:
                    pass
            
            status = "âœ…" if os.path.exists(results_path) else "âŒ"
            print(f"  {i+1:2d}. {eval_dir} - {time_str} - å‡†ç¡®ç‡: {accuracy} {status}")


def compare_evaluations(project_root, eval_dirs=None):
    """æ¯”è¾ƒå¤šä¸ªè¯„ä¼°ç»“æœ"""
    evaluation_base = os.path.join(project_root, 'outputs', 'evaluation')
    
    if eval_dirs is None:
        # è‡ªåŠ¨é€‰æ‹©æœ€è¿‘çš„å‡ ä¸ªè¯„ä¼°
        all_eval_dirs = [d for d in os.listdir(evaluation_base) 
                        if d.startswith('eval_') and os.path.isdir(os.path.join(evaluation_base, d))]
        all_eval_dirs.sort(reverse=True)
        eval_dirs = all_eval_dirs[:3]  # æœ€è¿‘3ä¸ª
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœå¯¹æ¯” (æœ€è¿‘{len(eval_dirs)}æ¬¡)")
    print("="*80)
    
    results_data = []
    for eval_dir in eval_dirs:
        results_path = os.path.join(evaluation_base, eval_dir, 'reports', 'evaluation_results.json')
        if os.path.exists(results_path):
            try:
                with open(results_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                    results_data.append((eval_dir, results))
            except Exception as e:
                print(f"âŒ æ— æ³•è¯»å– {eval_dir}: {e}")
    
    if not results_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ")
        return
    
    # è¡¨å¤´
    print(f"{'è¯„ä¼°æ—¶é—´':<20} {'å‡†ç¡®ç‡':<10} {'F1-Score':<10} {'é”™è¯¯ç‡':<10} {'ç›¸é‚»æ··æ·†':<10}")
    print("-" * 80)
    
    # æ•°æ®è¡Œ
    for eval_dir, results in results_data:
        timestamp = eval_dir.replace('eval_', '')
        try:
            dt = datetime.datetime.strptime(timestamp, '%Y%m%d_%H%M%S')
            time_str = dt.strftime('%m-%d %H:%M')
        except:
            time_str = timestamp[:12]
        
        metrics = results['metrics']
        accuracy = f"{metrics['accuracy']:.4f}"
        f1_score = f"{metrics['f1_macro']:.4f}"
        error_rate = f"{results['error_info']['error_rate']:.4f}"
        adjacent_confusion = f"{metrics['adjacent_confusion_rate']:.4f}"
        
        print(f"{time_str:<20} {accuracy:<10} {f1_score:<10} {error_rate:<10} {adjacent_confusion:<10}")


def clean_old_results(project_root, keep_recent=5):
    """æ¸…ç†æ—§çš„è®­ç»ƒå’Œè¯„ä¼°ç»“æœï¼Œä¿ç•™æœ€è¿‘çš„å‡ ä¸ª"""
    checkpoint_base = os.path.join(project_root, 'models', 'checkpoints')
    evaluation_base = os.path.join(project_root, 'outputs', 'evaluation')
    
    print(f"\nğŸ§¹ æ¸…ç†æ—§ç»“æœ (ä¿ç•™æœ€è¿‘{keep_recent}ä¸ª)")
    print("="*80)
    
    # æ¸…ç†è®­ç»ƒæ£€æŸ¥ç‚¹
    if os.path.exists(checkpoint_base):
        train_dirs = [d for d in os.listdir(checkpoint_base) 
                     if d.startswith('train_') and os.path.isdir(os.path.join(checkpoint_base, d))]
        train_dirs.sort(reverse=True)  # æœ€æ–°çš„åœ¨å‰
        
        to_delete = train_dirs[keep_recent:]  # è¦åˆ é™¤çš„ç›®å½•
        
        print(f"ğŸ“ è®­ç»ƒæ£€æŸ¥ç‚¹: æ€»å…±{len(train_dirs)}ä¸ªï¼Œåˆ é™¤{len(to_delete)}ä¸ª")
        for train_dir in to_delete:
            train_path = os.path.join(checkpoint_base, train_dir)
            try:
                import shutil
                shutil.rmtree(train_path)
                print(f"  âœ… å·²åˆ é™¤: {train_dir}")
            except Exception as e:
                print(f"  âŒ åˆ é™¤å¤±è´¥ {train_dir}: {e}")
    
    # æ¸…ç†è¯„ä¼°ç»“æœ
    if os.path.exists(evaluation_base):
        eval_dirs = [d for d in os.listdir(evaluation_base) 
                    if d.startswith('eval_') and os.path.isdir(os.path.join(evaluation_base, d))]
        eval_dirs.sort(reverse=True)  # æœ€æ–°çš„åœ¨å‰
        
        to_delete = eval_dirs[keep_recent:]  # è¦åˆ é™¤çš„ç›®å½•
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœ: æ€»å…±{len(eval_dirs)}ä¸ªï¼Œåˆ é™¤{len(to_delete)}ä¸ª")
        for eval_dir in to_delete:
            eval_path = os.path.join(evaluation_base, eval_dir)
            try:
                import shutil
                shutil.rmtree(eval_path)
                print(f"  âœ… å·²åˆ é™¤: {eval_dir}")
            except Exception as e:
                print(f"  âŒ åˆ é™¤å¤±è´¥ {eval_dir}: {e}")


def view_training_config(project_root, train_dir):
    """æŸ¥çœ‹ç‰¹å®šè®­ç»ƒçš„é…ç½®å‚æ•°"""
    log_base = os.path.join(project_root, 'outputs', 'logs')
    config_path = os.path.join(log_base, train_dir, 'config_backup.yaml')
    log_path = os.path.join(log_base, train_dir, 'training.log')

    print(f"\nğŸ“‹ è®­ç»ƒé…ç½®è¯¦æƒ…: {train_dir}")
    print("="*80)

    # æ˜¾ç¤ºé…ç½®æ–‡ä»¶
    if os.path.exists(config_path):
        print("ğŸ“„ è®­ç»ƒé…ç½®å‚æ•°:")
        print("-" * 40)
        try:
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)

            # æ˜¾ç¤ºå…³é”®é…ç½®
            print(f"è®­ç»ƒè½®æ•°: {config['training']['epochs']}")
            print(f"æ‰¹æ¬¡å¤§å°: {config['dataloader']['batch_size']}")
            print(f"å­¦ä¹ ç‡: {config['training']['learning_rate']}")
            print(f"ä¼˜åŒ–å™¨: {config['training']['optimizer']}")
            print(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {config['training']['scheduler']['name']}")
            print(f"ä½¿ç”¨é¢„è®­ç»ƒ: {config['model']['pretrained']}")
            print(f"ç±»åˆ«æ•°é‡: {config['data']['num_classes']}")
            print(f"è¾“å…¥å°ºå¯¸: {config['data']['input_shape']}")

        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–é…ç½®æ–‡ä»¶: {e}")
    else:
        print("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")

    # æ˜¾ç¤ºè®­ç»ƒæ—¥å¿—æ‘˜è¦
    if os.path.exists(log_path):
        print(f"\nğŸ“ è®­ç»ƒæ—¥å¿—æ‘˜è¦:")
        print("-" * 40)
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            # æå–å…³é”®ä¿¡æ¯
            for line in lines:
                if "æœ€ä½³éªŒè¯ç²¾åº¦" in line or "è®­ç»ƒå®Œæˆ" in line or "æ€»è®­ç»ƒæ—¶é•¿" in line:
                    print(line.strip())

        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶: {e}")
    else:
        print("âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨")


def main():
    """ä¸»å‡½æ•°"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)

    print("ğŸ”§ è®­ç»ƒå†å²ç®¡ç†å·¥å…·")
    print("="*80)
    print("1. æŸ¥çœ‹è®­ç»ƒå†å²")
    print("2. æ¯”è¾ƒè¯„ä¼°ç»“æœ")
    print("3. æŸ¥çœ‹è®­ç»ƒé…ç½®")
    print("4. æ¸…ç†æ—§ç»“æœ")
    print("5. é€€å‡º")
    print("="*80)
    
    while True:
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ (1-5): ").strip()

        if choice == '1':
            list_training_runs(project_root)
        elif choice == '2':
            compare_evaluations(project_root)
        elif choice == '3':
            # æŸ¥çœ‹è®­ç»ƒé…ç½®
            checkpoint_base = os.path.join(project_root, 'models', 'checkpoints')
            if os.path.exists(checkpoint_base):
                train_dirs = [d for d in os.listdir(checkpoint_base)
                             if d.startswith('train_') and os.path.isdir(os.path.join(checkpoint_base, d))]
                train_dirs.sort(reverse=True)

                if train_dirs:
                    print("\né€‰æ‹©è¦æŸ¥çœ‹çš„è®­ç»ƒ:")
                    for i, train_dir in enumerate(train_dirs[:10]):
                        print(f"  {i+1}. {train_dir}")

                    try:
                        idx = int(input("è¯·è¾“å…¥åºå·: ").strip()) - 1
                        if 0 <= idx < len(train_dirs):
                            view_training_config(project_root, train_dirs[idx])
                        else:
                            print("âŒ æ— æ•ˆåºå·")
                    except:
                        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                else:
                    print("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒè®°å½•")
            else:
                print("âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨")

        elif choice == '4':
            keep_num = input("ä¿ç•™æœ€è¿‘å‡ ä¸ªç»“æœ? (é»˜è®¤5): ").strip()
            try:
                keep_num = int(keep_num) if keep_num else 5
            except:
                keep_num = 5

            confirm = input(f"ç¡®è®¤æ¸…ç†æ—§ç»“æœï¼Œä¿ç•™æœ€è¿‘{keep_num}ä¸ª? (y/N): ").strip().lower()
            if confirm == 'y':
                clean_old_results(project_root, keep_num)
            else:
                print("âŒ å·²å–æ¶ˆæ¸…ç†æ“ä½œ")
        elif choice == '5':
            print("ğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥1-5")


if __name__ == "__main__":
    main()
