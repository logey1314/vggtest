"""
å­¦ä¹ ç‡è°ƒåº¦å™¨å¯è§†åŒ–è„šæœ¬
ç”¨äºé¢„è§ˆä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å˜åŒ–æ›²çº¿
"""
import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

import torch
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from .visualize_utils import (
    setup_plot_style,
    save_plot_with_timestamp,
    create_output_directory,
    get_project_root,
    print_tool_header,
    print_completion_message,
    create_color_palette
)


def visualize_lr_schedules():
    """å¯è§†åŒ–ä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å˜åŒ–æ›²çº¿"""
    
    # é…ç½®å‚æ•°
    EPOCHS = 20
    LEARNING_RATE = 0.0001
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ç”¨äºæµ‹è¯•
    model = torch.nn.Linear(10, 1)
    
    # ä¸åŒè°ƒåº¦å™¨é…ç½®
    schedulers_config = {
        'StepLR': {
            'scheduler': lambda opt: optim.lr_scheduler.StepLR(opt, step_size=7, gamma=0.5),
            'color': '#1f77b4',
            'linestyle': '-'
        },
        'MultiStepLR': {
            'scheduler': lambda opt: optim.lr_scheduler.MultiStepLR(opt, milestones=[10, 15], gamma=0.1),
            'color': '#ff7f0e',
            'linestyle': '--'
        },
        'CosineAnnealingLR': {
            'scheduler': lambda opt: optim.lr_scheduler.CosineAnnealingLR(opt, T_max=20, eta_min=1e-6),
            'color': '#2ca02c',
            'linestyle': '-.'
        },
        'ReduceLROnPlateau': {
            'scheduler': lambda opt: optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=3),
            'color': '#d62728',
            'linestyle': ':'
        }
    }
    
    # è®¾ç½®ç»˜å›¾æ ·å¼
    chinese_support = setup_plot_style()
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    for name, config in schedulers_config.items():
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
        scheduler = config['scheduler'](optimizer)
        
        learning_rates = []
        epochs = list(range(EPOCHS))
        
        for epoch in range(EPOCHS):
            # è®°å½•å½“å‰å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            learning_rates.append(current_lr)
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
            if name == 'ReduceLROnPlateau':
                # æ¨¡æ‹ŸéªŒè¯æŸå¤±å˜åŒ–ï¼ˆå‰æœŸä¸‹é™ï¼ŒåæœŸå¹³ç¨³ï¼‰
                if epoch < 5:
                    fake_val_loss = 1.0 - epoch * 0.1
                elif epoch < 10:
                    fake_val_loss = 0.5 - (epoch - 5) * 0.05
                else:
                    fake_val_loss = 0.25 + np.random.normal(0, 0.01)  # æ·»åŠ å™ªå£°æ¨¡æ‹Ÿå¹³ç¨³æœŸ
                scheduler.step(fake_val_loss)
            else:
                scheduler.step()
        
        # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿
        ax.plot(epochs, learning_rates, 
                label=name, 
                color=config['color'], 
                linestyle=config['linestyle'],
                linewidth=2,
                marker='o',
                markersize=4)
    
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Learning Rate', fontsize=12)
    
    if chinese_support:
        ax.set_title('ä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å˜åŒ–æ›²çº¿', fontsize=14, fontweight='bold')
        info_text = ('è¯´æ˜ï¼š\n'
                    'â€¢ StepLR: æ¯7ä¸ªepoché™ä½50%\n'
                    'â€¢ MultiStepLR: ç¬¬10,15ä¸ªepoché™ä½90%\n'
                    'â€¢ CosineAnnealingLR: ä½™å¼¦é€€ç«åˆ°1e-6\n'
                    'â€¢ ReduceLROnPlateau: éªŒè¯æŸå¤±ä¸é™æ—¶å‡åŠ')
    else:
        ax.set_title('Learning Rate Scheduler Comparison', fontsize=14, fontweight='bold')
        info_text = ('Description:\n'
                    'â€¢ StepLR: Reduce by 50% every 7 epochs\n'
                    'â€¢ MultiStepLR: Reduce by 90% at epochs 10,15\n'
                    'â€¢ CosineAnnealingLR: Cosine annealing to 1e-6\n'
                    'â€¢ ReduceLROnPlateau: Reduce when val loss plateaus')
    
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')  # ä½¿ç”¨å¯¹æ•°åæ ‡æ›´å¥½åœ°æ˜¾ç¤ºå˜åŒ–
    
    # æ·»åŠ è¯´æ˜æ–‡æœ¬
    ax.text(0.02, 0.98, info_text,
            transform=ax.transAxes,
            fontsize=9,
            verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    project_root = get_project_root()
    output_dir = create_output_directory(os.path.join(project_root, 'outputs', 'plots'))
    output_path = save_plot_with_timestamp(fig, output_dir, 'lr_schedules_comparison')
    
    return output_path


def print_lr_schedule_info():
    """æ‰“å°å„ç§å­¦ä¹ ç‡è°ƒåº¦å™¨çš„è¯¦ç»†ä¿¡æ¯"""
    print("ğŸ“š å­¦ä¹ ç‡è°ƒåº¦å™¨è¯¦ç»†è¯´æ˜")
    print("=" * 60)
    
    print("\n1. ğŸ“‰ StepLR (é˜¶æ¢¯å¼è¡°å‡)")
    print("   â€¢ ç‰¹ç‚¹: æ¯éš”å›ºå®šepochæ•°é™ä½å­¦ä¹ ç‡")
    print("   â€¢ é€‚ç”¨: è®­ç»ƒç¨³å®šï¼Œéœ€è¦å®šæœŸé™ä½å­¦ä¹ ç‡çš„åœºæ™¯")
    print("   â€¢ å‚æ•°: step_size=7, gamma=0.5 (æ¯7ä¸ªepoché™ä½50%)")
    
    print("\n2. ğŸ“Š MultiStepLR (å¤šé˜¶æ¢¯è¡°å‡)")
    print("   â€¢ ç‰¹ç‚¹: åœ¨æŒ‡å®šçš„epoché™ä½å­¦ä¹ ç‡")
    print("   â€¢ é€‚ç”¨: éœ€è¦åœ¨ç‰¹å®šè®­ç»ƒé˜¶æ®µè°ƒæ•´å­¦ä¹ ç‡")
    print("   â€¢ å‚æ•°: milestones=[10,15], gamma=0.1 (ç¬¬10,15ä¸ªepoché™ä½90%)")
    
    print("\n3. ğŸŒŠ CosineAnnealingLR (ä½™å¼¦é€€ç«)")
    print("   â€¢ ç‰¹ç‚¹: å­¦ä¹ ç‡æŒ‰ä½™å¼¦å‡½æ•°å¹³æ»‘ä¸‹é™")
    print("   â€¢ é€‚ç”¨: éœ€è¦å¹³æ»‘å­¦ä¹ ç‡å˜åŒ–ï¼Œé¿å…çªç„¶è·³è·ƒ")
    print("   â€¢ å‚æ•°: T_max=20, eta_min=1e-6 (20ä¸ªepochå†…é™åˆ°1e-6)")
    print("   â€¢ ä¼˜åŠ¿: å¹³æ»‘å˜åŒ–ï¼Œæœ‰åŠ©äºæ¨¡å‹æ”¶æ•›åˆ°æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜")
    
    print("\n4. ğŸ¯ ReduceLROnPlateau (è‡ªé€‚åº”è¡°å‡)")
    print("   â€¢ ç‰¹ç‚¹: å½“ç›‘æ§æŒ‡æ ‡åœæ­¢æ”¹å–„æ—¶è‡ªåŠ¨é™ä½å­¦ä¹ ç‡")
    print("   â€¢ é€‚ç”¨: ä¸ç¡®å®šä½•æ—¶é™ä½å­¦ä¹ ç‡çš„åœºæ™¯")
    print("   â€¢ å‚æ•°: patience=3, factor=0.5 (è¿ç»­3ä¸ªepochæ— æ”¹å–„æ—¶é™ä½50%)")
    print("   â€¢ ä¼˜åŠ¿: è‡ªé€‚åº”è°ƒæ•´ï¼Œæ— éœ€æ‰‹åŠ¨è®¾å®šæ—¶æœº")
    
    print("\nğŸ’¡ æ¨èä½¿ç”¨åœºæ™¯:")
    print("   â€¢ ğŸ¥‡ CosineAnnealingLR: å¤§å¤šæ•°æƒ…å†µä¸‹çš„é¦–é€‰")
    print("   â€¢ ğŸ¥ˆ ReduceLROnPlateau: ä¸ç¡®å®šè®­ç»ƒç‰¹æ€§æ—¶")
    print("   â€¢ ğŸ¥‰ StepLR: è®­ç»ƒè¿‡ç¨‹å¯é¢„æµ‹æ—¶")


def main():
    """ä¸»å‡½æ•°"""
    print_tool_header("å­¦ä¹ ç‡è°ƒåº¦å™¨å¯è§†åŒ–å·¥å…·", "é¢„è§ˆä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å˜åŒ–æ›²çº¿")
    
    # æ‰“å°è¯¦ç»†ä¿¡æ¯
    print_lr_schedule_info()
    
    print("\nğŸ“ˆ ç”Ÿæˆå­¦ä¹ ç‡å˜åŒ–æ›²çº¿...")
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    try:
        output_path = visualize_lr_schedules()
        if output_path:
            print_completion_message(output_path, "ç°åœ¨ä½ å¯ä»¥æ ¹æ®å›¾è¡¨é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚")
        else:
            print("âŒ å¯è§†åŒ–å¤±è´¥")
    except Exception as e:
        print(f"\nâŒ å¯è§†åŒ–å¤±è´¥: {e}")


if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œé…ç½® - å¯ä»¥åœ¨ PyCharm ä¸­ç›´æ¥ç‚¹å‡»è¿è¡Œ
    """
    main()
