"""
å¤šæ¨¡å‹å›¾åƒåˆ†ç±»è®­ç»ƒè„šæœ¬
æ”¯æŒVGG16ã€ResNetç­‰å¤šç§æ¨¡å‹æ¶æ„
"""
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import time
from tqdm import tqdm
import numpy as np
import yaml
import datetime
import logging
import shutil

# å¯¼å…¥å·¥å‚ç³»ç»Ÿ
from src.models import create_model
from src.training import create_optimizer, create_scheduler, create_loss_function
from src.data.dataset import DataGenerator
from src.utils.visualization import generate_all_plots




def load_config(config_path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return None
    except yaml.YAMLError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return None


def stratified_train_val_split(lines, train_ratio=0.8, random_seed=None, num_classes=None):
    """
    åˆ†å±‚é‡‡æ ·åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­å„ç±»åˆ«çš„æ¯”ä¾‹ä¿æŒä¸€è‡´

    Args:
        lines: æ ‡æ³¨æ•°æ®è¡Œåˆ—è¡¨
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        random_seed: éšæœºç§å­
        num_classes: ç±»åˆ«æ•°é‡ï¼ˆåŠ¨æ€é€‚åº”ï¼‰

    Returns:
        train_lines, val_lines: è®­ç»ƒé›†å’ŒéªŒè¯é›†æ•°æ®
    """
    if random_seed is not None:
        np.random.seed(random_seed)

    # åŠ¨æ€ç¡®å®šç±»åˆ«æ•°é‡
    if num_classes is None:
        # ä»æ•°æ®ä¸­è‡ªåŠ¨æ¨æ–­ç±»åˆ«æ•°é‡
        all_class_ids = set()
        for line in lines:
            class_id = int(line.split(';')[0])
            all_class_ids.add(class_id)
        num_classes = len(all_class_ids)
        print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ° {num_classes} ä¸ªç±»åˆ«: {sorted(all_class_ids)}")

    # åŠ¨æ€åˆ›å»ºç±»åˆ«åˆ†ç»„å­—å…¸
    class_lines = {i: [] for i in range(num_classes)}
    for line in lines:
        class_id = int(line.split(';')[0])
        if class_id < num_classes:  # ç¡®ä¿ç±»åˆ«IDåœ¨æœ‰æ•ˆèŒƒå›´å†…
            class_lines[class_id].append(line)
        else:
            print(f"âš ï¸  è·³è¿‡æ— æ•ˆç±»åˆ«ID: {class_id} (è¶…å‡ºèŒƒå›´ 0-{num_classes-1})")

    # ç»Ÿè®¡åŸå§‹åˆ†å¸ƒ
    total_samples = len(lines)
    print(f"\nğŸ“Š åŸå§‹æ•°æ®åˆ†å¸ƒ:")
    for class_id, class_data in class_lines.items():
        count = len(class_data)
        percentage = count / total_samples * 100 if total_samples > 0 else 0
        print(f"   ç±»åˆ«{class_id}: {count:,}æ ·æœ¬ ({percentage:.1f}%)")

    # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œåˆ†å±‚åˆ’åˆ†
    train_lines = []
    val_lines = []

    print(f"\nğŸ¯ åˆ†å±‚é‡‡æ ·åˆ’åˆ† (è®­ç»ƒé›†{train_ratio*100:.0f}% / éªŒè¯é›†{(1-train_ratio)*100:.0f}%):")

    for class_id, class_data in class_lines.items():
        if len(class_data) == 0:
            print(f"   ç±»åˆ«{class_id}: 0è®­ç»ƒ + 0éªŒè¯ (æ— æ•°æ®)")
            continue

        # æ‰“ä¹±å½“å‰ç±»åˆ«çš„æ•°æ®
        np.random.shuffle(class_data)

        # è®¡ç®—åˆ’åˆ†ç‚¹
        split_point = int(len(class_data) * train_ratio)

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        class_train = class_data[:split_point]
        class_val = class_data[split_point:]

        train_lines.extend(class_train)
        val_lines.extend(class_val)

        print(f"   ç±»åˆ«{class_id}: {len(class_train)}è®­ç»ƒ + {len(class_val)}éªŒè¯")

    # æ‰“ä¹±æœ€ç»ˆçš„è®­ç»ƒé›†å’ŒéªŒè¯é›†
    np.random.shuffle(train_lines)
    np.random.shuffle(val_lines)

    # éªŒè¯åˆ†å±‚æ•ˆæœ
    print(f"\nâœ… åˆ†å±‚é‡‡æ ·ç»“æœéªŒè¯:")

    # åŠ¨æ€åˆ›å»ºç±»åˆ«è®¡æ•°å­—å…¸
    train_class_counts = {i: 0 for i in range(num_classes)}
    for line in train_lines:
        class_id = int(line.split(';')[0])
        if class_id in train_class_counts:
            train_class_counts[class_id] += 1

    # ç»Ÿè®¡éªŒè¯é›†åˆ†å¸ƒ
    val_class_counts = {i: 0 for i in range(num_classes)}
    for line in val_lines:
        class_id = int(line.split(';')[0])
        if class_id in val_class_counts:
            val_class_counts[class_id] += 1

    print(f"   è®­ç»ƒé›†åˆ†å¸ƒ:")
    for class_id, count in train_class_counts.items():
        percentage = count / len(train_lines) * 100 if len(train_lines) > 0 else 0
        print(f"     ç±»åˆ«{class_id}: {count:,}æ ·æœ¬ ({percentage:.1f}%)")

    print(f"   éªŒè¯é›†åˆ†å¸ƒ:")
    for class_id, count in val_class_counts.items():
        percentage = count / len(val_lines) * 100 if len(val_lines) > 0 else 0
        print(f"     ç±»åˆ«{class_id}: {count:,}æ ·æœ¬ ({percentage:.1f}%)")

    if random_seed is not None:
        np.random.seed(None)  # é‡ç½®éšæœºç§å­

    return train_lines, val_lines


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""

    # ==================== åŠ è½½é…ç½®æ–‡ä»¶ ====================
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    config_path = os.path.join(project_root, 'configs', 'training_config.yaml')

    print(f"ğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    config = load_config(config_path)

    if config is None:
        print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
        return

    # ==================== è§£æé…ç½®å‚æ•° ====================
    # è®­ç»ƒå‚æ•°
    EPOCHS = config['training']['epochs']
    BATCH_SIZE = config['dataloader']['batch_size']
    LEARNING_RATE = config['training']['learning_rate']
    NUM_CLASSES = config['data']['num_classes']
    INPUT_SHAPE = config['data']['input_shape']

    # æ•°æ®é›†é…ç½®
    TRAIN_VAL_SPLIT = config['data']['train_val_split']
    RANDOM_SEED = config['data']['random_seed']
    STRATIFIED_SPLIT = config['data']['stratified_split']

    # æ•°æ®åŠ è½½å™¨é…ç½®
    NUM_WORKERS = config['dataloader']['num_workers']
    SHUFFLE_TRAIN = config['dataloader']['shuffle']



    # æ•°æ®å¢å¼ºé…ç½®
    aug_config = config['augmentation']

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é¢„è®¾é…ç½®
    preset_name = aug_config.get('use_preset', None)
    if preset_name and preset_name in aug_config.get('presets', {}):
        print(f"ğŸ¨ ä½¿ç”¨æ•°æ®å¢å¼ºé¢„è®¾: {preset_name}")
        preset_config = aug_config['presets'][preset_name]
        # åˆå¹¶é¢„è®¾é…ç½®å’Œå½“å‰é…ç½®ï¼Œé¢„è®¾ä¼˜å…ˆ
        merged_config = {**aug_config, **preset_config}
        aug_config = merged_config

    AUGMENTATION_CONFIG = {
        'enable_flip': aug_config.get('enable_flip', True),
        'enable_rotation': aug_config.get('enable_rotation', True),
        'enable_color_jitter': aug_config.get('enable_color_jitter', True),
        'enable_scale_jitter': aug_config.get('enable_scale_jitter', True),
        'jitter': aug_config.get('jitter', 0.3),
        'scale_range': tuple(aug_config.get('scale_range', [0.75, 1.25])),
        'rotation_range': aug_config.get('rotation_range', 15),
        'hue_range': aug_config.get('hue_range', 0.1),
        'saturation_range': aug_config.get('saturation_range', 1.5),
        'value_range': aug_config.get('value_range', 1.5),
        'flip_probability': aug_config.get('flip_probability', 0.5),
        'rotation_probability': aug_config.get('rotation_probability', 0.5),
    }

    # è·¯å¾„é…ç½®
    ANNOTATION_PATH = config['data']['annotation_file']
    CHECKPOINT_DIR = config['save']['checkpoint_dir']

    # ä¿å­˜é…ç½®
    SAVE_BEST_ONLY = config['save']['save_best_only']
    SAVE_FREQUENCY = config['save']['save_frequency']
    SAVE_CHECKPOINT_EVERY_EPOCH = config['save']['save_checkpoint_every_epoch']

    # æ—¥å¿—é…ç½®
    VERBOSE = config['logging']['verbose']

    # æ•°æ®é…ç½®
    DATA_DIR = config['data']['data_dir']
    CLASS_NAMES = config['data']['class_names']

    # ==================== è·¯å¾„å¤„ç† ====================
    # åˆ›å»ºæ—¶é—´æˆ³ç”¨äºåŒºåˆ†ä¸åŒè®­ç»ƒ
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    annotation_path = os.path.join(project_root, ANNOTATION_PATH)

    # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•ç»“æ„
    base_checkpoint_dir = os.path.join(project_root, CHECKPOINT_DIR)
    base_log_dir = os.path.join(project_root, config['logging']['log_dir'])
    base_plot_dir = os.path.join(project_root, config['logging']['plot_dir'])

    # æ—¶é—´æˆ³ç›®å½•
    checkpoint_dir = os.path.join(base_checkpoint_dir, f"train_{timestamp}")
    log_dir = os.path.join(base_log_dir, f"train_{timestamp}")
    plot_dir = os.path.join(base_plot_dir, f"train_{timestamp}")

    # latestç›®å½•ï¼ˆç”¨äºè„šæœ¬è‡ªåŠ¨è°ƒç”¨ï¼‰
    latest_checkpoint_dir = os.path.join(base_checkpoint_dir, "latest")
    latest_log_dir = os.path.join(base_log_dir, "latest")
    latest_plot_dir = os.path.join(base_plot_dir, "latest")

    if VERBOSE:
        print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
        print(f"ğŸ“„ æ ‡æ³¨æ–‡ä»¶: {annotation_path}")
        print(f"ğŸ¤– ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹: {'æ˜¯' if config['model']['pretrained'] else 'å¦'}")
        print(f"â° è®­ç»ƒæ—¶é—´æˆ³: {timestamp}")
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
        print(f"ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")
        print(f"ğŸ“Š å›¾è¡¨ç›®å½•: {plot_dir}")

    # ==================== åˆ›å»ºç›®å½•å’Œè®¾ç½®æ—¥å¿— ====================
    # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
    for directory in [checkpoint_dir, log_dir, plot_dir, latest_checkpoint_dir, latest_log_dir, latest_plot_dir]:
        os.makedirs(directory, exist_ok=True)

    # å¤‡ä»½é…ç½®æ–‡ä»¶åˆ°æ—¥å¿—ç›®å½•
    config_backup_path = os.path.join(log_dir, "config_backup.yaml")
    config_backup_latest_path = os.path.join(latest_log_dir, "config_backup.yaml")
    shutil.copy2(config_path, config_backup_path)
    shutil.copy2(config_path, config_backup_latest_path)

    # è®¾ç½®æ—¥å¿—è®°å½•
    log_file_path = os.path.join(log_dir, "training.log")
    log_file_latest_path = os.path.join(latest_log_dir, "training.log")

    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file_path, encoding='utf-8'),
            logging.FileHandler(log_file_latest_path, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )

    logger = logging.getLogger(__name__)

    # è®°å½•è®­ç»ƒå¼€å§‹ä¿¡æ¯
    logger.info("="*80)
    logger.info("ğŸš€ å¼€å§‹æ–°çš„è®­ç»ƒä»»åŠ¡")
    logger.info("="*80)
    logger.info(f"è®­ç»ƒæ—¶é—´æˆ³: {timestamp}")
    logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    logger.info(f"é…ç½®æ–‡ä»¶: {config_path}")
    logger.info(f"é…ç½®å¤‡ä»½: {config_backup_path}")
    logger.info(f"æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
    logger.info(f"æ—¥å¿—ç›®å½•: {log_dir}")
    logger.info(f"å›¾è¡¨ç›®å½•: {plot_dir}")

    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file_path}")
    print(f"ğŸ“‹ é…ç½®å¤‡ä»½: {config_backup_path}")

    # ==================== æ•°æ®é›†å‡†å¤‡ ====================
    # å¦‚æœæ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ—§ä½ç½®è¯»å–
    if not os.path.exists(annotation_path):
        old_annotation_path = os.path.join(project_root, 'cls_train.txt')
        if os.path.exists(old_annotation_path):
            annotation_path = old_annotation_path
            print(f"âš ï¸  ä½¿ç”¨æ—§æ ‡æ³¨æ–‡ä»¶: {annotation_path}")
        else:
            print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {annotation_path}")
            print("è¯·å…ˆè¿è¡Œ scripts/generate_annotations.py ç”Ÿæˆæ ‡æ³¨æ–‡ä»¶")
            return
    
    with open(annotation_path, 'r') as f:
        lines = f.readlines()

    # æ•°æ®é›†åˆ’åˆ†
    if STRATIFIED_SPLIT:
        print(f"\nğŸ¯ ä½¿ç”¨åˆ†å±‚é‡‡æ ·åˆ’åˆ†æ•°æ®é›†...")
        train_lines, val_lines = stratified_train_val_split(
            lines,
            train_ratio=TRAIN_VAL_SPLIT,
            random_seed=RANDOM_SEED,
            num_classes=NUM_CLASSES
        )
    else:
        print(f"\nğŸ“Š ä½¿ç”¨éšæœºåˆ’åˆ†æ•°æ®é›†...")
        np.random.seed(RANDOM_SEED)  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„éšæœºç§å­
        np.random.shuffle(lines)  # æ•°æ®æ‰“ä¹±
        np.random.seed(None)

        # éšæœºåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_lines = lines[:int(len(lines) * TRAIN_VAL_SPLIT)]
        val_lines = lines[int(len(lines) * TRAIN_VAL_SPLIT):]

        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_lines):,}")
        print(f"   éªŒè¯æ ·æœ¬: {len(val_lines):,}")

    # æ›´æ–°æ ·æœ¬æ•°é‡
    num_train = len(train_lines)
    num_val = len(val_lines)

    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(lines)}")
    print(f"   è®­ç»ƒæ ·æœ¬: {num_train}")
    print(f"   éªŒè¯æ ·æœ¬: {num_val}")
    print(f"   è®­ç»ƒ/éªŒè¯æ¯”ä¾‹: {num_train/len(lines):.1%}/{num_val/len(lines):.1%}")

    # è®°å½•æ•°æ®é›†ä¿¡æ¯åˆ°æ—¥å¿—
    logger.info("="*50)
    logger.info("ğŸ“Š æ•°æ®é›†ä¿¡æ¯")
    logger.info("="*50)
    logger.info(f"æ ‡æ³¨æ–‡ä»¶: {annotation_path}")
    logger.info(f"æ€»æ ·æœ¬æ•°: {len(lines)}")
    logger.info(f"è®­ç»ƒæ ·æœ¬: {num_train}")
    logger.info(f"éªŒè¯æ ·æœ¬: {num_val}")
    logger.info(f"è®­ç»ƒ/éªŒè¯æ¯”ä¾‹: {num_train/len(lines):.1%}/{num_val/len(lines):.1%}")

    # åˆ›å»ºæ•°æ®é›†
    train_data = DataGenerator(train_lines, INPUT_SHAPE, True, AUGMENTATION_CONFIG)
    val_data = DataGenerator(val_lines, INPUT_SHAPE, False, None)  # éªŒè¯é›†ä¸ä½¿ç”¨æ•°æ®å¢å¼º
    val_len = len(val_data)

    # ==================== æ•°æ®åŠ è½½å™¨ ====================
    gen_train = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE_TRAIN, num_workers=NUM_WORKERS)
    gen_test = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)

    print(f"   Batch Size: {BATCH_SIZE}")
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(gen_train)}")
    print(f"   éªŒè¯æ‰¹æ¬¡æ•°: {len(gen_test)}")

    # ==================== æ„å»ºç½‘ç»œ ====================
    device = torch.device('cuda' if torch.cuda.is_available() else "cpu")
    print(f"\nğŸ–¥ï¸  è®¾å¤‡ä¿¡æ¯:")
    print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"   GPUåç§°: {torch.cuda.get_device_name(0)}")
        print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    # è®°å½•è®¾å¤‡ä¿¡æ¯åˆ°æ—¥å¿—
    logger.info("="*50)
    logger.info("ğŸ–¥ï¸ è®¾å¤‡ä¿¡æ¯")
    logger.info("="*50)
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        logger.info(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    print(f"\nğŸ—ï¸  æ„å»ºç½‘ç»œ:")

    # ä½¿ç”¨æ¨¡å‹å·¥å‚åˆ›å»ºæ¨¡å‹
    model_config = config['model'].copy()
    model_config['num_classes'] = NUM_CLASSES
    net = create_model(model_config)
    net.to(device)

    # è·å–æ¨¡å‹ä¿¡æ¯
    if hasattr(net, 'get_parameter_count'):
        param_info = net.get_parameter_count()
        total_params = param_info['total']
        trainable_params = param_info['trainable']
    else:
        total_params = sum(p.numel() for p in net.parameters())
        trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)

    # è·å–æ¨¡å‹åç§°
    model_name = net.get_name() if hasattr(net, 'get_name') else config['model']['name'].upper()

    # è®°å½•æ¨¡å‹ä¿¡æ¯åˆ°æ—¥å¿—
    logger.info("="*50)
    logger.info("ğŸ—ï¸ æ¨¡å‹ä¿¡æ¯")
    logger.info("="*50)
    logger.info(f"æ¨¡å‹åç§°: {model_name}")
    logger.info(f"ç±»åˆ«æ•°é‡: {NUM_CLASSES}")
    logger.info(f"ä½¿ç”¨é¢„è®­ç»ƒ: {config['model']['pretrained']}")
    logger.info(f"æ¨¡å‹å‚æ•°æ€»é‡: {total_params:,}")
    logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # ==================== ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ ====================
    # ä½¿ç”¨ä¼˜åŒ–å™¨å·¥å‚åˆ›å»ºä¼˜åŒ–å™¨
    optimizer_config = config['training']['optimizer'].copy()
    optimizer_config['params']['lr'] = LEARNING_RATE  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å­¦ä¹ ç‡
    optim = create_optimizer(net, optimizer_config)

    # ä½¿ç”¨è°ƒåº¦å™¨å·¥å‚åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_config = config['training']['scheduler']
    scheduler = create_scheduler(optim, scheduler_config, EPOCHS)

    print(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
    print(f"   ä¼˜åŒ–å™¨: {config['training']['optimizer']['name']}")
    print(f"   åˆå§‹å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"   è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"   æŸå¤±å‡½æ•°: {config['training']['loss_function']['name']}")
    print(f"   ä¿å­˜æœ€ä½³æ¨¡å‹: {'æ˜¯' if SAVE_BEST_ONLY else 'å¦'}")
    print(f"   ä¿å­˜é¢‘ç‡: æ¯{SAVE_FREQUENCY}ä¸ªepoch" if not SAVE_CHECKPOINT_EVERY_EPOCH else "æ¯ä¸ªepoch")
    if scheduler is not None:
        print(f"   å­¦ä¹ ç‡è°ƒåº¦å™¨: {config['training']['scheduler']['name']}")
    else:
        print(f"   å­¦ä¹ ç‡è°ƒåº¦å™¨: æ— ")

    print(f"\nğŸ¨ æ•°æ®å¢å¼ºé…ç½®:")
    print(f"   æ°´å¹³ç¿»è½¬: {'âœ…' if AUGMENTATION_CONFIG['enable_flip'] else 'âŒ'}")
    print(f"   éšæœºæ—‹è½¬: {'âœ…' if AUGMENTATION_CONFIG['enable_rotation'] else 'âŒ'} (Â±{AUGMENTATION_CONFIG['rotation_range']}Â°)")
    print(f"   è‰²å½©æŠ–åŠ¨: {'âœ…' if AUGMENTATION_CONFIG['enable_color_jitter'] else 'âŒ'}")
    print(f"   å°ºåº¦æŠ–åŠ¨: {'âœ…' if AUGMENTATION_CONFIG['enable_scale_jitter'] else 'âŒ'} ({AUGMENTATION_CONFIG['scale_range']})")
    print(f"   é•¿å®½æ¯”æŠ–åŠ¨: {AUGMENTATION_CONFIG['jitter']}")
    print(f"   ç¿»è½¬æ¦‚ç‡: {AUGMENTATION_CONFIG['flip_probability']}")
    print(f"   æ—‹è½¬æ¦‚ç‡: {AUGMENTATION_CONFIG['rotation_probability']}")

    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ (å…± {EPOCHS} ä¸ª epoch)")
    print("="*80)

    # è®°å½•è®­ç»ƒå¼€å§‹
    logger.info("="*80)
    logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ (å…± {EPOCHS} ä¸ª epoch)")
    logger.info("="*80)

    # ç›®å½•å·²åœ¨å‰é¢åˆ›å»ºï¼Œè®°å½•è®­ç»ƒé…ç½®åˆ°æ—¥å¿—
    logger.info("="*50)
    logger.info("ğŸ“Š è®­ç»ƒé…ç½®å‚æ•°")
    logger.info("="*50)
    logger.info(f"è®­ç»ƒè½®æ•°: {EPOCHS}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    logger.info(f"å­¦ä¹ ç‡: {LEARNING_RATE}")
    logger.info(f"ç±»åˆ«æ•°é‡: {NUM_CLASSES}")
    logger.info(f"è¾“å…¥å°ºå¯¸: {INPUT_SHAPE}")
    logger.info(f"æ¨¡å‹ç±»å‹: {config['model']['name']}")
    logger.info(f"ä½¿ç”¨é¢„è®­ç»ƒ: {config['model']['pretrained']}")
    logger.info(f"ä¼˜åŒ–å™¨: {config['training']['optimizer']['name']}")
    logger.info(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {config['training']['scheduler']['name']}")
    logger.info(f"æ•°æ®å¢å¼ºé…ç½®: {AUGMENTATION_CONFIG is not None}")

    # è®­ç»ƒå†å²è®°å½•
    train_losses = []
    val_losses = []
    train_accuracies = []  # æ–°å¢ï¼šè®°å½•è®­ç»ƒå‡†ç¡®ç‡
    val_accuracies = []
    best_acc = 0.0

    # ==================== åˆ›å»ºæŸå¤±å‡½æ•° ====================
    # ä½¿ç”¨æŸå¤±å‡½æ•°å·¥å‚åˆ›å»ºæŸå¤±å‡½æ•°
    loss_config = config['training']['loss_function']
    criterion = create_loss_function(loss_config, train_lines, NUM_CLASSES)
    criterion.to(device)

    # è®°å½•æŸå¤±å‡½æ•°ä¿¡æ¯åˆ°æ—¥å¿—
    logger.info("="*50)
    logger.info("ğŸ¯ æŸå¤±å‡½æ•°é…ç½®")
    logger.info("="*50)
    logger.info(f"æŸå¤±å‡½æ•°ç±»å‹: {config['training']['loss_function']['name']}")
    logger.info(f"åˆ†å±‚é‡‡æ ·: {'å¯ç”¨' if STRATIFIED_SPLIT else 'æœªå¯ç”¨'}")

    # ==================== è®­ç»ƒå¾ªç¯ ====================
    for epoch in range(EPOCHS):
        epoch_start_time = time.time()

        # ==================== è®­ç»ƒé˜¶æ®µ ====================
        net.train()
        total_train_loss = 0.0
        train_correct = 0
        train_total = 0

        # è®­ç»ƒè¿›åº¦æ¡
        train_pbar = tqdm(gen_train, desc=f'Epoch {epoch+1}/{EPOCHS} [Train]',
                         ncols=100, leave=False)

        for batch_idx, (img, label, _) in enumerate(train_pbar):
            img = img.to(device)
            label = label.to(device)

            optim.zero_grad()
            output = net(img)
            train_loss = criterion(output, label)  # ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°
            train_loss.backward()
            optim.step()

            # ç»Ÿè®¡
            total_train_loss += train_loss.item()
            _, predicted = output.max(1)
            train_total += label.size(0)
            train_correct += predicted.eq(label).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            current_loss = total_train_loss / (batch_idx + 1)
            current_acc = 100. * train_correct / train_total
            train_pbar.set_postfix({
                'Loss': f'{current_loss:.4f}',
                'Acc': f'{current_acc:.2f}%'
            })

        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step()
        current_lr = optim.param_groups[0]['lr']

        # ==================== éªŒè¯é˜¶æ®µ ====================
        net.eval()
        total_val_loss = 0.0
        val_correct = 0
        val_total = 0

        # éªŒè¯è¿›åº¦æ¡
        val_pbar = tqdm(gen_test, desc=f'Epoch {epoch+1}/{EPOCHS} [Val]  ',
                       ncols=100, leave=False)

        with torch.no_grad():
            for batch_idx, (img, label, _) in enumerate(val_pbar):
                img = img.to(device)
                label = label.to(device)

                output = net(img)
                val_loss = criterion(output, label)  # ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°

                # ç»Ÿè®¡
                total_val_loss += val_loss.item()
                _, predicted = output.max(1)
                val_total += label.size(0)
                val_correct += predicted.eq(label).sum().item()

                # æ›´æ–°è¿›åº¦æ¡
                current_loss = total_val_loss / (batch_idx + 1)
                current_acc = 100. * val_correct / val_total
                val_pbar.set_postfix({
                    'Loss': f'{current_loss:.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })

        # è®¡ç®—å¹³å‡å€¼
        avg_train_loss = total_train_loss / len(gen_train)
        avg_val_loss = total_val_loss / len(gen_test)
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total

        # è®°å½•å†å²
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        train_accuracies.append(train_acc)  # æ–°å¢ï¼šè®°å½•è®­ç»ƒå‡†ç¡®ç‡
        val_accuracies.append(val_acc)

        # ReduceLROnPlateau éœ€è¦åœ¨éªŒè¯åè°ƒç”¨
        if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(avg_val_loss)
            # æ›´æ–°å­¦ä¹ ç‡ï¼ˆå¯èƒ½åœ¨stepåå‘ç”Ÿå˜åŒ–ï¼‰
            current_lr = optim.param_groups[0]['lr']

        # è®¡ç®—è®­ç»ƒæ—¶é—´
        epoch_time = time.time() - epoch_start_time

        # æ‰“å°epochæ€»ç»“
        print(f"\nEpoch {epoch+1}/{EPOCHS} å®Œæˆ:")
        print(f"  è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"  éªŒè¯ - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%")
        print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
        print(f"  ç”¨æ—¶: {epoch_time:.1f}s")

        # è®°å½•epochç»“æœåˆ°æ—¥å¿—
        logger.info(f"Epoch {epoch+1}/{EPOCHS} å®Œæˆ:")
        logger.info(f"  è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%")
        logger.info(f"  éªŒè¯ - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%")
        logger.info(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
        logger.info(f"  ç”¨æ—¶: {epoch_time:.1f}s")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            if SAVE_BEST_ONLY:  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜æœ€ä½³æ¨¡å‹
                # ä¿å­˜åˆ°æ—¶é—´æˆ³ç›®å½•
                torch.save(net.state_dict(), os.path.join(checkpoint_dir, "best_model.pth"))
                # åŒæ—¶ä¿å­˜åˆ°latestç›®å½•ï¼ˆç”¨äºè¯„ä¼°è„šæœ¬ï¼‰
                torch.save(net.state_dict(), os.path.join(latest_checkpoint_dir, "best_model.pth"))
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}% (æ¨¡å‹å·²ä¿å­˜)")
                logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}% (æ¨¡å‹å·²ä¿å­˜)")
            else:
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")
                logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")

        # æ ¹æ®é…ç½®ä¿å­˜æ£€æŸ¥ç‚¹
        should_save_checkpoint = False
        if SAVE_CHECKPOINT_EVERY_EPOCH:
            should_save_checkpoint = True
        elif (epoch + 1) % SAVE_FREQUENCY == 0:
            should_save_checkpoint = True

        if should_save_checkpoint:
            checkpoint_data = {
                'epoch': epoch + 1,
                'model_state_dict': net.state_dict(),
                'optimizer_state_dict': optim.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'val_acc': val_acc,
                'best_acc': best_acc,
                'scheduler_type': config['training']['scheduler']['name']
            }

            # åªæœ‰å½“è°ƒåº¦å™¨å­˜åœ¨æ—¶æ‰ä¿å­˜å…¶çŠ¶æ€
            if scheduler is not None:
                checkpoint_data['scheduler_state_dict'] = scheduler.state_dict()

            # ä¿å­˜æ£€æŸ¥ç‚¹åˆ°æ—¶é—´æˆ³ç›®å½•
            torch.save(checkpoint_data, os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch+1}.pth"))
            # åŒæ—¶ä¿å­˜åˆ°latestç›®å½•
            torch.save(checkpoint_data, os.path.join(latest_checkpoint_dir, f"checkpoint_epoch_{epoch+1}.pth"))

        print("-" * 80)

    print(f"\nğŸ¯ è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {checkpoint_dir}/")
    print("="*80)

    # ==================== è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ====================
    print(f"\nğŸ“Š å¼€å§‹ç”Ÿæˆè®­ç»ƒåˆ†æå›¾è¡¨...")
    try:
        # ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨
        evaluation_results = generate_all_plots(
            train_losses=train_losses,
            val_losses=val_losses,
            train_accuracies=train_accuracies,
            val_accuracies=val_accuracies,
            model=net,
            dataloader=gen_test,
            device=device,
            class_names=CLASS_NAMES,
            plot_dir=plot_dir,
            latest_plot_dir=latest_plot_dir
        )

        print(f"âœ… æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“ å›¾è¡¨ä¿å­˜ä½ç½®:")
        print(f"   ğŸ“Š æ—¶é—´æˆ³ç›®å½•: {plot_dir}")
        print(f"   ğŸ“Š æœ€æ–°ç›®å½•: {latest_plot_dir}")

        # è®°å½•å›¾è¡¨ç”Ÿæˆä¿¡æ¯åˆ°æ—¥å¿—
        logger.info("ğŸ“Š å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
        logger.info(f"å›¾è¡¨ä¿å­˜ä½ç½®: {plot_dir}")
        logger.info(f"æœ€æ–°å›¾è¡¨ä½ç½®: {latest_plot_dir}")

    except Exception as e:
        print(f"âš ï¸  å›¾è¡¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        logger.error(f"å›¾è¡¨ç”Ÿæˆé”™è¯¯: {e}")
        print("è®­ç»ƒå·²å®Œæˆï¼Œä½†å›¾è¡¨ç”Ÿæˆå¤±è´¥ã€‚å¯ä»¥ç¨åæ‰‹åŠ¨è¿è¡Œå¯è§†åŒ–è„šæœ¬ã€‚")

    # è®°å½•è®­ç»ƒå®Œæˆä¿¡æ¯
    training_end_time = datetime.datetime.now()
    total_training_time = training_end_time - datetime.datetime.strptime(timestamp, "%Y%m%d_%H%M%S")

    logger.info("="*80)
    logger.info("ğŸ¯ è®­ç»ƒå®Œæˆ!")
    logger.info("="*80)
    logger.info(f"æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")
    logger.info(f"è®­ç»ƒå¼€å§‹æ—¶é—´: {timestamp}")
    logger.info(f"è®­ç»ƒç»“æŸæ—¶é—´: {training_end_time.strftime('%Y%m%d_%H%M%S')}")
    logger.info(f"æ€»è®­ç»ƒæ—¶é•¿: {total_training_time}")
    logger.info(f"æ¨¡å‹ä¿å­˜ä½ç½®: {checkpoint_dir}/")
    logger.info(f"æ—¥å¿—ä¿å­˜ä½ç½®: {log_dir}/")
    logger.info(f"é…ç½®å¤‡ä»½ä½ç½®: {config_backup_path}")
    logger.info("="*80)


if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œé…ç½® - å¯ä»¥åœ¨ PyCharm ä¸­ç›´æ¥ç‚¹å‡»è¿è¡Œ
    """
    main()
