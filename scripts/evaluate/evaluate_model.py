"""
æ¨¡å‹è¯„ä¼°ä¸»è„šæœ¬
å¯¹è®­ç»ƒå¥½çš„æ··å‡åœŸåè½åº¦åˆ†ç±»æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°
æ”¯æŒæŒ‡å®šè®­ç»ƒè½®æ¬¡è·¯å¾„è¿›è¡Œè¯„ä¼°
"""

import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

import torch
import torch.nn.functional as F
import numpy as np
import yaml
import datetime
from torch.utils.data import DataLoader
from tqdm import tqdm

# å¯¼å…¥å·¥å‚ç³»ç»Ÿ
from src.models import create_model
from src.data.dataset import DataGenerator
from src.evaluation import ReportGenerator
from src.utils.font_manager import setup_matplotlib_font, get_font_manager
from scripts.evaluate.evaluate_utils import (
    get_corresponding_model_path,
    extract_timestamp_from_path,
    generate_eval_timestamp,
    print_path_info,
    load_training_config
)



def load_config(config_path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return None
    except yaml.YAMLError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return None


def load_model(model_path, config, device='cpu'):
    """
    æ ¹æ®é…ç½®æ–‡ä»¶åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹

    Args:
        model_path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾„
        config (dict): è®­ç»ƒé…ç½®æ–‡ä»¶
        device (str): è®¾å¤‡ç±»å‹

    Returns:
        torch.nn.Module: åŠ è½½çš„æ¨¡å‹
    """
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")

    # ä»é…ç½®æ–‡ä»¶è·å–æ¨¡å‹ä¿¡æ¯
    model_config = config['model'].copy()
    model_config['num_classes'] = config['data']['num_classes']
    model_config['pretrained'] = False  # è¯„ä¼°æ—¶ä¸éœ€è¦é¢„è®­ç»ƒæƒé‡

    print(f"ğŸ¤– æ¨¡å‹ç±»å‹: {model_config['name']}")
    print(f"ğŸ“Š åˆ†ç±»æ•°é‡: {model_config['num_classes']}")

    # ä½¿ç”¨å·¥å‚ç³»ç»Ÿåˆ›å»ºæ¨¡å‹
    try:
        model = create_model(model_config)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ: {model.get_name() if hasattr(model, 'get_name') else model_config['name']}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return None

    # åŠ è½½æƒé‡
    try:
        if device == 'cpu':
            state_dict = torch.load(model_path, map_location='cpu')
        else:
            state_dict = torch.load(model_path)

        # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
        if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:
            # æ£€æŸ¥ç‚¹æ ¼å¼
            model.load_state_dict(state_dict['model_state_dict'])
            print(f"ğŸ“‹ åŠ è½½æ£€æŸ¥ç‚¹æ ¼å¼æƒé‡")
        else:
            # ç›´æ¥æ¨¡å‹æƒé‡æ ¼å¼
            model.load_state_dict(state_dict)
            print(f"ğŸ“‹ åŠ è½½ç›´æ¥æƒé‡æ ¼å¼")

        model.to(device)
        model.eval()

        print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        return model

    except Exception as e:
        print(f"âŒ æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e}")
        return None


def create_evaluation_dataloader(config, project_root):
    """
    åˆ›å»ºè¯„ä¼°æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    
    Args:
        config (dict): é…ç½®å­—å…¸
        project_root (str): é¡¹ç›®æ ¹ç›®å½•
        
    Returns:
        DataLoader: è¯„ä¼°æ•°æ®åŠ è½½å™¨
    """
    # ä½¿ç”¨æµ‹è¯•é›†æ ‡æ³¨æ–‡ä»¶è¿›è¡Œè¯„ä¼°
    # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœé…ç½®ä¸­æ²¡æœ‰test_annotation_fileï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
    if 'test_annotation_file' in config['data']:
        annotation_file = config['data']['test_annotation_file']
        dataset_type = "æµ‹è¯•é›†"
        print(f"ğŸ“Š è¯„ä¼°æ¨¡å¼: ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œæœ€ç»ˆæ€§èƒ½è¯„ä¼°")
    else:
        # å…¼å®¹æ—§é…ç½®ï¼šå›é€€åˆ°é»˜è®¤æµ‹è¯•é›†è·¯å¾„
        annotation_file = "data/annotations/cls_test.txt"
        dataset_type = "æµ‹è¯•é›†ï¼ˆé»˜è®¤è·¯å¾„ï¼‰"
        print(f"ğŸ“Š è¯„ä¼°æ¨¡å¼: ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œæœ€ç»ˆæ€§èƒ½è¯„ä¼°ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰")
        print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°test_annotation_fileï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„")
    
    annotation_path = os.path.join(project_root, annotation_file)
    print(f"ğŸ“ è¯„ä¼°æ•°æ®é›†: {dataset_type}")
    print(f"ğŸ“„ æ ‡æ³¨æ–‡ä»¶: {annotation_path}")
    
    if not os.path.exists(annotation_path):
        print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {annotation_path}")
        return None
    
    # è¯»å–æ ‡æ³¨æ–‡ä»¶
    with open(annotation_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    lines = [line.strip() for line in lines if line.strip()]
    
    if len(lines) == 0:
        print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸ºç©º: {annotation_path}")
        return None
    
    print(f"ğŸ“Š åŠ è½½æ ‡æ³¨æ–‡ä»¶: {annotation_path}")
    print(f"   æ€»æ ·æœ¬æ•°: {len(lines)}")
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆè¯„ä¼°æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼ºï¼‰
    input_shape = config['data']['input_shape']
    dataset = DataGenerator(lines, input_shape, random=False)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    batch_size = config['dataloader']['batch_size']
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,  # è¯„ä¼°æ—¶ä¸æ‰“ä¹±
        num_workers=0,  # è¯„ä¼°æ—¶ä½¿ç”¨å•çº¿ç¨‹
        drop_last=False
    )
    
    return dataloader


def evaluate_model(model, dataloader, device='cpu'):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        model (torch.nn.Module): æ¨¡å‹
        dataloader (DataLoader): æ•°æ®åŠ è½½å™¨
        device (str): è®¾å¤‡ç±»å‹
        
    Returns:
        tuple: (y_true, y_pred, y_prob, image_paths)
    """
    model.eval()
    
    y_true = []
    y_pred = []
    y_prob = []
    image_paths = []
    
    print("ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    
    with torch.no_grad():
        for batch_idx, (images, labels, paths) in enumerate(tqdm(dataloader, desc="è¯„ä¼°è¿›åº¦")):
            images = images.to(device)
            labels = labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            probabilities = F.softmax(outputs, dim=1)
            
            # è·å–é¢„æµ‹ç»“æœ
            _, predicted = torch.max(outputs.data, 1)
            
            # æ”¶é›†ç»“æœ
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
            y_prob.extend(probabilities.cpu().numpy())
            image_paths.extend(paths)
    
    return np.array(y_true), np.array(y_pred), np.array(y_prob), image_paths


def load_training_config(train_path, project_root):
    """
    åŠ è½½è®­ç»ƒæ—¶çš„é…ç½®æ–‡ä»¶

    Args:
        train_path (str): è®­ç»ƒè·¯å¾„
        project_root (str): é¡¹ç›®æ ¹ç›®å½•

    Returns:
        tuple: (config_dict, config_source)
    """
    config = None
    config_source = None

    if train_path:
        # å°è¯•ä»è®­ç»ƒè·¯å¾„åŠ è½½é…ç½®å¤‡ä»½
        config_backup_path = os.path.join(train_path, 'config_backup.yaml')
        if os.path.exists(config_backup_path):
            try:
                with open(config_backup_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                config_source = f"è®­ç»ƒé…ç½®å¤‡ä»½: {config_backup_path}"
                print(f"âœ… ä»è®­ç»ƒé…ç½®å¤‡ä»½åŠ è½½: {config_backup_path}")
            except Exception as e:
                print(f"âš ï¸  é…ç½®å¤‡ä»½åŠ è½½å¤±è´¥: {e}")

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é…ç½®å¤‡ä»½ï¼Œä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶
    if config is None:
        default_config_path = os.path.join(project_root, 'configs', 'training_config.yaml')
        if os.path.exists(default_config_path):
            try:
                with open(default_config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                config_source = f"é»˜è®¤é…ç½®æ–‡ä»¶: {default_config_path}"
                print(f"âœ… ä»é»˜è®¤é…ç½®æ–‡ä»¶åŠ è½½: {default_config_path}")
            except Exception as e:
                print(f"âŒ é»˜è®¤é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                return None, None

    if config:
        print(f"ğŸ“‹ é…ç½®æ¥æº: {config_source}")
        print(f"ğŸ¤– æ¨¡å‹ç±»å‹: {config['model']['name']}")
        print(f"ğŸ“Š åˆ†ç±»æ•°é‡: {config['data']['num_classes']}")
        print(f"ğŸ“ è¾“å…¥å°ºå¯¸: {config['data']['input_shape']}")

    return config, config_source


def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""

    # ==================== é…ç½®å‚æ•° ====================
    # ğŸ¯ æŒ‡å®šè¦è¯„ä¼°çš„è®­ç»ƒè½®æ¬¡è·¯å¾„ï¼ˆåœ¨è¿™é‡Œä¿®æ”¹ï¼‰
    # è®¾ç½®ä¸ºå…·ä½“è·¯å¾„æ¥è¯„ä¼°æŒ‡å®šè½®æ¬¡ï¼Œè®¾ç½®ä¸º None ä½¿ç”¨é»˜è®¤çš„ latest æ¨¡å‹
    SPECIFIC_TRAIN_PATH = "/var/yjs/zes/vgg/outputs/logs/train_20250914_112502"

    # SPECIFIC_TRAIN_PATH = None  # ä½¿ç”¨é»˜è®¤çš„ latest æ¨¡å‹

    # ==================== å­—ä½“è®¾ç½® ====================
    print("ğŸ”¤ è®¾ç½®å­—ä½“...")
    setup_matplotlib_font()
    font_manager = get_font_manager()
    font_manager.print_font_status()

    # ==================== è·¯å¾„è®¾ç½® ====================
    # è·å–é¡¹ç›®è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(script_dir))  # ä¸Šä¸¤çº§ç›®å½•
    
    # ä¿®å¤è·¯å¾„é—®é¢˜ï¼šå¦‚æœè·¯å¾„ä»¥ /root å¼€å¤´ï¼Œå»æ‰ /root å‰ç¼€
    if project_root.startswith('/root/'):
        project_root = project_root[6:]  # å»æ‰ '/root' å‰ç¼€
        print(f"ğŸ”§ ä¿®å¤é¡¹ç›®æ ¹ç›®å½•è·¯å¾„: {project_root}")

    # æ ¹æ®é…ç½®ç¡®å®šæ¨¡å‹è·¯å¾„å’Œè¯„ä¼°ç›®å½•
    if SPECIFIC_TRAIN_PATH:
        print(f"ğŸ¯ æŒ‡å®šè®­ç»ƒè½®æ¬¡æ¨¡å¼")
        print(f"   è®­ç»ƒè·¯å¾„: {SPECIFIC_TRAIN_PATH}")

        # è·å–å¯¹åº”çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
        model_path = get_corresponding_model_path(SPECIFIC_TRAIN_PATH, project_root)
        if not model_path:
            print("âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„æ¨¡å‹æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
            return

        # æå–æ—¶é—´æˆ³å¹¶ç”Ÿæˆè¯„ä¼°ç›®å½•å
        timestamp = extract_timestamp_from_path(SPECIFIC_TRAIN_PATH)
        eval_timestamp = generate_eval_timestamp(timestamp)

    else:
        print(f"ğŸ“‹ é»˜è®¤æ¨¡å¼ï¼šä½¿ç”¨ latest æ¨¡å‹")
        # ä½¿ç”¨é»˜è®¤çš„ latest æ¨¡å‹
        model_path = os.path.join(project_root, 'models', 'checkpoints', 'latest', 'best_model.pth')
        eval_timestamp = datetime.datetime.now().strftime("eval_%Y%m%d_%H%M%S")

    # ==================== ç¡®å®šè¯„ä¼°ç±»å‹å’Œä¿å­˜ç›®å½• ====================
    # ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°ï¼Œä½†ä¿å­˜åˆ°evaluationç›®å½•ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
    base_eval_dir = os.path.join(project_root, 'outputs', 'evaluation')
    eval_type = "æµ‹è¯•é›†"
    
    save_dir = os.path.join(base_eval_dir, eval_timestamp)
    latest_eval_dir = os.path.join(base_eval_dir, "latest")

    # ==================== åŠ è½½é…ç½® ====================
    print("ğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶...")
    config, config_source = load_training_config(SPECIFIC_TRAIN_PATH, project_root)
    if config is None:
        print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
        return

    # æ‰“å°è·¯å¾„ä¿¡æ¯
    if SPECIFIC_TRAIN_PATH:
        print_path_info(SPECIFIC_TRAIN_PATH, model_path, save_dir, config_source)

    print("ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°")
    print("="*80)
    print(f"ğŸ“Š è¯„ä¼°ç±»å‹: {eval_type}æœ€ç»ˆè¯„ä¼°")
    print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"ğŸ“„ é…ç½®æ–‡ä»¶æ¥æº: {config_source}")
    print(f"ğŸ¤– æ¨¡å‹æ–‡ä»¶: {model_path}")
    print(f"ğŸ’¾ ä¿å­˜ç›®å½•: {save_dir}")
    print("="*80)

    # ==================== è®¾å¤‡æ£€æŸ¥ ====================
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

    # ==================== æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ ====================
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒæˆ–æ£€æŸ¥æ¨¡å‹è·¯å¾„")
        return

    # ==================== åŠ è½½æ¨¡å‹ ====================
    model = load_model(model_path, config, device)
    if model is None:
        return

    # ==================== åˆ›å»ºæ•°æ®åŠ è½½å™¨ ====================
    dataloader = create_evaluation_dataloader(config, project_root)
    if dataloader is None:
        return

    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(dataloader.dataset)}")
    print(f"   æ‰¹æ¬¡å¤§å°: {dataloader.batch_size}")
    print(f"   æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")

    # ==================== æ¨¡å‹è¯„ä¼° ====================
    y_true, y_pred, y_prob, image_paths = evaluate_model(model, dataloader, device)

    print(f"âœ… è¯„ä¼°å®Œæˆ")
    print(f"   æ€»æ ·æœ¬æ•°: {len(y_true)}")
    print(f"   é¢„æµ‹å‡†ç¡®ç‡: {np.mean(y_true == y_pred):.4f}")

    # ==================== ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š ====================
    print("ğŸ“‹ å‡†å¤‡ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    class_names = config['data']['class_names']
    model_type = config['model']['name'].upper()
    model_name = f"{model_type}-{config['data']['num_classes']}ç±»"

    print(f"ğŸ“Š ç±»åˆ«æ•°é‡: {len(class_names)}")
    print(f"ğŸ¤– æ¨¡å‹åç§°: {model_name}")

    # åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
    print("ğŸ”§ åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨...")
    report_generator = ReportGenerator(class_names, model_name)

    # ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Šï¼ˆä¿å­˜åˆ°æ—¶é—´æˆ³ç›®å½•ï¼‰
    print(f"ğŸ“‹ å¼€å§‹ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
    print(f"ğŸ’¾ ä¿å­˜ç›®å½•: {save_dir}")
    
    try:
        results = report_generator.generate_comprehensive_report(
            y_true=y_true,
            y_pred=y_pred,
            y_prob=y_prob,
            image_paths=image_paths,
            save_dir=save_dir
        )
        print("âœ… ç»¼åˆè¯„ä¼°æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        print("ğŸ” å°è¯•ç”Ÿæˆç®€åŒ–æŠ¥å‘Š...")
        # å¦‚æœç»¼åˆæŠ¥å‘Šå¤±è´¥ï¼Œè‡³å°‘è¾“å‡ºåŸºæœ¬æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, classification_report
        accuracy = accuracy_score(y_true, y_pred)
        print(f"æ•´ä½“å‡†ç¡®ç‡: {accuracy:.4f}")
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_true, y_pred, target_names=class_names))
        return

    # åŒæ—¶ä¿å­˜åˆ°latestç›®å½•
    print(f"\nğŸ“‹ åŒæ—¶ä¿å­˜åˆ°latestç›®å½•: {latest_eval_dir}")
    report_generator.generate_comprehensive_report(
        y_true=y_true,
        y_pred=y_pred,
        y_prob=y_prob,
        image_paths=image_paths,
        save_dir=latest_eval_dir
    )

    # ç‰¹å¾å¯è§†åŒ–åˆ†æå·²ç§»è‡³ç‹¬ç«‹è„šæœ¬ scripts/evaluate/feature_visualization.py
    # å¦‚éœ€è¿›è¡Œç‰¹å¾åˆ†æï¼Œè¯·å•ç‹¬è¿è¡Œè¯¥è„šæœ¬

    # ==================== è¾“å‡ºå…³é”®ç»“æœ ====================
    print("\n" + "="*80)
    print(f"ğŸ“Š {eval_type}è¯„ä¼°ç»“æœæ‘˜è¦")
    print("="*80)
    print(f"æ•´ä½“å‡†ç¡®ç‡: {results['metrics']['accuracy']:.4f}")
    print(f"å®å¹³å‡F1-Score: {results['metrics']['f1_macro']:.4f}")
    print(f"é”™è¯¯æ ·æœ¬æ•°: {results['error_info']['total_errors']}")
    print(f"é”™è¯¯ç‡: {results['error_info']['error_rate']:.4f}")

    if 'top2_accuracy' in results['metrics']:
        print(f"Top-2å‡†ç¡®ç‡: {results['metrics']['top2_accuracy']:.4f}")

    print(f"ç›¸é‚»ç±»åˆ«æ··æ·†ç‡: {results['metrics']['adjacent_confusion_rate']:.4f}")
    print(f"ä¸¥é‡é”™è¯¯ç‡: {results['metrics']['severe_error_rate']:.4f}")

    print("\nå„ç±»åˆ«æ€§èƒ½:")
    for class_name in class_names:
        precision = results['metrics'][f'precision_{class_name}']
        recall = results['metrics'][f'recall_{class_name}']
        f1 = results['metrics'][f'f1_{class_name}']
        accuracy = results['metrics'][f'accuracy_{class_name}']
        print(f"  {class_name}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}, A={accuracy:.3f}")

    print("="*80)
    print("ğŸ‰ è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ä¿å­˜çš„æ–‡ä»¶ã€‚")


if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œé…ç½® - å¯ä»¥åœ¨ PyCharm ä¸­ç›´æ¥ç‚¹å‡»è¿è¡Œ

    ä½¿ç”¨æ–¹æ³•:
    1. ä¿®æ”¹ main å‡½æ•°ä¸­çš„ SPECIFIC_TRAIN_PATH å˜é‡
    2. è®¾ç½®ä¸ºå…·ä½“çš„è®­ç»ƒè·¯å¾„æ¥è¯„ä¼°æŒ‡å®šè½®æ¬¡
    3. è®¾ç½®ä¸º None æ¥ä½¿ç”¨é»˜è®¤çš„ latest æ¨¡å‹
    4. åœ¨ PyCharm ä¸­ç›´æ¥è¿è¡Œæ­¤è„šæœ¬
    """
    main()
