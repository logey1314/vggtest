"""
ç‹¬ç«‹ç‰¹å¾å¯è§†åŒ–åˆ†æå·¥å…·
ä¸“é—¨ç”¨äºæ··å‡åœŸåè½åº¦è¯†åˆ«çš„æ·±åº¦ç‰¹å¾å¯è§†åŒ–åˆ†æ

åŠŸèƒ½åŒ…æ‹¬:
- å®Œæ•´çš„13å±‚VGG16å·ç§¯ç‰¹å¾å›¾å¯è§†åŒ–
- å¢å¼ºçš„ç±»æ¿€æ´»æ˜ å°„(CAM)åˆ†æ
- t-SNEé™ç»´å¯è§†åŒ–
- æ··å‡åœŸåè½åº¦ä¸“ç”¨ç‰¹å¾åˆ†æ
- ç»Ÿè®¡åˆ†æå’Œç±»åˆ«å¯¹æ¯”

ä½¿ç”¨æ–¹æ³•:
1. ä¿®æ”¹mainå‡½æ•°ä¸­çš„train_path
2. ç›´æ¥åœ¨PyCharmä¸­è¿è¡Œæ­¤è„šæœ¬
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import cv2
from PIL import Image
import seaborn as sns
from typing import List, Dict, Tuple, Optional
import yaml
import datetime
from tqdm import tqdm
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(script_dir))
sys.path.append(project_root)

from scripts.visualize.visualize_utils import setup_plot_style, save_plot_with_timestamp
from src.models.model_factory import create_model
from src.data.dataset import DataGenerator
from torch.utils.data import DataLoader
from src.utils.font_manager import setup_matplotlib_font, get_font_manager
from scripts.evaluate.evaluate_utils import (
    extract_timestamp_from_path,
    find_model_file,
    find_training_config,
    load_training_config,
    generate_eval_timestamp
)


class FeatureExtractor:
    """ç‰¹å¾æå–å™¨ - ç”¨äºæå–VGG16æ¨¡å‹çš„ä¸­é—´å±‚ç‰¹å¾"""
    
    def __init__(self, model, device='cpu'):
        """
        åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        
        Args:
            model: VGG16æ¨¡å‹å®ä¾‹
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.device = device
        self.model.eval()
        
        # å®šä¹‰VGG16çš„å®Œæ•´13å±‚å·ç§¯å±‚ä½ç½®
        self.layer_names = {
            # Block 1: ä½çº§ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¹ç†ï¼‰- 224Ã—224
            'conv1_1': 0,   # 64é€šé“ - åŸºç¡€è¾¹ç¼˜æ£€æµ‹
            'conv1_2': 2,   # 64é€šé“ - å¢å¼ºè¾¹ç¼˜ç‰¹å¾

            # Block 2: ä¸­ä½çº§ç‰¹å¾ï¼ˆå½¢çŠ¶ã€æ¨¡å¼ï¼‰- 112Ã—112
            'conv2_1': 5,   # 128é€šé“ - ç®€å•å½¢çŠ¶
            'conv2_2': 7,   # 128é€šé“ - å¤æ‚å½¢çŠ¶

            # Block 3: ä¸­çº§ç‰¹å¾ï¼ˆå±€éƒ¨æ¨¡å¼ï¼‰- 56Ã—56
            'conv3_1': 10,  # 256é€šé“ - å±€éƒ¨çº¹ç†æ¨¡å¼
            'conv3_2': 12,  # 256é€šé“ - å¢å¼ºçº¹ç†ç‰¹å¾
            'conv3_3': 14,  # 256é€šé“ - å¤æ‚çº¹ç†ç»„åˆ

            # Block 4: ä¸­é«˜çº§ç‰¹å¾ï¼ˆå¯¹è±¡éƒ¨åˆ†ï¼‰- 28Ã—28
            'conv4_1': 17,  # 512é€šé“ - æ··å‡åœŸè¡¨é¢ç‰¹å¾
            'conv4_2': 19,  # 512é€šé“ - åè½åº¦ç›¸å…³ç‰¹å¾
            'conv4_3': 21,  # 512é€šé“ - é«˜çº§è¡¨é¢æ¨¡å¼

            # Block 5: é«˜çº§ç‰¹å¾ï¼ˆè¯­ä¹‰ç‰¹å¾ï¼‰- 14Ã—14
            'conv5_1': 24,  # 512é€šé“ - åè½åº¦è¯­ä¹‰ç‰¹å¾
            'conv5_2': 26,  # 512é€šé“ - åˆ†ç±»å†³ç­–ç‰¹å¾
            'conv5_3': 28,  # 512é€šé“ - æœ€ç»ˆè¯­ä¹‰è¡¨ç¤º
        }

        # æŒ‰Blockç»„ç»‡çš„å±‚åç§°
        self.layer_blocks = {
            'Block1_EdgeTexture': ['conv1_1', 'conv1_2'],
            'Block2_ShapePattern': ['conv2_1', 'conv2_2'],
            'Block3_LocalPattern': ['conv3_1', 'conv3_2', 'conv3_3'],
            'Block4_ObjectPart': ['conv4_1', 'conv4_2', 'conv4_3'],
            'Block5_Semantic': ['conv5_1', 'conv5_2', 'conv5_3']
        }

        # å®Œæ•´çš„13å±‚åˆ—è¡¨
        self.all_conv_layers = [
            'conv1_1', 'conv1_2', 'conv2_1', 'conv2_2', 'conv3_1', 'conv3_2', 'conv3_3',
            'conv4_1', 'conv4_2', 'conv4_3', 'conv5_1', 'conv5_2', 'conv5_3'
        ]
        
        # å­˜å‚¨ç‰¹å¾å›¾çš„é’©å­
        self.features = {}
        self.hooks = []
        
    def register_hooks(self, layer_names: List[str]):
        """
        æ³¨å†Œå‰å‘é’©å­ä»¥æå–æŒ‡å®šå±‚çš„ç‰¹å¾
        
        Args:
            layer_names: è¦æå–ç‰¹å¾çš„å±‚åç§°åˆ—è¡¨
        """
        def get_activation(name):
            def hook(model, input, output):
                self.features[name] = output.detach()
            return hook
        
        # æ¸…é™¤ä¹‹å‰çš„é’©å­
        self.remove_hooks()
        
        # æ³¨å†Œæ–°çš„é’©å­
        for layer_name in layer_names:
            if layer_name in self.layer_names:
                layer_idx = self.layer_names[layer_name]
                hook = self.model.features[layer_idx].register_forward_hook(get_activation(layer_name))
                self.hooks.append(hook)
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰é’©å­"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
        self.features = {}
    
    def extract_features(self, image_tensor: torch.Tensor, layer_names: List[str]) -> Dict[str, torch.Tensor]:
        """
        æå–æŒ‡å®šå±‚çš„ç‰¹å¾å›¾

        Args:
            image_tensor: è¾“å…¥å›¾åƒå¼ é‡ [1, 3, H, W]
            layer_names: è¦æå–çš„å±‚åç§°åˆ—è¡¨

        Returns:
            Dict[str, torch.Tensor]: å±‚åç§°åˆ°ç‰¹å¾å›¾çš„æ˜ å°„
        """
        # éªŒè¯å¹¶è¿‡æ»¤å±‚åç§°ï¼Œåªä¿ç•™å·ç§¯å±‚
        valid_layer_names = self._validate_layer_names(layer_names)
        if not valid_layer_names:
            print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„å·ç§¯å±‚å¯ä»¥æå–ç‰¹å¾")
            return {}

        self.register_hooks(valid_layer_names)

        try:
            with torch.no_grad():
                image_tensor = image_tensor.to(self.device)
                _ = self.model(image_tensor)

            # éªŒè¯æå–çš„ç‰¹å¾ç»´åº¦
            validated_features = self._validate_feature_dimensions(self.features.copy())
            return validated_features

        except Exception as e:
            print(f"âš ï¸  ç‰¹å¾æå–å¤±è´¥: {e}")
            self.remove_hooks()
            return {}

    def _validate_layer_names(self, layer_names: List[str]) -> List[str]:
        """
        éªŒè¯å¹¶è¿‡æ»¤å±‚åç§°ï¼Œåªä¿ç•™æœ‰æ•ˆçš„å·ç§¯å±‚

        Args:
            layer_names: è¦éªŒè¯çš„å±‚åç§°åˆ—è¡¨

        Returns:
            List[str]: æœ‰æ•ˆçš„å·ç§¯å±‚åç§°åˆ—è¡¨
        """
        valid_layers = []
        for layer_name in layer_names:
            if layer_name in self.layer_names:
                # ç¡®ä¿æ˜¯å·ç§¯å±‚ï¼ˆconvå¼€å¤´ï¼‰
                if layer_name.startswith('conv'):
                    valid_layers.append(layer_name)
                else:
                    print(f"âš ï¸  è·³è¿‡éå·ç§¯å±‚: {layer_name}")
            else:
                print(f"âš ï¸  æœªçŸ¥å±‚åç§°: {layer_name}")

        return valid_layers

    def _validate_feature_dimensions(self, features: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        éªŒè¯ç‰¹å¾å›¾ç»´åº¦ï¼Œè¿‡æ»¤æ‰å¼‚å¸¸çš„ç‰¹å¾

        Args:
            features: åŸå§‹ç‰¹å¾å­—å…¸

        Returns:
            Dict[str, torch.Tensor]: éªŒè¯åçš„ç‰¹å¾å­—å…¸
        """
        validated_features = {}

        for layer_name, feature_tensor in features.items():
            try:
                # æ£€æŸ¥ç‰¹å¾å›¾ç»´åº¦
                if len(feature_tensor.shape) == 4:  # [B, C, H, W]
                    batch_size, channels, height, width = feature_tensor.shape

                    # éªŒè¯ç»´åº¦åˆç†æ€§ - æ”¾å®½æ‰¹æ¬¡å¤§å°é™åˆ¶
                    if batch_size > 0 and channels > 0 and height > 0 and width > 0:
                        # æ£€æŸ¥ç‰¹å¾å›¾å°ºå¯¸æ˜¯å¦åˆç†ï¼ˆä¸èƒ½å¤ªå¤§æˆ–å¤ªå°ï¼‰
                        if 1 <= height <= 224 and 1 <= width <= 224 and 1 <= channels <= 512:
                            validated_features[layer_name] = feature_tensor
                            print(f"âœ… {layer_name}: {batch_size}Ã—{channels}Ã—{height}Ã—{width}")
                        else:
                            print(f"âš ï¸  {layer_name} ç»´åº¦å¼‚å¸¸: {channels}Ã—{height}Ã—{width}")
                    else:
                        print(f"âš ï¸  {layer_name} æ‰¹æ¬¡æˆ–ç»´åº¦å¼‚å¸¸: {feature_tensor.shape}")
                else:
                    print(f"âš ï¸  {layer_name} ä¸æ˜¯4Dç‰¹å¾å›¾: {feature_tensor.shape}")

            except Exception as e:
                print(f"âš ï¸  éªŒè¯ {layer_name} ç‰¹å¾æ—¶å‡ºé”™: {e}")

        return validated_features


class GradCAM:
    """Grad-CAMå¯è§†åŒ–ç±»"""
    
    def __init__(self, model, target_layer_name: str, device='cpu'):
        """
        åˆå§‹åŒ–Grad-CAM
        
        Args:
            model: VGG16æ¨¡å‹å®ä¾‹
            target_layer_name: ç›®æ ‡å±‚åç§°ï¼ˆé€šå¸¸æ˜¯æœ€åä¸€ä¸ªå·ç§¯å±‚ï¼‰
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.device = device
        self.target_layer_name = target_layer_name
        
        # ç‰¹å¾æå–å™¨
        self.feature_extractor = FeatureExtractor(model, device)
        
        # å­˜å‚¨æ¢¯åº¦å’Œç‰¹å¾å›¾
        self.gradients = None
        self.activations = None
        
    def save_gradient(self, grad):
        """ä¿å­˜æ¢¯åº¦çš„é’©å­å‡½æ•°"""
        self.gradients = grad
    
    def generate_cam(self, image_tensor: torch.Tensor, class_idx: int) -> np.ndarray:
        """
        ç”Ÿæˆç±»æ¿€æ´»æ˜ å°„
        
        Args:
            image_tensor: è¾“å…¥å›¾åƒå¼ é‡ [1, 3, H, W]
            class_idx: ç›®æ ‡ç±»åˆ«ç´¢å¼•
            
        Returns:
            np.ndarray: CAMçƒ­åŠ›å›¾
        """
        self.model.eval()
        image_tensor = image_tensor.to(self.device)
        image_tensor.requires_grad_()
        
        # æ³¨å†Œé’©å­æå–ç‰¹å¾å’Œæ¢¯åº¦
        self.feature_extractor.register_hooks([self.target_layer_name])
        
        # å‰å‘ä¼ æ’­
        output = self.model(image_tensor)
        
        # è·å–ç‰¹å¾å›¾
        self.activations = self.feature_extractor.features[self.target_layer_name]
        
        # æ³¨å†Œæ¢¯åº¦é’©å­
        self.activations.register_hook(self.save_gradient)
        
        # åå‘ä¼ æ’­
        self.model.zero_grad()
        class_score = output[0, class_idx]
        class_score.backward()
        
        # è®¡ç®—æƒé‡ï¼ˆæ¢¯åº¦çš„å…¨å±€å¹³å‡æ± åŒ–ï¼‰
        weights = torch.mean(self.gradients, dim=[2, 3], keepdim=True)
        
        # è®¡ç®—CAM
        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)
        cam = F.relu(cam)
        
        # å½’ä¸€åŒ–åˆ°0-1
        cam = cam.squeeze().cpu().numpy()
        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
        
        # æ¸…ç†é’©å­
        self.feature_extractor.remove_hooks()
        
        return cam


class TSNEVisualizer:
    """t-SNEé™ç»´å¯è§†åŒ–ç±»"""

    def __init__(self, class_names: List[str], use_english_labels: bool = False):
        """
        åˆå§‹åŒ–t-SNEå¯è§†åŒ–å™¨

        Args:
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            use_english_labels: æ˜¯å¦ä½¿ç”¨è‹±æ–‡æ ‡ç­¾
        """
        self.class_names = class_names
        self.use_english_labels = use_english_labels

    def extract_features_for_tsne(self, model, dataloader, layer_name: str, device='cpu', max_samples=100):
        """
        æå–æŒ‡å®šå±‚çš„ç‰¹å¾ç”¨äºt-SNEåˆ†æ

        Args:
            model: æ¨¡å‹å®ä¾‹
            dataloader: æ•°æ®åŠ è½½å™¨
            layer_name: è¦æå–çš„å±‚åç§°
            device: è®¡ç®—è®¾å¤‡
            max_samples: æœ€å¤§æ ·æœ¬æ•°é‡

        Returns:
            Tuple[np.ndarray, np.ndarray]: (ç‰¹å¾çŸ©é˜µ, æ ‡ç­¾æ•°ç»„)
        """
        model.eval()
        features_list = []
        labels_list = []

        # åˆ›å»ºç‰¹å¾æå–å™¨
        feature_extractor = FeatureExtractor(model, device)

        print(f"ğŸ” æå– {layer_name} å±‚ç‰¹å¾ç”¨äºt-SNEåˆ†æ...")

        with torch.no_grad():
            sample_count = 0
            for batch_idx, batch_data in enumerate(tqdm(dataloader, desc="æå–ç‰¹å¾")):
                # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                if len(batch_data) == 3:
                    images, labels, _ = batch_data
                elif len(batch_data) == 2:
                    images, labels = batch_data
                else:
                    print(f"âš ï¸  æœªçŸ¥çš„æ•°æ®æ ¼å¼: {len(batch_data)} ä¸ªå…ƒç´ ")
                    continue
                if sample_count >= max_samples:
                    break

                # ç¡®ä¿å¼ é‡ç»´åº¦æ­£ç¡® [B, 3, H, W]
                if len(images.shape) == 5:  # [B, 1, 3, H, W] -> [B, 3, H, W]
                    images = images.squeeze(1)
                elif len(images.shape) == 3:  # [3, H, W] -> [1, 3, H, W]
                    images = images.unsqueeze(0)

                images = images.to(device)

                # æå–ç‰¹å¾
                try:
                    layer_features = feature_extractor.extract_features(images, [layer_name])
                except Exception as e:
                    print(f"âš ï¸  ç‰¹å¾æå–å¤±è´¥: {e}")
                    continue

                if layer_name in layer_features:
                    # è·å–ç‰¹å¾å¹¶å±•å¹³
                    feature_map = layer_features[layer_name]  # [B, C, H, W]
                    batch_size = feature_map.size(0)

                    # éªŒè¯ç‰¹å¾å›¾ç»´åº¦æ˜¯å¦æ­£ç¡®
                    if len(feature_map.shape) == 4 and batch_size > 0:
                        # å…¨å±€å¹³å‡æ± åŒ–
                        pooled_features = F.adaptive_avg_pool2d(feature_map, (1, 1))  # [B, C, 1, 1]
                        pooled_features = pooled_features.view(batch_size, -1)  # [B, C]

                        features_list.append(pooled_features.cpu().numpy())
                        labels_list.extend(labels.cpu().numpy())

                        sample_count += batch_size
                        print(f"    âœ… {layer_name} ç‰¹å¾æå–æˆåŠŸ: {feature_map.shape} -> {pooled_features.shape}")
                    else:
                        print(f"    âš ï¸  {layer_name} ç‰¹å¾å›¾ç»´åº¦å¼‚å¸¸: {feature_map.shape}")
                else:
                    print(f"    âš ï¸  {layer_name} ç‰¹å¾æœªæ‰¾åˆ°")

        # æ¸…ç†é’©å­
        feature_extractor.remove_hooks()

        if features_list:
            features_matrix = np.vstack(features_list)
            labels_array = np.array(labels_list)
            print(f"âœ… æå–å®Œæˆ: {features_matrix.shape[0]} ä¸ªæ ·æœ¬, {features_matrix.shape[1]} ç»´ç‰¹å¾")
            return features_matrix, labels_array
        else:
            print("âŒ æœªèƒ½æå–åˆ°ä»»ä½•ç‰¹å¾")
            return None, None

    def visualize_tsne(self, features: np.ndarray, labels: np.ndarray,
                      layer_name: str, save_dir: str, perplexity=30) -> str:
        """
        ç”Ÿæˆt-SNEå¯è§†åŒ–å›¾

        Args:
            features: ç‰¹å¾çŸ©é˜µ [N, D]
            labels: æ ‡ç­¾æ•°ç»„ [N]
            layer_name: å±‚åç§°
            save_dir: ä¿å­˜ç›®å½•
            perplexity: t-SNEå›°æƒ‘åº¦å‚æ•°

        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        print(f"ğŸ¨ ç”Ÿæˆ {layer_name} çš„t-SNEå¯è§†åŒ–...")

        # å¦‚æœç‰¹å¾ç»´åº¦å¤ªé«˜ï¼Œå…ˆç”¨PCAé™ç»´
        if features.shape[1] > 50:
            print(f"   ç‰¹å¾ç»´åº¦è¾ƒé«˜({features.shape[1]})ï¼Œå…ˆç”¨PCAé™ç»´åˆ°50ç»´")
            pca = PCA(n_components=50)
            features = pca.fit_transform(features)

        # t-SNEé™ç»´
        print(f"   æ‰§è¡Œt-SNEé™ç»´ (perplexity={perplexity})...")
        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42,
                   init='pca', learning_rate='auto')
        features_2d = tsne.fit_transform(features)

        # åˆ›å»ºå¯è§†åŒ–
        fig, ax = plt.subplots(figsize=(12, 10))

        # ä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…é¢œè‰²
        colors = plt.cm.Set3(np.linspace(0, 1, len(self.class_names)))

        for i, class_name in enumerate(self.class_names):
            mask = labels == i
            if np.any(mask):
                ax.scatter(features_2d[mask, 0], features_2d[mask, 1],
                          c=[colors[i]], label=class_name, alpha=0.7, s=50)

        # ä½¿ç”¨è‹±æ–‡æˆ–ä¸­æ–‡æ ‡é¢˜
        if hasattr(self, 'use_english_labels') and self.use_english_labels:
            ax.set_title(f'{layer_name} Feature Space t-SNE Visualization', fontsize=16, pad=20)
            ax.set_xlabel('t-SNE Dimension 1', fontsize=12)
            ax.set_ylabel('t-SNE Dimension 2', fontsize=12)
            ax.legend(title='Slump Classes', bbox_to_anchor=(1.05, 1), loc='upper left')
        else:
            ax.set_title(f'{layer_name} ç‰¹å¾ç©ºé—´ t-SNE å¯è§†åŒ–', fontsize=16, pad=20)
            ax.set_xlabel('t-SNE ç»´åº¦ 1', fontsize=12)
            ax.set_ylabel('t-SNE ç»´åº¦ 2', fontsize=12)
            ax.legend(title='åè½åº¦ç±»åˆ«', bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        info_text = f"æ ·æœ¬æ•°: {len(features_2d)}\nåŸå§‹ç»´åº¦: {features.shape[1]}\nå›°æƒ‘åº¦: {perplexity}"
        ax.text(0.02, 0.98, info_text, transform=ax.transAxes,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        os.makedirs(save_dir, exist_ok=True)
        save_path = save_plot_with_timestamp(fig, save_dir, f'tsne_{layer_name}', show_plot=False)

        return save_path


class FeatureVisualizer:
    """ç‰¹å¾å¯è§†åŒ–ä¸»ç±»"""
    
    def __init__(self, model, class_names: List[str], device='cpu'):
        """
        åˆå§‹åŒ–ç‰¹å¾å¯è§†åŒ–å™¨
        
        Args:
            model: VGG16æ¨¡å‹å®ä¾‹
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.class_names = class_names
        self.device = device

        # å…ˆè®¾ç½®ç»˜å›¾æ ·å¼å’Œå­—ä½“
        setup_plot_style()
        has_chinese_font = setup_matplotlib_font()

        # æ£€æŸ¥å­—ä½“æ”¯æŒ
        self.use_english_labels = not has_chinese_font
        if self.use_english_labels:
            print("âš ï¸  æœªæ£€æµ‹åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡é¢˜")

        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆåœ¨å­—ä½“è®¾ç½®ä¹‹åï¼‰
        self.feature_extractor = FeatureExtractor(model, device)
        self.grad_cam = GradCAM(model, 'conv5_3', device)  # ä½¿ç”¨æœ€åä¸€ä¸ªå·ç§¯å±‚
        self.tsne_visualizer = TSNEVisualizer(class_names, self.use_english_labels)
    
    def visualize_feature_maps_by_blocks(self, image_tensor: torch.Tensor, image_path: str,
                                       save_dir: str) -> Dict[str, str]:
        """
        æŒ‰Blockç»“æ„å¯è§†åŒ–ç‰¹å¾å›¾

        Args:
            image_tensor: è¾“å…¥å›¾åƒå¼ é‡
            image_path: åŸå§‹å›¾åƒè·¯å¾„
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            Dict[str, str]: Blockåç§°åˆ°ä¿å­˜è·¯å¾„çš„æ˜ å°„
        """
        print(f"ğŸ¨ å¼€å§‹æŒ‰Blockç»“æ„ç”Ÿæˆç‰¹å¾å›¾...")

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)

        results = {}

        # ä¸ºæ¯ä¸ªBlockç”Ÿæˆç‰¹å¾å›¾
        for block_name, layer_names in self.feature_extractor.layer_blocks.items():
            print(f"ğŸ“Š ç”Ÿæˆ {block_name} ç‰¹å¾å›¾...")

            try:
                save_path = self._visualize_single_block(
                    image_tensor, image_path, layer_names, block_name, save_dir
                )
                if save_path:
                    results[block_name] = save_path
                    print(f"  âœ… {block_name} å®Œæˆ")
                else:
                    print(f"  âš ï¸  {block_name} ç”Ÿæˆå¤±è´¥")
            except Exception as e:
                print(f"  âŒ {block_name} ç”Ÿæˆå¼‚å¸¸: {e}")

        # ç”Ÿæˆæ¦‚è§ˆå›¾
        try:
            print("ğŸ“Š ç”Ÿæˆæ‰€æœ‰Blockæ¦‚è§ˆå›¾...")
            overview_path = self._visualize_blocks_overview(
                image_tensor, image_path, save_dir
            )
            if overview_path:
                results['overview'] = overview_path
                print("  âœ… æ¦‚è§ˆå›¾å®Œæˆ")
        except Exception as e:
            print(f"  âŒ æ¦‚è§ˆå›¾ç”Ÿæˆå¼‚å¸¸: {e}")

        return results

    def _visualize_single_block(self, image_tensor: torch.Tensor, image_path: str,
                               layer_names: List[str], block_name: str, save_dir: str) -> str:
        """
        å¯è§†åŒ–å•ä¸ªBlockçš„ç‰¹å¾å›¾

        Args:
            image_tensor: è¾“å…¥å›¾åƒå¼ é‡
            image_path: åŸå§‹å›¾åƒè·¯å¾„
            layer_names: è¯¥BlockåŒ…å«çš„å±‚åç§°
            block_name: Blockåç§°
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # æå–è¯¥Blockçš„ç‰¹å¾
        features = self.feature_extractor.extract_features(image_tensor, layer_names)

        if not features:
            print(f"    âš ï¸  {block_name} æœªèƒ½æå–åˆ°ç‰¹å¾")
            return None

        valid_layers = list(features.keys())

        # åˆ›å»ºå­å›¾ - æ¯ä¸ªBlockçš„å±‚æ•°ä¸å¤šï¼Œä½¿ç”¨åˆé€‚çš„å¸ƒå±€
        num_layers = len(valid_layers)
        num_channels = 8  # æ¯å±‚æ˜¾ç¤º8ä¸ªé€šé“

        fig, axes = plt.subplots(num_layers + 1, num_channels,
                                figsize=(20, 3 * (num_layers + 1)))
        
        # æ˜¾ç¤ºåŸå›¾
        try:
            original_image = Image.open(image_path).convert('RGB')
            axes[0, 0].imshow(original_image)
            if self.use_english_labels:
                axes[0, 0].set_title(f'Original Image\n{block_name}', fontsize=12, fontweight='bold')
            else:
                axes[0, 0].set_title(f'åŸå§‹å›¾åƒ\n{block_name}', fontsize=12, fontweight='bold')
            axes[0, 0].axis('off')
        except Exception as e:
            print(f"    âš ï¸  åŠ è½½åŸå§‹å›¾åƒå¤±è´¥: {e}")
            axes[0, 0].text(0.5, 0.5, 'Image\nNot Found', ha='center', va='center')
            axes[0, 0].set_title(f'Original Image\n{block_name}', fontsize=12)
            axes[0, 0].axis('off')

        # éšè—åŸå›¾è¡Œçš„å…¶ä»–å­å›¾
        for i in range(1, num_channels):
            axes[0, i].axis('off')
        
        # å¯è§†åŒ–æ¯å±‚çš„ç‰¹å¾å›¾
        for layer_idx, layer_name in enumerate(valid_layers):
            try:
                if layer_name not in features:
                    print(f"âš ï¸  {layer_name} ç‰¹å¾æœªæ‰¾åˆ°ï¼Œè·³è¿‡")
                    continue

                feature_map = features[layer_name].squeeze().cpu().numpy()

                # éªŒè¯ç‰¹å¾å›¾ç»´åº¦
                if len(feature_map.shape) < 2:
                    print(f"âš ï¸  {layer_name} ç‰¹å¾å›¾ç»´åº¦å¼‚å¸¸: {feature_map.shape}")
                    continue

                # å¦‚æœæ˜¯3Dç‰¹å¾å›¾ [C, H, W]
                if len(feature_map.shape) == 3:
                    available_channels = min(num_channels, feature_map.shape[0])

                    for channel_idx in range(available_channels):
                        ax = axes[layer_idx + 1, channel_idx]

                        # æ£€æŸ¥é€šé“æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                        channel_data = feature_map[channel_idx]
                        if np.isnan(channel_data).any() or np.isinf(channel_data).any():
                            ax.text(0.5, 0.5, 'Invalid\nData', ha='center', va='center')
                            if self.use_english_labels:
                                ax.set_title(f'{layer_name}\nChannel {channel_idx}', fontsize=10)
                            else:
                                ax.set_title(f'{layer_name}\né€šé“ {channel_idx}', fontsize=10)
                            ax.axis('off')
                            continue

                        # æ˜¾ç¤ºç‰¹å¾å›¾
                        im = ax.imshow(channel_data, cmap='viridis')
                        if self.use_english_labels:
                            ax.set_title(f'{layer_name}\nChannel {channel_idx}', fontsize=10)
                        else:
                            ax.set_title(f'{layer_name}\né€šé“ {channel_idx}', fontsize=10)
                        ax.axis('off')

                        # æ·»åŠ é¢œè‰²æ¡
                        try:
                            plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
                        except:
                            pass  # å¦‚æœé¢œè‰²æ¡æ·»åŠ å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ

                    # éšè—å¤šä½™çš„å­å›¾
                    for channel_idx in range(available_channels, num_channels):
                        axes[layer_idx + 1, channel_idx].axis('off')

                # å¦‚æœæ˜¯2Dç‰¹å¾å›¾ [H, W]
                elif len(feature_map.shape) == 2:
                    ax = axes[layer_idx + 1, 0]
                    im = ax.imshow(feature_map, cmap='viridis')
                    ax.set_title(f'{layer_name}', fontsize=10)
                    ax.axis('off')

                    try:
                        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
                    except:
                        pass

                    # éšè—å…¶ä»–å­å›¾
                    for channel_idx in range(1, num_channels):
                        axes[layer_idx + 1, channel_idx].axis('off')

            except Exception as e:
                print(f"âš ï¸  å¯è§†åŒ– {layer_name} ç‰¹å¾å›¾æ—¶å‡ºé”™: {e}")
                # å¡«å……é”™è¯¯ä¿¡æ¯
                for channel_idx in range(num_channels):
                    ax = axes[layer_idx + 1, channel_idx]
                    ax.text(0.5, 0.5, f'Error\n{layer_name}', ha='center', va='center')
                    ax.set_title(f'{layer_name}', fontsize=10)
                    ax.axis('off')
        
        plt.suptitle('VGG16 ç‰¹å¾å›¾å¯è§†åŒ–', fontsize=16, y=0.98)
        # è®¾ç½®æ•´ä½“æ ‡é¢˜
        if self.use_english_labels:
            block_descriptions = {
                'Block1_EdgeTexture': 'Block 1: Edge & Texture Features',
                'Block2_ShapePattern': 'Block 2: Shape & Pattern Features',
                'Block3_LocalPattern': 'Block 3: Local Pattern Features',
                'Block4_ObjectPart': 'Block 4: Object Part Features',
                'Block5_Semantic': 'Block 5: Semantic Features'
            }
        else:
            block_descriptions = {
                'Block1_EdgeTexture': 'Block 1: è¾¹ç¼˜çº¹ç†ç‰¹å¾',
                'Block2_ShapePattern': 'Block 2: å½¢çŠ¶æ¨¡å¼ç‰¹å¾',
                'Block3_LocalPattern': 'Block 3: å±€éƒ¨æ¨¡å¼ç‰¹å¾',
                'Block4_ObjectPart': 'Block 4: å¯¹è±¡éƒ¨åˆ†ç‰¹å¾',
                'Block5_Semantic': 'Block 5: è¯­ä¹‰ç‰¹å¾'
            }

        fig.suptitle(block_descriptions.get(block_name, block_name), fontsize=16, fontweight='bold')
        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        filename = f"{block_name.lower()}_features"
        save_path = save_plot_with_timestamp(fig, save_dir, filename, show_plot=False)

        return save_path

    def _visualize_blocks_overview(self, image_tensor: torch.Tensor, image_path: str, save_dir: str) -> str:
        """
        ç”Ÿæˆæ‰€æœ‰Blockçš„æ¦‚è§ˆå›¾

        Args:
            image_tensor: è¾“å…¥å›¾åƒå¼ é‡
            image_path: åŸå§‹å›¾åƒè·¯å¾„
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # é€‰æ‹©æ¯ä¸ªBlockçš„ä»£è¡¨æ€§å±‚
        representative_layers = ['conv1_2', 'conv2_2', 'conv3_3', 'conv4_3', 'conv5_3']

        # æå–ä»£è¡¨æ€§ç‰¹å¾
        features = self.feature_extractor.extract_features(image_tensor, representative_layers)

        if not features:
            return None

        # åˆ›å»º5x9çš„å­å›¾å¸ƒå±€ (5ä¸ªBlockï¼Œæ¯ä¸ªBlockæ˜¾ç¤º8ä¸ªé€šé“+æ ‡é¢˜)
        fig, axes = plt.subplots(5, 9, figsize=(24, 15))

        # Blockä¿¡æ¯
        block_info = [
            ('conv1_2', 'Block1: Edge & Texture' if self.use_english_labels else 'Block1: è¾¹ç¼˜çº¹ç†'),
            ('conv2_2', 'Block2: Shape & Pattern' if self.use_english_labels else 'Block2: å½¢çŠ¶æ¨¡å¼'),
            ('conv3_3', 'Block3: Local Pattern' if self.use_english_labels else 'Block3: å±€éƒ¨æ¨¡å¼'),
            ('conv4_3', 'Block4: Object Part' if self.use_english_labels else 'Block4: å¯¹è±¡éƒ¨åˆ†'),
            ('conv5_3', 'Block5: Semantic' if self.use_english_labels else 'Block5: è¯­ä¹‰ç‰¹å¾')
        ]

        # æ˜¾ç¤ºåŸå§‹å›¾åƒï¼ˆåœ¨ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—ï¼‰
        try:
            original_image = Image.open(image_path).convert('RGB')
            axes[0, 0].imshow(original_image)
            axes[0, 0].set_title('Original' if self.use_english_labels else 'åŸå›¾', fontsize=10, fontweight='bold')
            axes[0, 0].axis('off')
        except:
            axes[0, 0].text(0.5, 0.5, 'Image\nNot Found', ha='center', va='center')
            axes[0, 0].axis('off')

        # ä¸ºæ¯ä¸ªBlockç”Ÿæˆæ¦‚è§ˆ
        for block_idx, (layer_name, block_title) in enumerate(block_info):
            if layer_name in features:
                feature_map = features[layer_name].squeeze().cpu().numpy()

                if len(feature_map.shape) == 3:  # [C, H, W]
                    # æ˜¾ç¤ºå‰8ä¸ªé€šé“
                    num_show = min(8, feature_map.shape[0])

                    for ch_idx in range(num_show):
                        col_idx = ch_idx + 1 if block_idx == 0 else ch_idx
                        if col_idx < 9:
                            ax = axes[block_idx, col_idx]

                            channel_data = feature_map[ch_idx]
                            if not (np.isnan(channel_data).any() or np.isinf(channel_data).any()):
                                ax.imshow(channel_data, cmap='viridis')
                                ax.set_title(f'Ch{ch_idx}', fontsize=8)
                            else:
                                ax.text(0.5, 0.5, 'Invalid', ha='center', va='center')
                            ax.axis('off')

                    # éšè—å¤šä½™çš„å­å›¾
                    start_col = num_show + 1 if block_idx == 0 else num_show
                    for col_idx in range(start_col, 9):
                        axes[block_idx, col_idx].axis('off')
                else:
                    # å¦‚æœä¸æ˜¯3Dç‰¹å¾å›¾ï¼Œéšè—æ‰€æœ‰é€šé“
                    start_col = 1 if block_idx == 0 else 0
                    for col_idx in range(start_col, 9):
                        axes[block_idx, col_idx].axis('off')
            else:
                # å¦‚æœæ²¡æœ‰è¯¥å±‚ç‰¹å¾ï¼Œéšè—æ‰€æœ‰é€šé“
                start_col = 1 if block_idx == 0 else 0
                for col_idx in range(start_col, 9):
                    axes[block_idx, col_idx].axis('off')

        # è®¾ç½®æ•´ä½“æ ‡é¢˜
        if self.use_english_labels:
            fig.suptitle('VGG16 Feature Maps Overview - All Blocks', fontsize=18, fontweight='bold')
        else:
            fig.suptitle('VGG16 ç‰¹å¾å›¾æ¦‚è§ˆ - æ‰€æœ‰Block', fontsize=18, fontweight='bold')

        plt.tight_layout()

        # ä¿å­˜æ¦‚è§ˆå›¾
        save_path = save_plot_with_timestamp(fig, save_dir, 'overview_all_blocks', show_plot=False)

        return save_path

    def visualize_grad_cam(self, image_tensor: torch.Tensor, image_path: str,
                          predicted_class: int, true_class: int, save_dir: str) -> str:
        """
        å¯è§†åŒ–Grad-CAMçƒ­åŠ›å›¾

        Args:
            image_tensor: è¾“å…¥å›¾åƒå¼ é‡
            image_path: åŸå§‹å›¾åƒè·¯å¾„
            predicted_class: é¢„æµ‹ç±»åˆ«ç´¢å¼•
            true_class: çœŸå®ç±»åˆ«ç´¢å¼•
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # ç”ŸæˆCAM - ç¡®ä¿å¼ é‡éœ€è¦æ¢¯åº¦
        try:
            # è®¾ç½®å¼ é‡éœ€è¦æ¢¯åº¦
            image_tensor = image_tensor.clone().detach().requires_grad_(True)

            cam_pred = self.grad_cam.generate_cam(image_tensor, predicted_class)
            cam_true = self.grad_cam.generate_cam(image_tensor, true_class)
        except Exception as e:
            print(f"âš ï¸  CAMç”Ÿæˆå¤±è´¥: {e}")
            return None

        # åŠ è½½åŸå›¾
        original_image = np.array(Image.open(image_path))

        # åˆ›å»ºçƒ­åŠ›å›¾å åŠ 
        def overlay_cam(image, cam, alpha=0.4):
            # è°ƒæ•´CAMå°ºå¯¸åˆ°å›¾åƒå°ºå¯¸
            cam_resized = cv2.resize(cam, (image.shape[1], image.shape[0]))

            # åˆ›å»ºçƒ­åŠ›å›¾
            heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)
            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)

            # å åŠ åˆ°åŸå›¾
            overlay = heatmap * alpha + image * (1 - alpha)
            return overlay.astype(np.uint8)

        # åˆ›å»ºå¯è§†åŒ–
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        # ç¬¬ä¸€è¡Œï¼šé¢„æµ‹ç±»åˆ«çš„CAM
        axes[0, 0].imshow(original_image)
        axes[0, 0].set_title('åŸå§‹å›¾åƒ', fontsize=12)
        axes[0, 0].axis('off')

        axes[0, 1].imshow(cam_pred, cmap='jet')
        axes[0, 1].set_title(f'é¢„æµ‹ç±»åˆ« CAM\n{self.class_names[predicted_class]}', fontsize=12)
        axes[0, 1].axis('off')

        overlay_pred = overlay_cam(original_image, cam_pred)
        axes[0, 2].imshow(overlay_pred)
        axes[0, 2].set_title(f'é¢„æµ‹ç±»åˆ«å åŠ å›¾\n{self.class_names[predicted_class]}', fontsize=12)
        axes[0, 2].axis('off')

        # ç¬¬äºŒè¡Œï¼šçœŸå®ç±»åˆ«çš„CAM
        axes[1, 0].imshow(original_image)
        axes[1, 0].set_title('åŸå§‹å›¾åƒ', fontsize=12)
        axes[1, 0].axis('off')

        axes[1, 1].imshow(cam_true, cmap='jet')
        axes[1, 1].set_title(f'çœŸå®ç±»åˆ« CAM\n{self.class_names[true_class]}', fontsize=12)
        axes[1, 1].axis('off')

        overlay_true = overlay_cam(original_image, cam_true)
        axes[1, 2].imshow(overlay_true)
        axes[1, 2].set_title(f'çœŸå®ç±»åˆ«å åŠ å›¾\n{self.class_names[true_class]}', fontsize=12)
        axes[1, 2].axis('off')

        plt.suptitle('Grad-CAM ç±»æ¿€æ´»æ˜ å°„å¯è§†åŒ–', fontsize=16, y=0.95)
        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        os.makedirs(save_dir, exist_ok=True)
        save_path = save_plot_with_timestamp(fig, save_dir, 'grad_cam', show_plot=False)

        return save_path

    def analyze_activation_patterns(self, image_tensors: List[torch.Tensor],
                                  labels: List[int], save_dir: str) -> str:
        """
        åˆ†æä¸åŒç±»åˆ«çš„æ¿€æ´»æ¨¡å¼

        Args:
            image_tensors: å›¾åƒå¼ é‡åˆ—è¡¨
            labels: å¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        layer_names = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']

        # å­˜å‚¨æ¯ä¸ªç±»åˆ«æ¯å±‚çš„å¹³å‡æ¿€æ´»å¼ºåº¦
        class_activations = {class_idx: {layer: [] for layer in layer_names}
                           for class_idx in range(len(self.class_names))}

        print("ğŸ” åˆ†ææ¿€æ´»æ¨¡å¼...")
        for i, (img_tensor, label) in enumerate(zip(image_tensors, labels)):
            try:
                # ç¡®ä¿å¼ é‡ç»´åº¦æ­£ç¡®
                if len(img_tensor.shape) == 3:  # [3, H, W]
                    input_tensor = img_tensor.unsqueeze(0)  # [1, 3, H, W]
                elif len(img_tensor.shape) == 4:  # [1, 3, H, W]
                    input_tensor = img_tensor.squeeze(0).unsqueeze(0) if img_tensor.shape[0] == 1 else img_tensor
                else:
                    print(f"âš ï¸  æ ·æœ¬ {i} å¼ é‡ç»´åº¦å¼‚å¸¸: {img_tensor.shape}")
                    continue

                features = self.feature_extractor.extract_features(input_tensor, layer_names)
            except Exception as e:
                print(f"âš ï¸  æ ·æœ¬ {i} ç‰¹å¾æå–å¤±è´¥: {e}")
                continue

            for layer_name in layer_names:
                if layer_name in features:
                    try:
                        # è®¡ç®—è¯¥å±‚çš„å¹³å‡æ¿€æ´»å¼ºåº¦
                        activation = features[layer_name].mean().item()
                        class_activations[label][layer_name].append(activation)
                    except Exception as e:
                        print(f"âš ï¸  å±‚ {layer_name} æ¿€æ´»è®¡ç®—å¤±è´¥: {e}")
                else:
                    print(f"âš ï¸  å±‚ {layer_name} ç‰¹å¾æœªæ‰¾åˆ°")

        # è®¡ç®—æ¯ä¸ªç±»åˆ«æ¯å±‚çš„å¹³å‡æ¿€æ´»å¼ºåº¦
        avg_activations = np.zeros((len(self.class_names), len(layer_names)))

        for class_idx in range(len(self.class_names)):
            for layer_idx, layer_name in enumerate(layer_names):
                if class_activations[class_idx][layer_name]:
                    avg_activations[class_idx, layer_idx] = np.mean(class_activations[class_idx][layer_name])

        # åˆ›å»ºçƒ­åŠ›å›¾
        fig, ax = plt.subplots(figsize=(10, 6))

        im = ax.imshow(avg_activations, cmap='viridis', aspect='auto')

        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(range(len(layer_names)))
        ax.set_xticklabels(layer_names, rotation=45)
        ax.set_yticks(range(len(self.class_names)))
        ax.set_yticklabels(self.class_names)

        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(len(self.class_names)):
            for j in range(len(layer_names)):
                text = ax.text(j, i, f'{avg_activations[i, j]:.3f}',
                             ha="center", va="center", color="white", fontsize=10)

        if self.use_english_labels:
            ax.set_title('Average Activation Intensity by Class and Layer', fontsize=14, pad=20)
            ax.set_xlabel('Convolutional Layers', fontsize=12)
            ax.set_ylabel('Classes', fontsize=12)
        else:
            ax.set_title('ä¸åŒç±»åˆ«åœ¨å„å±‚çš„å¹³å‡æ¿€æ´»å¼ºåº¦', fontsize=14, pad=20)
            ax.set_xlabel('å·ç§¯å±‚', fontsize=12)
            ax.set_ylabel('ç±»åˆ«', fontsize=12)

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax)
        if self.use_english_labels:
            cbar.set_label('Average Activation Intensity', fontsize=12)
        else:
            cbar.set_label('å¹³å‡æ¿€æ´»å¼ºåº¦', fontsize=12)

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        os.makedirs(save_dir, exist_ok=True)
        save_path = save_plot_with_timestamp(fig, save_dir, 'activation_patterns', show_plot=False)

        return save_path

    def compare_error_features(self, correct_samples: List[Tuple[torch.Tensor, str, int]],
                             error_samples: List[Tuple[torch.Tensor, str, int, int]],
                             save_dir: str) -> str:
        """
        å¯¹æ¯”æ­£ç¡®é¢„æµ‹å’Œé”™è¯¯é¢„æµ‹æ ·æœ¬çš„ç‰¹å¾å·®å¼‚

        Args:
            correct_samples: æ­£ç¡®æ ·æœ¬åˆ—è¡¨ [(tensor, path, true_label), ...]
            error_samples: é”™è¯¯æ ·æœ¬åˆ—è¡¨ [(tensor, path, true_label, pred_label), ...]
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        layer_names = ['conv3_1', 'conv4_1', 'conv5_1']  # é€‰æ‹©ä¸­é«˜å±‚ç‰¹å¾

        # åˆ†ææ­£ç¡®æ ·æœ¬çš„ç‰¹å¾
        print("ğŸ” åˆ†ææ­£ç¡®é¢„æµ‹æ ·æœ¬çš„ç‰¹å¾...")
        correct_features = {layer: [] for layer in layer_names}

        for tensor, _, _ in correct_samples[:10]:  # é™åˆ¶æ ·æœ¬æ•°é‡
            try:
                # ç¡®ä¿å¼ é‡ç»´åº¦æ­£ç¡® [1, 3, H, W]
                if len(tensor.shape) == 3:  # [3, H, W]
                    input_tensor = tensor.unsqueeze(0)  # [1, 3, H, W]
                elif len(tensor.shape) == 4:  # [1, 3, H, W]
                    input_tensor = tensor
                else:
                    print(f"âš ï¸  å¼ é‡ç»´åº¦å¼‚å¸¸: {tensor.shape}")
                    continue

                features = self.feature_extractor.extract_features(input_tensor, layer_names)
                for layer_name in layer_names:
                    if layer_name in features:
                        # è®¡ç®—ç‰¹å¾å›¾çš„ç»Ÿè®¡ä¿¡æ¯
                        feature_map = features[layer_name].squeeze().cpu().numpy()

                        # éªŒè¯ç‰¹å¾å›¾æ•°æ®æœ‰æ•ˆæ€§
                        if feature_map.size > 0 and not (np.isnan(feature_map).any() or np.isinf(feature_map).any()):
                            stats = {
                                'mean': np.mean(feature_map),
                                'std': np.std(feature_map),
                                'max': np.max(feature_map),
                                'sparsity': np.mean(feature_map == 0)  # ç¨€ç–æ€§
                            }
                            correct_features[layer_name].append(stats)
                        else:
                            print(f"âš ï¸  æ­£ç¡®æ ·æœ¬ {layer_name} ç‰¹å¾å›¾æ•°æ®æ— æ•ˆ")
                    else:
                        print(f"âš ï¸  æ­£ç¡®æ ·æœ¬æœªæ‰¾åˆ° {layer_name} ç‰¹å¾")
            except Exception as e:
                print(f"âš ï¸  å¤„ç†æ­£ç¡®æ ·æœ¬ç‰¹å¾æ—¶å‡ºé”™: {e}")

        # åˆ†æé”™è¯¯æ ·æœ¬çš„ç‰¹å¾
        print("ğŸ” åˆ†æé”™è¯¯é¢„æµ‹æ ·æœ¬çš„ç‰¹å¾...")
        error_features = {layer: [] for layer in layer_names}

        for tensor, _, _, _ in error_samples[:10]:  # é™åˆ¶æ ·æœ¬æ•°é‡
            try:
                # ç¡®ä¿å¼ é‡ç»´åº¦æ­£ç¡® [1, 3, H, W]
                if len(tensor.shape) == 3:  # [3, H, W]
                    input_tensor = tensor.unsqueeze(0)  # [1, 3, H, W]
                elif len(tensor.shape) == 4:  # [1, 3, H, W]
                    input_tensor = tensor
                else:
                    print(f"âš ï¸  å¼ é‡ç»´åº¦å¼‚å¸¸: {tensor.shape}")
                    continue

                features = self.feature_extractor.extract_features(input_tensor, layer_names)
                for layer_name in layer_names:
                    if layer_name in features:
                        feature_map = features[layer_name].squeeze().cpu().numpy()

                        # éªŒè¯ç‰¹å¾å›¾æ•°æ®æœ‰æ•ˆæ€§
                        if feature_map.size > 0 and not (np.isnan(feature_map).any() or np.isinf(feature_map).any()):
                            stats = {
                                'mean': np.mean(feature_map),
                                'std': np.std(feature_map),
                                'max': np.max(feature_map),
                                'sparsity': np.mean(feature_map == 0)
                            }
                            error_features[layer_name].append(stats)
                        else:
                            print(f"âš ï¸  é”™è¯¯æ ·æœ¬ {layer_name} ç‰¹å¾å›¾æ•°æ®æ— æ•ˆ")
                    else:
                        print(f"âš ï¸  é”™è¯¯æ ·æœ¬æœªæ‰¾åˆ° {layer_name} ç‰¹å¾")
            except Exception as e:
                print(f"âš ï¸  å¤„ç†é”™è¯¯æ ·æœ¬ç‰¹å¾æ—¶å‡ºé”™: {e}")

        # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        metrics = ['mean', 'std', 'max', 'sparsity']
        metric_names = ['å¹³å‡å€¼', 'æ ‡å‡†å·®', 'æœ€å¤§å€¼', 'ç¨€ç–æ€§']

        for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
            ax = axes[idx // 2, idx % 2]

            # è®¡ç®—æ¯å±‚çš„å¹³å‡ç»Ÿè®¡å€¼
            correct_values = []
            error_values = []

            for layer_name in layer_names:
                if correct_features[layer_name]:
                    correct_avg = np.mean([stats[metric] for stats in correct_features[layer_name]])
                    correct_values.append(correct_avg)
                else:
                    correct_values.append(0)

                if error_features[layer_name]:
                    error_avg = np.mean([stats[metric] for stats in error_features[layer_name]])
                    error_values.append(error_avg)
                else:
                    error_values.append(0)

            # ç»˜åˆ¶å¯¹æ¯”æŸ±çŠ¶å›¾
            x = np.arange(len(layer_names))
            width = 0.35

            bars1 = ax.bar(x - width/2, correct_values, width, label='æ­£ç¡®é¢„æµ‹', alpha=0.8)
            bars2 = ax.bar(x + width/2, error_values, width, label='é”™è¯¯é¢„æµ‹', alpha=0.8)

            ax.set_xlabel('å·ç§¯å±‚')
            ax.set_ylabel(metric_name)
            ax.set_title(f'ç‰¹å¾{metric_name}å¯¹æ¯”')
            ax.set_xticks(x)
            ax.set_xticklabels(layer_names)
            ax.legend()

            # æ·»åŠ æ•°å€¼æ ‡æ³¨
            for bar in bars1:
                height = bar.get_height()
                ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')

            for bar in bars2:
                height = bar.get_height()
                ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3), textcoords="offset points", ha='center', va='bottom')

        plt.suptitle('æ­£ç¡®é¢„æµ‹ vs é”™è¯¯é¢„æµ‹ ç‰¹å¾å¯¹æ¯”åˆ†æ', fontsize=16, y=0.95)
        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        os.makedirs(save_dir, exist_ok=True)
        save_path = save_plot_with_timestamp(fig, save_dir, 'error_feature_comparison', show_plot=False)

        return save_path

    def generate_comprehensive_feature_analysis(self, image_tensors: List[torch.Tensor],
                                              image_paths: List[str], true_labels: List[int],
                                              pred_labels: List[int], save_dir: str) -> Dict[str, str]:
        """
        ç”Ÿæˆç»¼åˆç‰¹å¾åˆ†ææŠ¥å‘Š

        Args:
            image_tensors: å›¾åƒå¼ é‡åˆ—è¡¨
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            true_labels: çœŸå®æ ‡ç­¾åˆ—è¡¨
            pred_labels: é¢„æµ‹æ ‡ç­¾åˆ—è¡¨
            save_dir: ä¿å­˜ç›®å½•

        Returns:
            Dict[str, str]: ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        print("ğŸ¨ å¼€å§‹ç”Ÿæˆç»¼åˆç‰¹å¾å¯è§†åŒ–åˆ†æ...")
        print(f"ğŸ“Š è¾“å…¥æ ·æœ¬æ•°é‡: {len(image_tensors)}")

        # åˆ›å»ºä¿å­˜ç›®å½•
        feature_dir = os.path.join(save_dir, 'feature_analysis')
        os.makedirs(feature_dir, exist_ok=True)

        results = {}

        # æ£€æŸ¥è¾“å…¥æ•°æ®
        if len(image_tensors) == 0:
            print("âŒ æ²¡æœ‰è¾“å…¥æ ·æœ¬ï¼Œè·³è¿‡ç‰¹å¾åˆ†æ")
            return results

        # 1. ç‰¹å¾å›¾å¯è§†åŒ–ï¼ˆé€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æ ·æœ¬ï¼‰
        print("ğŸ“Š ç”Ÿæˆç‰¹å¾å›¾å¯è§†åŒ–...")
        num_feature_samples = min(2, len(image_tensors))  # å‡å°‘æ ·æœ¬æ•°é‡
        sample_indices = np.random.choice(len(image_tensors), num_feature_samples, replace=False)
        print(f"ğŸ¯ é€‰æ‹© {num_feature_samples} ä¸ªæ ·æœ¬è¿›è¡Œç‰¹å¾å›¾å¯è§†åŒ–: {sample_indices}")

        for i, idx in enumerate(sample_indices):
            try:
                print(f"ğŸ” å¤„ç†ç‰¹å¾å›¾æ ·æœ¬ {i+1}/{len(sample_indices)}: {os.path.basename(image_paths[idx])}")
                feature_map_dir = os.path.join(feature_dir, 'feature_maps')

                # ä½¿ç”¨æ–°çš„æŒ‰Blockç»“æ„çš„ç‰¹å¾å›¾å¯è§†åŒ–
                try:
                    block_results = self.visualize_feature_maps_by_blocks(
                        image_tensors[idx], image_paths[idx], feature_map_dir
                    )

                    if block_results:
                        # å°†æ¯ä¸ªBlockçš„ç»“æœæ·»åŠ åˆ°æ€»ç»“æœä¸­
                        for block_name, save_path in block_results.items():
                            results[f'sample_{i+1}_{block_name}'] = save_path
                        print(f"  âœ… ç‰¹å¾å›¾ç”ŸæˆæˆåŠŸ: {len(block_results)} ä¸ªBlock")
                    else:
                        print(f"  âš ï¸  ç‰¹å¾å›¾ç”Ÿæˆå¤±è´¥")

                except Exception as e:
                    print(f"  âŒ ç‰¹å¾å›¾ç”Ÿæˆå¼‚å¸¸: {e}")

                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            except Exception as e:
                print(f"âš ï¸  æ ·æœ¬ {i+1} ç‰¹å¾å›¾ç”Ÿæˆå¼‚å¸¸: {e}")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

        # 2. Grad-CAMå¯è§†åŒ–ï¼ˆé€‰æ‹©æ­£ç¡®å’Œé”™è¯¯çš„æ ·æœ¬ï¼‰
        print("ğŸ”¥ ç”ŸæˆGrad-CAMå¯è§†åŒ–...")
        try:
            grad_cam_dir = os.path.join(feature_dir, 'grad_cam')

            # æ‰¾åˆ°æ­£ç¡®å’Œé”™è¯¯çš„æ ·æœ¬
            correct_indices = [i for i, (true, pred) in enumerate(zip(true_labels, pred_labels)) if true == pred]
            error_indices = [i for i, (true, pred) in enumerate(zip(true_labels, pred_labels)) if true != pred]

            print(f"ğŸ“Š æ­£ç¡®æ ·æœ¬: {len(correct_indices)} ä¸ª, é”™è¯¯æ ·æœ¬: {len(error_indices)} ä¸ª")

            # å¯è§†åŒ–å‡ ä¸ªä»£è¡¨æ€§æ ·æœ¬
            if correct_indices:
                print("ğŸ¯ ç”Ÿæˆæ­£ç¡®é¢„æµ‹æ ·æœ¬çš„Grad-CAM...")
                idx = correct_indices[0]
                try:
                    save_path = self.visualize_grad_cam(
                        image_tensors[idx], image_paths[idx], pred_labels[idx], true_labels[idx], grad_cam_dir
                    )
                    results['grad_cam_correct'] = save_path
                    print("  âœ… æ­£ç¡®æ ·æœ¬Grad-CAMç”ŸæˆæˆåŠŸ")
                except Exception as e:
                    print(f"  âš ï¸  æ­£ç¡®æ ·æœ¬Grad-CAMç”Ÿæˆå¤±è´¥: {e}")

            if error_indices:
                print("ğŸ¯ ç”Ÿæˆé”™è¯¯é¢„æµ‹æ ·æœ¬çš„Grad-CAM...")
                idx = error_indices[0]
                try:
                    save_path = self.visualize_grad_cam(
                        image_tensors[idx], image_paths[idx], pred_labels[idx], true_labels[idx], grad_cam_dir
                    )
                    results['grad_cam_error'] = save_path
                    print("  âœ… é”™è¯¯æ ·æœ¬Grad-CAMç”ŸæˆæˆåŠŸ")
                except Exception as e:
                    print(f"  âš ï¸  é”™è¯¯æ ·æœ¬Grad-CAMç”Ÿæˆå¤±è´¥: {e}")

            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        except Exception as e:
            print(f"âš ï¸  Grad-CAMå¯è§†åŒ–å¤±è´¥: {e}")

        # 3. æ¿€æ´»æ¨¡å¼åˆ†æ
        print("âš¡ åˆ†ææ¿€æ´»æ¨¡å¼...")
        try:
            activation_dir = os.path.join(feature_dir, 'activation_analysis')
            save_path = self.analyze_activation_patterns(image_tensors, true_labels, activation_dir)
            results['activation_patterns'] = save_path
            print("  âœ… æ¿€æ´»æ¨¡å¼åˆ†æå®Œæˆ")
        except Exception as e:
            print(f"  âš ï¸  æ¿€æ´»æ¨¡å¼åˆ†æå¤±è´¥: {e}")

        # 4. é”™è¯¯æ ·æœ¬ç‰¹å¾å¯¹æ¯”
        if correct_indices and error_indices:
            print("ğŸ” å¯¹æ¯”é”™è¯¯æ ·æœ¬ç‰¹å¾...")
            try:
                comparison_dir = os.path.join(feature_dir, 'error_comparison')

                # å‡†å¤‡æ ·æœ¬æ•°æ®ï¼ˆå‡å°‘æ ·æœ¬æ•°é‡ï¼‰
                max_comparison_samples = min(3, len(correct_indices), len(error_indices))
                correct_samples = [(image_tensors[i], image_paths[i], true_labels[i])
                                 for i in correct_indices[:max_comparison_samples]]
                error_samples = [(image_tensors[i], image_paths[i], true_labels[i], pred_labels[i])
                               for i in error_indices[:max_comparison_samples]]

                print(f"ğŸ“Š å¯¹æ¯”æ ·æœ¬æ•°é‡: æ­£ç¡® {len(correct_samples)}, é”™è¯¯ {len(error_samples)}")
                save_path = self.compare_error_features(correct_samples, error_samples, comparison_dir)
                results['error_feature_comparison'] = save_path
                print("  âœ… é”™è¯¯æ ·æœ¬ç‰¹å¾å¯¹æ¯”å®Œæˆ")
            except Exception as e:
                print(f"  âš ï¸  é”™è¯¯æ ·æœ¬ç‰¹å¾å¯¹æ¯”å¤±è´¥: {e}")
        else:
            print("âš ï¸  æ²¡æœ‰è¶³å¤Ÿçš„æ­£ç¡®æˆ–é”™è¯¯æ ·æœ¬è¿›è¡Œå¯¹æ¯”åˆ†æ")

        # 5. t-SNEé™ç»´å¯è§†åŒ–
        print("ğŸ”¬ ç”Ÿæˆt-SNEé™ç»´å¯è§†åŒ–...")
        try:
            tsne_dir = os.path.join(feature_dir, 'tsne_analysis')

            # åˆ›å»ºæ•°æ®åŠ è½½å™¨ç”¨äºt-SNEåˆ†æ
            from torch.utils.data import TensorDataset, DataLoader

            # å‡†å¤‡æ•°æ®
            tensor_list = []
            label_list = []
            for i, (tensor, label) in enumerate(zip(image_tensors, true_labels)):
                if i < 20:  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥æé«˜é€Ÿåº¦
                    tensor_list.append(tensor)
                    label_list.append(label)

            if tensor_list:
                batch_tensors = torch.stack(tensor_list)
                batch_labels = torch.tensor(label_list)

                temp_dataset = TensorDataset(batch_tensors, batch_labels)
                temp_dataloader = DataLoader(temp_dataset, batch_size=4, shuffle=False)

                # å¯¹å…³é”®å±‚è¿›è¡Œt-SNEåˆ†æ
                key_layers = ['conv1_2', 'conv3_2', 'conv5_3']  # é€‰æ‹©ä»£è¡¨æ€§å±‚

                for layer_name in key_layers:
                    try:
                        print(f"  ğŸ” åˆ†æ {layer_name} å±‚...")
                        features, labels = self.tsne_visualizer.extract_features_for_tsne(
                            self.model, temp_dataloader, layer_name, self.device, max_samples=20
                        )

                        if features is not None and labels is not None:
                            save_path = self.tsne_visualizer.visualize_tsne(
                                features, labels, layer_name, tsne_dir
                            )
                            results[f'tsne_{layer_name}'] = save_path
                            print(f"    âœ… {layer_name} t-SNEå®Œæˆ")
                        else:
                            print(f"    âš ï¸  {layer_name} ç‰¹å¾æå–å¤±è´¥")

                    except Exception as e:
                        print(f"    âš ï¸  {layer_name} t-SNEåˆ†æå¤±è´¥: {e}")

                print("  âœ… t-SNEé™ç»´å¯è§†åŒ–å®Œæˆ")
            else:
                print("  âš ï¸  æ²¡æœ‰è¶³å¤Ÿçš„æ ·æœ¬è¿›è¡Œt-SNEåˆ†æ")

        except Exception as e:
            print(f"  âš ï¸  t-SNEé™ç»´å¯è§†åŒ–å¤±è´¥: {e}")

        # æœ€ç»ˆæ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print("âœ… ç‰¹å¾å¯è§†åŒ–åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š ç”Ÿæˆç»“æœæ•°é‡: {len(results)}")
        for key in results.keys():
            print(f"  â€¢ {key}")
        return results


# ==================== ç‹¬ç«‹è¿è¡Œæ”¯æŒå‡½æ•° ====================

def load_model_for_analysis(model_path: str, config: dict, device='cpu'):
    """
    åŠ è½½æ¨¡å‹ç”¨äºç‰¹å¾åˆ†æ

    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        config: é…ç½®å­—å…¸
        device: è®¡ç®—è®¾å¤‡

    Returns:
        torch.nn.Module: åŠ è½½çš„æ¨¡å‹
    """
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")

    # ä»é…ç½®æ–‡ä»¶è·å–æ¨¡å‹ä¿¡æ¯
    model_config = config['model'].copy()
    model_config['num_classes'] = config['data']['num_classes']
    model_config['pretrained'] = False  # åˆ†ææ—¶ä¸éœ€è¦é¢„è®­ç»ƒæƒé‡

    print(f"ğŸ¤– æ¨¡å‹ç±»å‹: {model_config['name']}")
    print(f"ğŸ“Š åˆ†ç±»æ•°é‡: {model_config['num_classes']}")

    # åˆ›å»ºæ¨¡å‹
    try:
        model = create_model(model_config)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ: {model.get_name() if hasattr(model, 'get_name') else model_config['name']}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return None

    # åŠ è½½æƒé‡
    try:
        if device == 'cpu':
            state_dict = torch.load(model_path, map_location='cpu')
        else:
            state_dict = torch.load(model_path)

        # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
        if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:
            model.load_state_dict(state_dict['model_state_dict'])
            print(f"ğŸ“‹ åŠ è½½æ£€æŸ¥ç‚¹æ ¼å¼æƒé‡")
        else:
            model.load_state_dict(state_dict)
            print(f"ğŸ“‹ åŠ è½½ç›´æ¥æƒé‡æ ¼å¼")

        model.to(device)
        model.eval()

        print(f"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        return model

    except Exception as e:
        print(f"âŒ æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e}")
        return None


def create_analysis_dataloader(config: dict, project_root: str, max_samples=50):
    """
    åˆ›å»ºç”¨äºç‰¹å¾åˆ†æçš„æ•°æ®åŠ è½½å™¨

    Args:
        config: é…ç½®å­—å…¸
        project_root: é¡¹ç›®æ ¹ç›®å½•
        max_samples: æœ€å¤§æ ·æœ¬æ•°é‡

    Returns:
        DataLoader: æ•°æ®åŠ è½½å™¨
    """
    # æ ‡æ³¨æ–‡ä»¶è·¯å¾„
    annotation_path = os.path.join(project_root, config['data']['annotation_file'])

    if not os.path.exists(annotation_path):
        print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {annotation_path}")
        return None

    # è¯»å–æ ‡æ³¨æ–‡ä»¶
    with open(annotation_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    lines = [line.strip() for line in lines if line.strip()]

    if len(lines) == 0:
        print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸ºç©º: {annotation_path}")
        return None

    # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥æé«˜åˆ†æé€Ÿåº¦
    if len(lines) > max_samples:
        print(f"ğŸ“Š é™åˆ¶æ ·æœ¬æ•°é‡: {len(lines)} -> {max_samples}")
        lines = lines[:max_samples]

    print(f"ğŸ“Š åŠ è½½æ ‡æ³¨æ–‡ä»¶: {annotation_path}")
    print(f"   åˆ†ææ ·æœ¬æ•°: {len(lines)}")

    # åˆ›å»ºæ•°æ®é›†ï¼ˆä¸ä½¿ç”¨æ•°æ®å¢å¼ºï¼‰
    input_shape = config['data']['input_shape']
    dataset = DataGenerator(lines, input_shape, random=False)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=4,  # å°æ‰¹æ¬¡ä»¥èŠ‚çœå†…å­˜
        shuffle=False,
        num_workers=0,
        drop_last=False
    )

    return dataloader


def main(train_path: str):
    """
    ä¸»å‡½æ•° - ç‹¬ç«‹ç‰¹å¾å¯è§†åŒ–åˆ†æ

    Args:
        train_path: è®­ç»ƒè·¯å¾„ï¼Œä¾‹å¦‚ "/var/yjs/zes/vgg/outputs/logs/train_20250826_041814"
    """
    print("ğŸ¨" + "="*80)
    print("ğŸ¨ ç‹¬ç«‹ç‰¹å¾å¯è§†åŒ–åˆ†æå·¥å…·")
    print("ğŸ¨ ä¸“é—¨ç”¨äºæ··å‡åœŸåè½åº¦è¯†åˆ«çš„æ·±åº¦ç‰¹å¾åˆ†æ")
    print("ğŸ¨" + "="*80)

    # ==================== å­—ä½“è®¾ç½® ====================
    print("ğŸ”¤ è®¾ç½®å­—ä½“...")
    setup_matplotlib_font()
    font_manager = get_font_manager()
    font_manager.print_font_status()

    # å¦‚æœæ²¡æœ‰ä¸­æ–‡å­—ä½“ï¼Œæ˜¾ç¤ºå®‰è£…æŒ‡å—
    if not font_manager.has_chinese_font:
        from src.utils.font_manager import print_font_installation_guide
        print_font_installation_guide()

    # ==================== è·¯å¾„å¤„ç† ====================
    print(f"ğŸ“ è¾“å…¥è®­ç»ƒè·¯å¾„: {train_path}")

    # è·å–é¡¹ç›®æ ¹ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(script_dir))

    # æå–æ—¶é—´æˆ³
    timestamp = extract_timestamp_from_path(train_path)
    if not timestamp:
        print("âŒ æ— æ³•ä»è·¯å¾„ä¸­æå–æ—¶é—´æˆ³")
        return

    print(f"â° æå–æ—¶é—´æˆ³: {timestamp}")

    # ==================== è‡ªåŠ¨å¯»æ‰¾æ–‡ä»¶ ====================
    # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
    model_path = find_model_file(project_root, timestamp)
    if not model_path:
        print("âŒ æœªæ‰¾åˆ°å¯¹åº”çš„æ¨¡å‹æ–‡ä»¶")
        return

    # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    config, config_source = load_training_config(train_path, project_root)
    if config is None:
        print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶")
        return

    # ç”Ÿæˆè¾“å‡ºç›®å½•
    eval_timestamp = generate_eval_timestamp(timestamp)
    output_dir = os.path.join(project_root, 'outputs', 'evaluation', eval_timestamp, 'feature_analysis')

    print(f"ğŸ¤– æ¨¡å‹æ–‡ä»¶: {model_path}")
    print(f"ğŸ“„ é…ç½®æ¥æº: {config_source}")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir}")

    # ==================== è®¾å¤‡æ£€æŸ¥ ====================
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

    # ==================== åŠ è½½æ¨¡å‹ ====================
    model = load_model_for_analysis(model_path, config, device)
    if model is None:
        return

    # ==================== åˆ›å»ºæ•°æ®åŠ è½½å™¨ ====================
    dataloader = create_analysis_dataloader(config, project_root, max_samples=30)
    if dataloader is None:
        return

    # ==================== åˆå§‹åŒ–å¯è§†åŒ–å™¨ ====================
    class_names = config['data']['class_names']
    visualizer = FeatureVisualizer(model, class_names, device)

    print(f"ğŸ¯ ç±»åˆ«åç§°: {class_names}")
    print(f"ğŸ“Š å¼€å§‹ç‰¹å¾å¯è§†åŒ–åˆ†æ...")

    # ==================== æ‰§è¡Œåˆ†æ ====================
    try:
        # æ”¶é›†æ ·æœ¬æ•°æ®
        sample_tensors = []
        sample_paths = []
        sample_labels = []

        print("ğŸ“Š æ”¶é›†æ ·æœ¬æ•°æ®...")
        with torch.no_grad():
            for batch_idx, (images, labels, paths) in enumerate(dataloader):
                sample_tensors.extend([img.unsqueeze(0) for img in images])
                sample_paths.extend(paths)
                sample_labels.extend(labels.cpu().numpy())

                if len(sample_tensors) >= 10:  # é™åˆ¶æ ·æœ¬æ•°é‡
                    break

        print(f"âœ… æ”¶é›†å®Œæˆ: {len(sample_tensors)} ä¸ªæ ·æœ¬")

        # è·å–çœŸå®çš„é¢„æµ‹æ ‡ç­¾
        print("ğŸ”® æ‰§è¡Œæ¨¡å‹æ¨ç†è·å–é¢„æµ‹æ ‡ç­¾...")
        pred_labels = []
        model.eval()
        with torch.no_grad():
            for i, tensor in enumerate(sample_tensors):
                try:
                    tensor = tensor.to(device)
                    outputs = model(tensor)
                    _, predicted = torch.max(outputs, 1)
                    pred_labels.append(predicted.cpu().item())
                    print(f"  æ ·æœ¬ {i+1}: çœŸå®={sample_labels[i]}, é¢„æµ‹={predicted.cpu().item()}")
                except Exception as e:
                    print(f"  âš ï¸  æ ·æœ¬ {i+1} æ¨ç†å¤±è´¥: {e}")
                    pred_labels.append(sample_labels[i])  # ä½¿ç”¨çœŸå®æ ‡ç­¾ä½œä¸ºå¤‡é€‰

        print(f"âœ… æ¨ç†å®Œæˆ: {len(pred_labels)} ä¸ªé¢„æµ‹ç»“æœ")

        # æ‰§è¡Œç»¼åˆç‰¹å¾åˆ†æ
        results = visualizer.generate_comprehensive_feature_analysis(
            image_tensors=sample_tensors,
            image_paths=sample_paths,
            true_labels=sample_labels,
            pred_labels=pred_labels,  # ä½¿ç”¨çœŸå®çš„é¢„æµ‹æ ‡ç­¾
            save_dir=output_dir
        )

        print("\nğŸ‰ ç‰¹å¾å¯è§†åŒ–åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        print("ğŸ“Š ç”Ÿæˆçš„åˆ†ææ–‡ä»¶:")
        for key, path in results.items():
            if path:
                print(f"   â€¢ {key}: {os.path.basename(path)}")

    except Exception as e:
        print(f"âŒ ç‰¹å¾åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def test_font_support():
    """æµ‹è¯•å­—ä½“æ”¯æŒæƒ…å†µ"""
    print("ğŸ”¤ æµ‹è¯•å­—ä½“æ”¯æŒ...")

    # å¯¼å…¥å­—ä½“ç®¡ç†å™¨
    from src.utils.font_manager import setup_matplotlib_font, get_font_manager, print_font_installation_guide
    import matplotlib.font_manager as fm

    # è®¾ç½®å­—ä½“
    setup_matplotlib_font()
    font_manager = get_font_manager()

    # æ˜¾ç¤ºå­—ä½“çŠ¶æ€
    print(f"æ“ä½œç³»ç»Ÿ: {font_manager.system}")
    print(f"å½“å‰å­—ä½“: {font_manager.current_font}")
    print(f"ä¸­æ–‡æ”¯æŒ: {'æ˜¯' if font_manager.has_chinese_font else 'å¦'}")

    # æ˜¾ç¤ºå¯ç”¨å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    chinese_fonts = ['SimHei', 'Microsoft YaHei', 'SimSun', 'WenQuanYi Micro Hei', 'Noto Sans CJK SC']

    print("\nğŸ” æ£€æŸ¥ä¸­æ–‡å­—ä½“:")
    for font in chinese_fonts:
        status = "âœ…" if font in available_fonts else "âŒ"
        print(f"  {status} {font}")

    if not font_manager.has_chinese_font:
        print("\nğŸ’¡ å­—ä½“å®‰è£…å»ºè®®:")
        print_font_installation_guide()

    return font_manager.has_chinese_font


if __name__ == "__main__":
    """
    ç‹¬ç«‹è¿è¡Œé…ç½®

    ä½¿ç”¨æ–¹æ³•:
    1. ä¿®æ”¹ä¸‹é¢çš„ train_path ä¸ºä½ çš„è®­ç»ƒè·¯å¾„
    2. åœ¨ PyCharm ä¸­ç›´æ¥è¿è¡Œæ­¤è„šæœ¬
    """

    # é¦–å…ˆæµ‹è¯•å­—ä½“æ”¯æŒ
    print("="*60)
    has_chinese = test_font_support()
    print("="*60)

    if not has_chinese:
        print("âš ï¸  æ£€æµ‹åˆ°ä¸­æ–‡å­—ä½“é—®é¢˜ï¼Œå»ºè®®å…ˆå®‰è£…ä¸­æ–‡å­—ä½“")
        print("   ç»§ç»­è¿è¡Œå°†ä½¿ç”¨è‹±æ–‡æ ‡é¢˜...")
        input("æŒ‰å›è½¦é”®ç»§ç»­...")

    # ğŸ¯ åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„è®­ç»ƒè·¯å¾„
    train_path = "/var/yjs/zes/vgg/outputs/logs/train_20250826_041814"

    # æ‰§è¡Œåˆ†æ
    main(train_path)
