"""
æ¨¡å‹è¯„ä¼°ä¸»è„šæœ¬
å¯¹è®­ç»ƒå¥½çš„æ··å‡åœŸåè½åº¦åˆ†ç±»æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn.functional as F
import numpy as np
import yaml
import datetime
from torch.utils.data import DataLoader
from tqdm import tqdm

from src.models.vgg import vgg16
from src.data.dataset import DataGenerator
from src.evaluation import ReportGenerator


def load_config(config_path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return None
    except yaml.YAMLError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return None


def load_model(model_path, num_classes=3, device='cpu'):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        model_path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾„
        num_classes (int): åˆ†ç±»æ•°é‡
        device (str): è®¾å¤‡ç±»å‹
        
    Returns:
        torch.nn.Module: åŠ è½½çš„æ¨¡å‹
    """
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆè¯„ä¼°æ—¶dropoutä¸å½±å“ç»“æœï¼Œä½¿ç”¨é»˜è®¤å€¼å³å¯ï¼‰
    model = vgg16(pretrained=False, num_classes=num_classes)
    
    # åŠ è½½æƒé‡
    try:
        if device == 'cpu':
            state_dict = torch.load(model_path, map_location='cpu')
        else:
            state_dict = torch.load(model_path)
        
        model.load_state_dict(state_dict)
        model.to(device)
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None


def evaluate_model(model, dataloader, device='cpu'):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        model (torch.nn.Module): æ¨¡å‹
        dataloader (DataLoader): æ•°æ®åŠ è½½å™¨
        device (str): è®¾å¤‡ç±»å‹
        
    Returns:
        tuple: (çœŸå®æ ‡ç­¾, é¢„æµ‹æ ‡ç­¾, é¢„æµ‹æ¦‚ç‡, å›¾åƒè·¯å¾„)
    """
    print("ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    
    model.eval()
    all_true_labels = []
    all_pred_labels = []
    all_pred_probs = []
    all_image_paths = []
    
    with torch.no_grad():
        for batch_idx, (images, labels, image_paths) in enumerate(tqdm(dataloader, desc="è¯„ä¼°è¿›åº¦")):
            images = images.to(device)
            labels = labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            probabilities = F.softmax(outputs, dim=1)
            predicted = torch.argmax(outputs, dim=1)
            
            # æ”¶é›†ç»“æœ
            all_true_labels.extend(labels.cpu().numpy())
            all_pred_labels.extend(predicted.cpu().numpy())
            all_pred_probs.extend(probabilities.cpu().numpy())
            all_image_paths.extend(image_paths)
    
    return (np.array(all_true_labels), 
            np.array(all_pred_labels), 
            np.array(all_pred_probs),
            all_image_paths)


def create_evaluation_dataloader(config, project_root):
    """
    åˆ›å»ºè¯„ä¼°æ•°æ®åŠ è½½å™¨
    
    Args:
        config (dict): é…ç½®å­—å…¸
        project_root (str): é¡¹ç›®æ ¹ç›®å½•
        
    Returns:
        DataLoader: è¯„ä¼°æ•°æ®åŠ è½½å™¨
    """
    # è¯»å–æ ‡æ³¨æ–‡ä»¶
    annotation_path = os.path.join(project_root, config['data']['annotation_file'])
    
    if not os.path.exists(annotation_path):
        print(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {annotation_path}")
        return None
    
    with open(annotation_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # è§£ææ ‡æ³¨
    annotation_lines = []
    for line in lines:
        line = line.strip()
        if line:
            annotation_lines.append(line)
    
    print(f"ğŸ“Š åŠ è½½æ ‡æ³¨æ–‡ä»¶: {len(annotation_lines)} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆè¯„ä¼°æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼ºï¼‰
    dataset = DataGenerator(
        annotation_lines=annotation_lines,
        input_shape=config['data']['input_shape'],
        random=False,  # è¯„ä¼°æ—¶ä¸ä½¿ç”¨éšæœºå¢å¼º
        augmentation_config=None
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=config['dataloader']['batch_size'],
        shuffle=False,  # è¯„ä¼°æ—¶ä¸æ‰“ä¹±
        num_workers=config['dataloader']['num_workers']
    )
    
    return dataloader


def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    
    # ==================== é…ç½®å‚æ•° ====================
    # è·å–é¡¹ç›®è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = os.path.join(project_root, 'configs', 'training_config.yaml')
    
    # æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨latestç›®å½•ä¸­çš„æœ€æ–°æ¨¡å‹ï¼‰
    model_path = os.path.join(project_root, 'models', 'checkpoints', 'latest', 'best_model.pth')
    
    # è¯„ä¼°ç»“æœä¿å­˜ç›®å½•ï¼ˆæ·»åŠ æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    base_eval_dir = os.path.join(project_root, 'outputs', 'evaluation')
    save_dir = os.path.join(base_eval_dir, f"eval_{timestamp}")

    # åŒæ—¶ä¿å­˜åˆ°latestç›®å½•
    latest_eval_dir = os.path.join(base_eval_dir, "latest")
    
    print("ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°")
    print("="*80)
    print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {config_path}")
    print(f"ğŸ¤– æ¨¡å‹æ–‡ä»¶: {model_path}")
    print(f"ğŸ’¾ ä¿å­˜ç›®å½•: {save_dir}")
    print("="*80)
    
    # ==================== åŠ è½½é…ç½® ====================
    config = load_config(config_path)
    if config is None:
        print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
        return
    
    # ==================== è®¾å¤‡æ£€æŸ¥ ====================
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ==================== æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ ====================
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒæˆ–æ£€æŸ¥æ¨¡å‹è·¯å¾„")
        return
    
    # ==================== åŠ è½½æ¨¡å‹ ====================
    num_classes = config['data']['num_classes']
    model = load_model(model_path, num_classes, device)
    if model is None:
        return
    
    # ==================== åˆ›å»ºæ•°æ®åŠ è½½å™¨ ====================
    dataloader = create_evaluation_dataloader(config, project_root)
    if dataloader is None:
        return
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(dataloader.dataset)}")
    print(f"   æ‰¹æ¬¡å¤§å°: {dataloader.batch_size}")
    print(f"   æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
    
    # ==================== æ¨¡å‹è¯„ä¼° ====================
    y_true, y_pred, y_prob, image_paths = evaluate_model(model, dataloader, device)
    
    print(f"âœ… è¯„ä¼°å®Œæˆ")
    print(f"   æ€»æ ·æœ¬æ•°: {len(y_true)}")
    print(f"   é¢„æµ‹å‡†ç¡®ç‡: {np.mean(y_true == y_pred):.4f}")
    
    # ==================== ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š ====================
    class_names = config['data']['class_names']
    model_name = f"VGG16-{config['data']['num_classes']}ç±»"
    
    # åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
    report_generator = ReportGenerator(class_names, model_name)
    
    # ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Šï¼ˆä¿å­˜åˆ°æ—¶é—´æˆ³ç›®å½•ï¼‰
    results = report_generator.generate_comprehensive_report(
        y_true=y_true,
        y_pred=y_pred,
        y_prob=y_prob,
        image_paths=image_paths,
        save_dir=save_dir
    )

    # åŒæ—¶ä¿å­˜åˆ°latestç›®å½•
    print(f"\nğŸ“‹ åŒæ—¶ä¿å­˜åˆ°latestç›®å½•: {latest_eval_dir}")
    report_generator.generate_comprehensive_report(
        y_true=y_true,
        y_pred=y_pred,
        y_prob=y_prob,
        image_paths=image_paths,
        save_dir=latest_eval_dir
    )
    
    # ==================== è¾“å‡ºå…³é”®ç»“æœ ====================
    print("\n" + "="*80)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
    print("="*80)
    print(f"æ•´ä½“å‡†ç¡®ç‡: {results['metrics']['accuracy']:.4f}")
    print(f"å®å¹³å‡F1-Score: {results['metrics']['f1_macro']:.4f}")
    print(f"é”™è¯¯æ ·æœ¬æ•°: {results['error_info']['total_errors']}")
    print(f"é”™è¯¯ç‡: {results['error_info']['error_rate']:.4f}")
    
    if 'top2_accuracy' in results['metrics']:
        print(f"Top-2å‡†ç¡®ç‡: {results['metrics']['top2_accuracy']:.4f}")
    
    print(f"ç›¸é‚»ç±»åˆ«æ··æ·†ç‡: {results['metrics']['adjacent_confusion_rate']:.4f}")
    print(f"ä¸¥é‡é”™è¯¯ç‡: {results['metrics']['severe_error_rate']:.4f}")
    
    print("\nå„ç±»åˆ«æ€§èƒ½:")
    for class_name in class_names:
        precision = results['metrics'][f'precision_{class_name}']
        recall = results['metrics'][f'recall_{class_name}']
        f1 = results['metrics'][f'f1_{class_name}']
        print(f"  {class_name}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")
    
    print("="*80)
    print("ğŸ‰ è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ä¿å­˜çš„æ–‡ä»¶ã€‚")


if __name__ == "__main__":
    main()
