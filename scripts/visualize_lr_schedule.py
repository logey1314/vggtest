"""
å­¦ä¹ ç‡è°ƒåº¦å™¨å¯è§†åŒ–è„šæœ¬
ç”¨äºé¢„è§ˆä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å˜åŒ–æ›²çº¿
"""
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np

def visualize_lr_schedules():
    """å¯è§†åŒ–ä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å˜åŒ–æ›²çº¿"""
    
    # é…ç½®å‚æ•°
    EPOCHS = 20
    LEARNING_RATE = 0.0001
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ç”¨äºæµ‹è¯•
    model = torch.nn.Linear(10, 1)
    
    # ä¸åŒè°ƒåº¦å™¨é…ç½®
    schedulers_config = {
        'StepLR': {
            'scheduler': lambda opt: optim.lr_scheduler.StepLR(opt, step_size=7, gamma=0.5),
            'color': 'blue',
            'linestyle': '-'
        },
        'MultiStepLR': {
            'scheduler': lambda opt: optim.lr_scheduler.MultiStepLR(opt, milestones=[10, 15], gamma=0.1),
            'color': 'red',
            'linestyle': '--'
        },
        'CosineAnnealingLR': {
            'scheduler': lambda opt: optim.lr_scheduler.CosineAnnealingLR(opt, T_max=20, eta_min=1e-6),
            'color': 'green',
            'linestyle': '-.'
        },
        'ReduceLROnPlateau': {
            'scheduler': lambda opt: optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=3),
            'color': 'orange',
            'linestyle': ':'
        }
    }
    
    plt.figure(figsize=(12, 8))
    
    for name, config in schedulers_config.items():
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
        scheduler = config['scheduler'](optimizer)
        
        learning_rates = []
        epochs = list(range(EPOCHS))
        
        for epoch in range(EPOCHS):
            # è®°å½•å½“å‰å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            learning_rates.append(current_lr)
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
            if name == 'ReduceLROnPlateau':
                # æ¨¡æ‹ŸéªŒè¯æŸå¤±å˜åŒ–ï¼ˆå‰æœŸä¸‹é™ï¼ŒåæœŸå¹³ç¨³ï¼‰
                if epoch < 5:
                    fake_val_loss = 1.0 - epoch * 0.1
                elif epoch < 10:
                    fake_val_loss = 0.5 - (epoch - 5) * 0.05
                else:
                    fake_val_loss = 0.25 + np.random.normal(0, 0.01)  # æ·»åŠ å™ªå£°æ¨¡æ‹Ÿå¹³ç¨³æœŸ
                scheduler.step(fake_val_loss)
            else:
                scheduler.step()
        
        # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿
        plt.plot(epochs, learning_rates, 
                label=name, 
                color=config['color'], 
                linestyle=config['linestyle'],
                linewidth=2,
                marker='o',
                markersize=4)
    
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Learning Rate', fontsize=12)
    plt.title('ä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å˜åŒ–æ›²çº¿', fontsize=14, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°åæ ‡æ›´å¥½åœ°æ˜¾ç¤ºå˜åŒ–
    
    # æ·»åŠ è¯´æ˜æ–‡æœ¬
    plt.text(0.02, 0.98, 
             'è¯´æ˜ï¼š\n'
             'â€¢ StepLR: æ¯7ä¸ªepoché™ä½50%\n'
             'â€¢ MultiStepLR: ç¬¬10,15ä¸ªepoché™ä½90%\n'
             'â€¢ CosineAnnealingLR: ä½™å¼¦é€€ç«åˆ°1e-6\n'
             'â€¢ ReduceLROnPlateau: éªŒè¯æŸå¤±ä¸é™æ—¶å‡åŠ',
             transform=plt.gca().transAxes,
             fontsize=9,
             verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    output_dir = os.path.join(project_root, 'outputs', 'plots')
    os.makedirs(output_dir, exist_ok=True)
    
    output_path = os.path.join(output_dir, 'lr_schedules_comparison.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨å¯¹æ¯”å›¾å·²ä¿å­˜: {output_path}")
    
    plt.show()


def print_lr_schedule_info():
    """æ‰“å°å„ç§å­¦ä¹ ç‡è°ƒåº¦å™¨çš„è¯¦ç»†ä¿¡æ¯"""
    print("ğŸ“š å­¦ä¹ ç‡è°ƒåº¦å™¨è¯¦ç»†è¯´æ˜")
    print("=" * 60)
    
    print("\n1. ğŸ“‰ StepLR (é˜¶æ¢¯å¼è¡°å‡)")
    print("   â€¢ ç‰¹ç‚¹: æ¯éš”å›ºå®šepochæ•°é™ä½å­¦ä¹ ç‡")
    print("   â€¢ é€‚ç”¨: è®­ç»ƒç¨³å®šï¼Œéœ€è¦å®šæœŸé™ä½å­¦ä¹ ç‡çš„åœºæ™¯")
    print("   â€¢ å‚æ•°: step_size=7, gamma=0.5 (æ¯7ä¸ªepoché™ä½50%)")
    
    print("\n2. ğŸ“Š MultiStepLR (å¤šé˜¶æ¢¯è¡°å‡)")
    print("   â€¢ ç‰¹ç‚¹: åœ¨æŒ‡å®šçš„epoché™ä½å­¦ä¹ ç‡")
    print("   â€¢ é€‚ç”¨: éœ€è¦åœ¨ç‰¹å®šè®­ç»ƒé˜¶æ®µè°ƒæ•´å­¦ä¹ ç‡")
    print("   â€¢ å‚æ•°: milestones=[10,15], gamma=0.1 (ç¬¬10,15ä¸ªepoché™ä½90%)")
    
    print("\n3. ğŸŒŠ CosineAnnealingLR (ä½™å¼¦é€€ç«)")
    print("   â€¢ ç‰¹ç‚¹: å­¦ä¹ ç‡æŒ‰ä½™å¼¦å‡½æ•°å¹³æ»‘ä¸‹é™")
    print("   â€¢ é€‚ç”¨: éœ€è¦å¹³æ»‘å­¦ä¹ ç‡å˜åŒ–ï¼Œé¿å…çªç„¶è·³è·ƒ")
    print("   â€¢ å‚æ•°: T_max=20, eta_min=1e-6 (20ä¸ªepochå†…é™åˆ°1e-6)")
    print("   â€¢ ä¼˜åŠ¿: å¹³æ»‘å˜åŒ–ï¼Œæœ‰åŠ©äºæ¨¡å‹æ”¶æ•›åˆ°æ›´å¥½çš„å±€éƒ¨æœ€ä¼˜")
    
    print("\n4. ğŸ¯ ReduceLROnPlateau (è‡ªé€‚åº”è¡°å‡)")
    print("   â€¢ ç‰¹ç‚¹: å½“ç›‘æ§æŒ‡æ ‡åœæ­¢æ”¹å–„æ—¶è‡ªåŠ¨é™ä½å­¦ä¹ ç‡")
    print("   â€¢ é€‚ç”¨: ä¸ç¡®å®šä½•æ—¶é™ä½å­¦ä¹ ç‡çš„åœºæ™¯")
    print("   â€¢ å‚æ•°: patience=3, factor=0.5 (è¿ç»­3ä¸ªepochæ— æ”¹å–„æ—¶é™ä½50%)")
    print("   â€¢ ä¼˜åŠ¿: è‡ªé€‚åº”è°ƒæ•´ï¼Œæ— éœ€æ‰‹åŠ¨è®¾å®šæ—¶æœº")
    
    print("\nğŸ’¡ æ¨èä½¿ç”¨åœºæ™¯:")
    print("   â€¢ ğŸ¥‡ CosineAnnealingLR: å¤§å¤šæ•°æƒ…å†µä¸‹çš„é¦–é€‰")
    print("   â€¢ ğŸ¥ˆ ReduceLROnPlateau: ä¸ç¡®å®šè®­ç»ƒç‰¹æ€§æ—¶")
    print("   â€¢ ğŸ¥‰ StepLR: è®­ç»ƒè¿‡ç¨‹å¯é¢„æµ‹æ—¶")


if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œé…ç½® - å¯ä»¥åœ¨ PyCharm ä¸­ç›´æ¥ç‚¹å‡»è¿è¡Œ
    """
    print("ğŸ” å­¦ä¹ ç‡è°ƒåº¦å™¨å¯è§†åŒ–å·¥å…·")
    print("=" * 50)
    
    # æ‰“å°è¯¦ç»†ä¿¡æ¯
    print_lr_schedule_info()
    
    print("\nğŸ“ˆ ç”Ÿæˆå­¦ä¹ ç‡å˜åŒ–æ›²çº¿...")
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    visualize_lr_schedules()
    
    print("\nâœ… å¯è§†åŒ–å®Œæˆï¼")
    print("ç°åœ¨ä½ å¯ä»¥æ ¹æ®å›¾è¡¨é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚")
