"""
æ¢å¤è®­ç»ƒè„šæœ¬
ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼Œé¿å…é‡æ–°å¼€å§‹è®­ç»ƒ
æ”¯æŒé€šè¿‡é…ç½®æ–‡ä»¶æ§åˆ¶ï¼Œå¯åœ¨PyCharmä¸­ç›´æ¥è¿è¡Œ
"""

import os
import sys
import yaml
import torch
import datetime
import logging
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.models.vgg import vgg16
from src.data.dataset import get_data_loaders
from src.utils.checkpoint_manager import CheckpointManager
from src.utils.visualization import generate_all_plots


def load_config(config_path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return None
    except yaml.YAMLError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return None


def setup_logging(log_dir, timestamp):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    os.makedirs(log_dir, exist_ok=True)
    
    log_file = os.path.join(log_dir, 'resume_training.log')
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info("="*80)
    logger.info("ğŸ”„ æ¢å¤è®­ç»ƒå¼€å§‹")
    logger.info("="*80)
    logger.info(f"è®­ç»ƒæ—¶é—´æˆ³: {timestamp}")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    return logger


def create_optimizer(model, optimizer_name, learning_rate):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    if optimizer_name.lower() == 'adam':
        return torch.optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name.lower() == 'sgd':
        return torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)
    elif optimizer_name.lower() == 'adamw':
        return torch.optim.AdamW(model.parameters(), lr=learning_rate)
    elif optimizer_name.lower() == 'rmsprop':
        return torch.optim.RMSprop(model.parameters(), lr=learning_rate)
    else:
        print(f"âš ï¸  æœªçŸ¥çš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_name}ï¼Œä½¿ç”¨é»˜è®¤Adam")
        return torch.optim.Adam(model.parameters(), lr=learning_rate)


def create_scheduler(optimizer, scheduler_type, scheduler_configs, epochs):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    if scheduler_type == 'None' or scheduler_type is None:
        return None
    
    # å¤„ç†CosineAnnealingLRçš„T_maxå‚æ•°
    cosine_config = scheduler_configs.get('CosineAnnealingLR', {}).copy()
    if cosine_config.get('T_max') == 'auto':
        cosine_config['T_max'] = epochs
    
    if 'eta_min' in cosine_config and isinstance(cosine_config['eta_min'], str):
        cosine_config['eta_min'] = float(cosine_config['eta_min'])
    
    scheduler_map = {
        'StepLR': lambda: torch.optim.lr_scheduler.StepLR(optimizer, **scheduler_configs['StepLR']),
        'MultiStepLR': lambda: torch.optim.lr_scheduler.MultiStepLR(optimizer, **scheduler_configs['MultiStepLR']),
        'CosineAnnealingLR': lambda: torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, **cosine_config),
        'ReduceLROnPlateau': lambda: torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, **scheduler_configs['ReduceLROnPlateau'])
    }
    
    if scheduler_type in scheduler_map:
        return scheduler_map[scheduler_type]()
    else:
        print(f"âš ï¸  æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}ï¼Œä¸ä½¿ç”¨è°ƒåº¦å™¨")
        return None


def create_loss_function(loss_function_config, train_lines, num_classes):
    """åˆ›å»ºæŸå¤±å‡½æ•°"""
    loss_name = loss_function_config['name']
    
    if loss_name == "CrossEntropyLoss":
        return torch.nn.CrossEntropyLoss()
    
    elif loss_name == "WeightedCrossEntropyLoss":
        # ç»Ÿè®¡è®­ç»ƒé›†ä¸­å„ç±»åˆ«æ ·æœ¬æ•°é‡
        class_counts = [0] * num_classes
        for line in train_lines:
            class_id = int(line.split(';')[0])
            if 0 <= class_id < num_classes:
                class_counts[class_id] += 1
        
        # è®¡ç®—æƒé‡
        total_samples = sum(class_counts)
        if loss_function_config.get('auto_weight', True):
            # è‡ªåŠ¨è®¡ç®—æƒé‡ï¼šæ ·æœ¬å°‘çš„ç±»åˆ«æƒé‡å¤§
            weights = [total_samples / (num_classes * count) if count > 0 else 1.0 for count in class_counts]
        else:
            # ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„æƒé‡
            weights = loss_function_config.get('manual_weights', [1.0] * num_classes)
        
        weights_tensor = torch.FloatTensor(weights)
        print(f"ğŸ“Š ç±»åˆ«æƒé‡: {[f'{w:.3f}' for w in weights]}")
        
        return torch.nn.CrossEntropyLoss(weight=weights_tensor)
    
    else:
        print(f"âš ï¸  æœªçŸ¥çš„æŸå¤±å‡½æ•°: {loss_name}ï¼Œä½¿ç”¨é»˜è®¤CrossEntropyLoss")
        return torch.nn.CrossEntropyLoss()


def main():
    """ä¸»å‡½æ•° - æ¢å¤è®­ç»ƒ"""
    print("ğŸ”„ VGG16 æ¢å¤è®­ç»ƒè„šæœ¬")
    print("="*80)
    
    # ==================== è·¯å¾„å’Œé…ç½®åŠ è½½ ====================
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    config_path = os.path.join(project_root, 'configs', 'training_config.yaml')
    
    print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {config_path}")
    
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    if config is None:
        print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ¢å¤è®­ç»ƒ
    resume_config = config.get('resume', {})
    if not resume_config.get('enabled', False):
        print("âŒ æ¢å¤è®­ç»ƒæœªå¯ç”¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® resume.enabled: true")
        print("ğŸ’¡ æç¤º: ä¿®æ”¹ configs/training_config.yaml ä¸­çš„ resume.enabled ä¸º true")
        return
    
    print("âœ… æ¢å¤è®­ç»ƒæ¨¡å¼å·²å¯ç”¨")
    
    # ==================== æ£€æŸ¥ç‚¹ç®¡ç† ====================
    checkpoint_manager = CheckpointManager(project_root)
    
    # æ‰“å°æ£€æŸ¥ç‚¹æ‘˜è¦
    checkpoint_manager.print_checkpoint_summary()
    
    # æŸ¥æ‰¾æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoint_path = None
    
    # 1. ä¼˜å…ˆä½¿ç”¨æŒ‡å®šçš„æ£€æŸ¥ç‚¹è·¯å¾„
    if resume_config.get('checkpoint_path'):
        checkpoint_path = os.path.join(project_root, resume_config['checkpoint_path'])
        if not os.path.exists(checkpoint_path):
            print(f"âŒ æŒ‡å®šçš„æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            checkpoint_path = None
    
    # 2. ä½¿ç”¨æŒ‡å®šçš„è®­ç»ƒç›®å½•
    if not checkpoint_path and resume_config.get('train_dir'):
        checkpoint_path = checkpoint_manager.find_latest_checkpoint(resume_config['train_dir'])
    
    # 3. è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹
    if not checkpoint_path and resume_config.get('auto_find_latest', True):
        checkpoint_path = checkpoint_manager.find_latest_checkpoint()
    
    if not checkpoint_path:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ£€æŸ¥ç‚¹æ–‡ä»¶")
        print("ğŸ’¡ è¯·æ£€æŸ¥:")
        print("   1. æ˜¯å¦å·²ç»è¿›è¡Œè¿‡è®­ç»ƒå¹¶ä¿å­˜äº†æ£€æŸ¥ç‚¹")
        print("   2. é…ç½®æ–‡ä»¶ä¸­çš„æ£€æŸ¥ç‚¹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("   3. æ£€æŸ¥ç‚¹æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        return
    
    print(f"ğŸ¯ å°†ä½¿ç”¨æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # ==================== æå–é…ç½®å‚æ•° ====================
    # ä»é…ç½®æ–‡ä»¶ä¸­æå–æ‰€æœ‰å¿…è¦çš„å‚æ•°
    EPOCHS = config['training']['epochs']
    LEARNING_RATE = config['training']['learning_rate']
    BATCH_SIZE = config['dataloader']['batch_size']
    NUM_WORKERS = config['dataloader']['num_workers']
    NUM_CLASSES = config['data']['num_classes']
    CLASS_NAMES = config['data']['class_names']
    
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"   ç›®æ ‡è½®æ•°: {EPOCHS}")
    print(f"   å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"   ç±»åˆ«æ•°é‡: {NUM_CLASSES}")
    
    # ==================== è®¾å¤‡å’Œæ¨¡å‹åˆå§‹åŒ– ====================
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    model = vgg16(
        pretrained=config['model']['pretrained'],
        num_classes=NUM_CLASSES,
        dropout=config['model']['dropout']
    )
    model.to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimizer(model, config['training']['optimizer'], LEARNING_RATE)

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_config = config['training']['scheduler']
    scheduler_type = scheduler_config['type']
    scheduler_configs = {
        'StepLR': scheduler_config.get('step_lr', {'step_size': 7, 'gamma': 0.5}),
        'MultiStepLR': scheduler_config.get('multi_step_lr', {'milestones': [10, 15], 'gamma': 0.1}),
        'CosineAnnealingLR': scheduler_config.get('cosine_annealing_lr', {'T_max': 'auto', 'eta_min': 0}),
        'ReduceLROnPlateau': scheduler_config.get('reduce_lr_on_plateau', {'mode': 'min', 'factor': 0.5, 'patience': 3})
    }
    scheduler = create_scheduler(optimizer, scheduler_type, scheduler_configs, EPOCHS)

    # ==================== åŠ è½½æ£€æŸ¥ç‚¹ ====================
    try:
        start_epoch, best_acc = checkpoint_manager.load_checkpoint(
            checkpoint_path, model, optimizer, scheduler
        )
        print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
        return

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­è®­ç»ƒ
    if start_epoch >= EPOCHS:
        print(f"âš ï¸  æ£€æŸ¥ç‚¹çš„epoch ({start_epoch}) å·²è¾¾åˆ°æˆ–è¶…è¿‡ç›®æ ‡è½®æ•° ({EPOCHS})")
        print("ğŸ’¡ å¦‚éœ€ç»§ç»­è®­ç»ƒï¼Œè¯·å¢åŠ é…ç½®æ–‡ä»¶ä¸­çš„ training.epochs å€¼")
        return

    # ==================== æ•°æ®åŠ è½½ ====================
    print(f"\nğŸ“Š å‡†å¤‡æ•°æ®åŠ è½½å™¨...")

    annotation_path = os.path.join(project_root, config['data']['annotation_file'])

    try:
        train_loader, val_loader, train_lines, val_lines = get_data_loaders(
            annotation_path=annotation_path,
            batch_size=BATCH_SIZE,
            num_workers=NUM_WORKERS,
            train_val_split=config['data']['train_val_split'],
            random_seed=config['data']['random_seed'],
            stratified_split=config['data']['stratified_split'],
            num_classes=NUM_CLASSES
        )
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_lines)}")
        print(f"   éªŒè¯æ ·æœ¬: {len(val_lines)}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # ==================== æŸå¤±å‡½æ•° ====================
    criterion = create_loss_function(
        config['training']['loss_function'],
        train_lines,
        NUM_CLASSES
    )
    criterion.to(device)

    # ==================== ç›®å½•è®¾ç½® ====================
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    # åˆ›å»ºæ–°çš„æ—¶é—´æˆ³ç›®å½•ï¼ˆæ¢å¤è®­ç»ƒä½¿ç”¨æ–°ç›®å½•ï¼‰
    base_checkpoint_dir = os.path.join(project_root, config['save']['checkpoint_dir'])
    base_log_dir = os.path.join(project_root, config['logging']['log_dir'])
    base_plot_dir = os.path.join(project_root, config['logging']['plot_dir'])

    checkpoint_dir = os.path.join(base_checkpoint_dir, f"resume_{timestamp}")
    log_dir = os.path.join(base_log_dir, f"resume_{timestamp}")
    plot_dir = os.path.join(base_plot_dir, f"resume_{timestamp}")

    # latestç›®å½•
    latest_checkpoint_dir = os.path.join(base_checkpoint_dir, "latest")
    latest_log_dir = os.path.join(base_log_dir, "latest")
    latest_plot_dir = os.path.join(base_plot_dir, "latest")

    # åˆ›å»ºç›®å½•
    for dir_path in [checkpoint_dir, log_dir, plot_dir, latest_checkpoint_dir, latest_log_dir, latest_plot_dir]:
        os.makedirs(dir_path, exist_ok=True)

    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(log_dir, timestamp)
    logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {checkpoint_path}")
    logger.info(f"èµ·å§‹epoch: {start_epoch + 1}")
    logger.info(f"ç›®æ ‡epoch: {EPOCHS}")
    logger.info(f"å½“å‰æœ€ä½³ç²¾åº¦: {best_acc:.2f}%")

    # å¤‡ä»½é…ç½®æ–‡ä»¶
    config_backup_path = os.path.join(log_dir, 'config_backup.yaml')
    with open(config_backup_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

    print(f"\nğŸ’¾ è¾“å‡ºç›®å½•:")
    print(f"   æ£€æŸ¥ç‚¹: {checkpoint_dir}")
    print(f"   æ—¥å¿—: {log_dir}")
    print(f"   å›¾è¡¨: {plot_dir}")

    # ==================== å¼€å§‹æ¢å¤è®­ç»ƒ ====================
    print(f"\nğŸš€ å¼€å§‹æ¢å¤è®­ç»ƒ (ä»ç¬¬{start_epoch + 1}è½®åˆ°ç¬¬{EPOCHS}è½®)")
    print("="*80)

    # è®­ç»ƒå†å²è®°å½•ï¼ˆä»æ¢å¤ç‚¹å¼€å§‹é‡æ–°è®°å½•ï¼‰
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    # è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, EPOCHS):
        print(f"\nEpoch {epoch + 1}/{EPOCHS}")
        print("-" * 80)

        # ==================== è®­ç»ƒé˜¶æ®µ ====================
        model.train()
        total_train_loss = 0.0
        train_correct = 0
        train_total = 0

        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Train]',
                         ncols=100, leave=False)

        for batch_idx, (img, label, _) in enumerate(train_pbar):
            img = img.to(device)
            label = label.to(device)

            optimizer.zero_grad()
            output = model(img)
            train_loss = criterion(output, label)
            train_loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            total_train_loss += train_loss.item()
            _, predicted = output.max(1)
            train_total += label.size(0)
            train_correct += predicted.eq(label).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            current_loss = total_train_loss / (batch_idx + 1)
            current_acc = 100. * train_correct / train_total
            train_pbar.set_postfix({
                'Loss': f'{current_loss:.4f}',
                'Acc': f'{current_acc:.2f}%'
            })

        # å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None and scheduler_type != 'ReduceLROnPlateau':
            scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']

        # ==================== éªŒè¯é˜¶æ®µ ====================
        model.eval()
        total_val_loss = 0.0
        val_correct = 0
        val_total = 0

        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Val]  ',
                       ncols=100, leave=False)

        with torch.no_grad():
            for batch_idx, (img, label, _) in enumerate(val_pbar):
                img = img.to(device)
                label = label.to(device)

                output = model(img)
                val_loss = criterion(output, label)

                # ç»Ÿè®¡
                total_val_loss += val_loss.item()
                _, predicted = output.max(1)
                val_total += label.size(0)
                val_correct += predicted.eq(label).sum().item()

                # æ›´æ–°è¿›åº¦æ¡
                current_loss = total_val_loss / (batch_idx + 1)
                current_acc = 100. * val_correct / val_total
                val_pbar.set_postfix({
                    'Loss': f'{current_loss:.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })

        # è®¡ç®—å¹³å‡å€¼
        avg_train_loss = total_train_loss / len(train_loader)
        avg_val_loss = total_val_loss / len(val_loader)
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total

        # è®°å½•å†å²
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        train_accuracies.append(train_acc)
        val_accuracies.append(val_acc)

        # ReduceLROnPlateau éœ€è¦åœ¨éªŒè¯åè°ƒç”¨
        if scheduler is not None and scheduler_type == 'ReduceLROnPlateau':
            scheduler.step(avg_val_loss)
            current_lr = optimizer.param_groups[0]['lr']

        # æ‰“å°epochç»“æœ
        print(f"Epoch {epoch+1}/{EPOCHS} å®Œæˆ:")
        print(f"  è®­ç»ƒ - Loss: {avg_train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"  éªŒè¯ - Loss: {avg_val_loss:.4f}, Acc: {val_acc:.2f}%")
        print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")

        # è®°å½•åˆ°æ—¥å¿—
        logger.info(f"Epoch {epoch+1}/{EPOCHS} - Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, "
                   f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%, LR: {current_lr:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            if config['save']['save_best_only']:
                # ä¿å­˜åˆ°æ—¶é—´æˆ³ç›®å½•
                torch.save(model.state_dict(), os.path.join(checkpoint_dir, "best_model.pth"))
                # åŒæ—¶ä¿å­˜åˆ°latestç›®å½•
                torch.save(model.state_dict(), os.path.join(latest_checkpoint_dir, "best_model.pth"))
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}% (æ¨¡å‹å·²ä¿å­˜)")
                logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}% (æ¨¡å‹å·²ä¿å­˜)")
            else:
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")
                logger.info(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")

        # æ ¹æ®é…ç½®ä¿å­˜æ£€æŸ¥ç‚¹
        should_save_checkpoint = False
        if config['save']['save_checkpoint_every_epoch']:
            should_save_checkpoint = True
        elif (epoch + 1) % config['save']['save_frequency'] == 0:
            should_save_checkpoint = True

        if should_save_checkpoint:
            checkpoint_data = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'val_acc': val_acc,
                'best_acc': best_acc,
                'scheduler_type': scheduler_type
            }

            if scheduler is not None:
                checkpoint_data['scheduler_state_dict'] = scheduler.state_dict()

            # ä¿å­˜æ£€æŸ¥ç‚¹
            torch.save(checkpoint_data, os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch+1}.pth"))
            torch.save(checkpoint_data, os.path.join(latest_checkpoint_dir, f"checkpoint_epoch_{epoch+1}.pth"))

        print("-" * 80)

    # ==================== è®­ç»ƒå®Œæˆ ====================
    print(f"\nğŸ¯ æ¢å¤è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {checkpoint_dir}/")
    print("="*80)

    # ==================== è‡ªåŠ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ====================
    print(f"\nğŸ“Š å¼€å§‹ç”Ÿæˆè®­ç»ƒåˆ†æå›¾è¡¨...")
    try:
        # ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨
        evaluation_results = generate_all_plots(
            train_losses=train_losses,
            val_losses=val_losses,
            train_accuracies=train_accuracies,
            val_accuracies=val_accuracies,
            model=model,
            dataloader=val_loader,
            device=device,
            class_names=CLASS_NAMES,
            plot_dir=plot_dir,
            latest_plot_dir=latest_plot_dir
        )

        print(f"âœ… æ‰€æœ‰å›¾è¡¨ç”Ÿæˆå®Œæˆ!")
        print(f"ğŸ“ å›¾è¡¨ä¿å­˜ä½ç½®:")
        print(f"   ğŸ“Š æ—¶é—´æˆ³ç›®å½•: {plot_dir}")
        print(f"   ğŸ“Š æœ€æ–°ç›®å½•: {latest_plot_dir}")

        logger.info("ğŸ“Š å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
        logger.info(f"å›¾è¡¨ä¿å­˜ä½ç½®: {plot_dir}")

    except Exception as e:
        print(f"âš ï¸  å›¾è¡¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        logger.error(f"å›¾è¡¨ç”Ÿæˆé”™è¯¯: {e}")
        print("è®­ç»ƒå·²å®Œæˆï¼Œä½†å›¾è¡¨ç”Ÿæˆå¤±è´¥ã€‚å¯ä»¥ç¨åæ‰‹åŠ¨è¿è¡Œå¯è§†åŒ–è„šæœ¬ã€‚")

    # è®°å½•è®­ç»ƒå®Œæˆä¿¡æ¯
    training_end_time = datetime.datetime.now()

    logger.info("="*80)
    logger.info("ğŸ¯ æ¢å¤è®­ç»ƒå®Œæˆ!")
    logger.info("="*80)
    logger.info(f"æœ€ä½³éªŒè¯ç²¾åº¦: {best_acc:.2f}%")
    logger.info(f"è®­ç»ƒå¼€å§‹æ—¶é—´: {timestamp}")
    logger.info(f"è®­ç»ƒç»“æŸæ—¶é—´: {training_end_time.strftime('%Y%m%d_%H%M%S')}")
    logger.info(f"æ¨¡å‹ä¿å­˜ä½ç½®: {checkpoint_dir}/")
    logger.info(f"æ—¥å¿—ä¿å­˜ä½ç½®: {log_dir}/")
    logger.info(f"é…ç½®å¤‡ä»½ä½ç½®: {config_backup_path}")
    logger.info("="*80)


if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œé…ç½® - å¯ä»¥åœ¨ PyCharm ä¸­ç›´æ¥ç‚¹å‡»è¿è¡Œ
    æ¢å¤è®­ç»ƒåŠŸèƒ½é€šè¿‡é…ç½®æ–‡ä»¶æ§åˆ¶

    ä½¿ç”¨æ–¹æ³•:
    1. ä¿®æ”¹ configs/training_config.yaml ä¸­çš„é…ç½®:
       - training.epochs: è®¾ç½®ç›®æ ‡è½®æ•° (å¦‚100)
       - resume.enabled: è®¾ç½®ä¸º true
       - resume.auto_find_latest: è®¾ç½®ä¸º true (è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ£€æŸ¥ç‚¹)

    2. åœ¨ PyCharm ä¸­ç›´æ¥è¿è¡Œæ­¤è„šæœ¬

    3. è„šæœ¬ä¼šè‡ªåŠ¨:
       - æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
       - ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒçŠ¶æ€
       - ç»§ç»­è®­ç»ƒåˆ°ç›®æ ‡è½®æ•°
       - ç”Ÿæˆå®Œæ•´çš„åˆ†æå›¾è¡¨
    """
    main()
