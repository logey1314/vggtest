#!/usr/bin/env python3
"""
è¯„ä¼°æ¨¡å—åŠŸèƒ½æµ‹è¯•
æµ‹è¯• scripts/evaluate/ æ¨¡å—çš„å„é¡¹åŠŸèƒ½
"""

import sys
import os
import tempfile
import shutil
import torch
import yaml
from unittest.mock import patch, MagicMock

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥è¦æµ‹è¯•çš„æ¨¡å—
from scripts.evaluate.evaluate_utils import (
    extract_timestamp_from_path,
    validate_train_path,
    find_model_file,
    get_corresponding_model_path,
    generate_eval_timestamp,
    print_path_info
)


class TestEvaluateModule:
    """è¯„ä¼°æ¨¡å—æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_results = []
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    def log_test(self, test_name, passed, message=""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        status = "âœ… PASS" if passed else "âŒ FAIL"
        self.test_results.append({
            'name': test_name,
            'passed': passed,
            'message': message
        })
        print(f"{status} {test_name}")
        if message:
            print(f"    {message}")
    
    def test_timestamp_extraction(self):
        """æµ‹è¯•æ—¶é—´æˆ³æå–åŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•1: æ—¶é—´æˆ³æå–åŠŸèƒ½")
        
        test_cases = [
            {
                'input': "/var/yjs/zes/vgg/outputs/logs/train_20250820_074155/",
                'expected': "20250820_074155",
                'description': "å®Œæ•´è·¯å¾„"
            },
            {
                'input': "train_20250820_074155",
                'expected': "20250820_074155", 
                'description': "ç›®å½•å"
            },
            {
                'input': "resume_20250820_074155",
                'expected': "20250820_074155",
                'description': "æ¢å¤è®­ç»ƒç›®å½•"
            },
            {
                'input': "invalid_path",
                'expected': None,
                'description': "æ— æ•ˆè·¯å¾„"
            }
        ]
        
        for case in test_cases:
            try:
                result = extract_timestamp_from_path(case['input'])
                passed = result == case['expected']
                message = f"{case['description']}: '{case['input']}' -> '{result}'"
                self.log_test(f"æ—¶é—´æˆ³æå– - {case['description']}", passed, message)
            except Exception as e:
                self.log_test(f"æ—¶é—´æˆ³æå– - {case['description']}", False, f"å¼‚å¸¸: {e}")
    
    def test_eval_timestamp_generation(self):
        """æµ‹è¯•è¯„ä¼°æ—¶é—´æˆ³ç”Ÿæˆ"""
        print("\nğŸ§ª æµ‹è¯•2: è¯„ä¼°æ—¶é—´æˆ³ç”Ÿæˆ")
        
        test_cases = [
            {
                'input': "20250820_074155",
                'expected': "eval_20250820_074155"
            },
            {
                'input': "20241225_120000",
                'expected': "eval_20241225_120000"
            }
        ]
        
        for case in test_cases:
            try:
                result = generate_eval_timestamp(case['input'])
                passed = result == case['expected']
                message = f"'{case['input']}' -> '{result}'"
                self.log_test("è¯„ä¼°æ—¶é—´æˆ³ç”Ÿæˆ", passed, message)
            except Exception as e:
                self.log_test("è¯„ä¼°æ—¶é—´æˆ³ç”Ÿæˆ", False, f"å¼‚å¸¸: {e}")
    
    def test_path_validation(self):
        """æµ‹è¯•è·¯å¾„éªŒè¯åŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•3: è·¯å¾„éªŒè¯åŠŸèƒ½")
        
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºæœ‰æ•ˆçš„æµ‹è¯•è·¯å¾„
            valid_path = os.path.join(temp_dir, "train_20250820_074155")
            os.makedirs(valid_path, exist_ok=True)
            
            test_cases = [
                {
                    'input': valid_path,
                    'expected': True,
                    'description': "æœ‰æ•ˆè·¯å¾„"
                },
                {
                    'input': "/nonexistent/path/train_20250820_074155",
                    'expected': False,
                    'description': "ä¸å­˜åœ¨çš„è·¯å¾„"
                },
                {
                    'input': temp_dir,  # å­˜åœ¨ä½†æ— æ—¶é—´æˆ³
                    'expected': False,
                    'description': "æ— æ—¶é—´æˆ³çš„è·¯å¾„"
                },
                {
                    'input': "",
                    'expected': False,
                    'description': "ç©ºè·¯å¾„"
                }
            ]
            
            for case in test_cases:
                try:
                    result = validate_train_path(case['input'])
                    passed = result == case['expected']
                    message = f"{case['description']}: {passed}"
                    self.log_test(f"è·¯å¾„éªŒè¯ - {case['description']}", passed, message)
                except Exception as e:
                    self.log_test(f"è·¯å¾„éªŒè¯ - {case['description']}", False, f"å¼‚å¸¸: {e}")
    
    def test_model_file_finding(self):
        """æµ‹è¯•æ¨¡å‹æ–‡ä»¶æŸ¥æ‰¾åŠŸèƒ½"""
        print("\nğŸ§ª æµ‹è¯•4: æ¨¡å‹æ–‡ä»¶æŸ¥æ‰¾åŠŸèƒ½")
        
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç¯å¢ƒ
        with tempfile.TemporaryDirectory() as temp_dir:
            timestamp = "20250820_074155"
            
            # åˆ›å»ºæ¨¡å‹ç›®å½•ç»“æ„
            checkpoint_dir = os.path.join(temp_dir, 'models', 'checkpoints', f'train_{timestamp}')
            os.makedirs(checkpoint_dir, exist_ok=True)
            
            # åˆ›å»ºæµ‹è¯•æ¨¡å‹æ–‡ä»¶
            best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')
            checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_50.pth')
            
            # åˆ›å»ºç©ºæ–‡ä»¶
            with open(best_model_path, 'w') as f:
                f.write("dummy model")
            with open(checkpoint_path, 'w') as f:
                f.write("dummy checkpoint")
            
            # æµ‹è¯•æŸ¥æ‰¾åŠŸèƒ½
            try:
                result = find_model_file(temp_dir, timestamp)
                passed = result == best_model_path
                message = f"æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {result}"
                self.log_test("æ¨¡å‹æ–‡ä»¶æŸ¥æ‰¾ - æœ€ä½³æ¨¡å‹", passed, message)
            except Exception as e:
                self.log_test("æ¨¡å‹æ–‡ä»¶æŸ¥æ‰¾ - æœ€ä½³æ¨¡å‹", False, f"å¼‚å¸¸: {e}")
            
            # æµ‹è¯•ä¸å­˜åœ¨çš„æ—¶é—´æˆ³
            try:
                result = find_model_file(temp_dir, "20990101_000000")
                passed = result is None
                self.log_test("æ¨¡å‹æ–‡ä»¶æŸ¥æ‰¾ - ä¸å­˜åœ¨çš„æ—¶é—´æˆ³", passed, "æ­£ç¡®è¿”å›None")
            except Exception as e:
                self.log_test("æ¨¡å‹æ–‡ä»¶æŸ¥æ‰¾ - ä¸å­˜åœ¨çš„æ—¶é—´æˆ³", False, f"å¼‚å¸¸: {e}")
    
    def test_config_loading(self):
        """æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½"""
        print("\nğŸ§ª æµ‹è¯•5: é…ç½®æ–‡ä»¶åŠ è½½")
        
        config_path = os.path.join(self.project_root, 'configs', 'training_config.yaml')
        
        try:
            # å¯¼å…¥é…ç½®åŠ è½½å‡½æ•°
            from scripts.evaluate.evaluate_model import load_config
            
            config = load_config(config_path)
            
            if config is None:
                self.log_test("é…ç½®æ–‡ä»¶åŠ è½½", False, "é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥")
            else:
                # æ£€æŸ¥å…³é”®é…ç½®é¡¹
                required_keys = ['data', 'model', 'training']
                missing_keys = [key for key in required_keys if key not in config]
                
                if missing_keys:
                    self.log_test("é…ç½®æ–‡ä»¶åŠ è½½", False, f"ç¼ºå°‘å…³é”®é…ç½®é¡¹: {missing_keys}")
                else:
                    self.log_test("é…ç½®æ–‡ä»¶åŠ è½½", True, "é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸï¼ŒåŒ…å«æ‰€æœ‰å¿…éœ€é¡¹")
                    
                    # æ£€æŸ¥æ•°æ®é…ç½®
                    if 'num_classes' in config['data'] and 'class_names' in config['data']:
                        num_classes = config['data']['num_classes']
                        class_names = config['data']['class_names']
                        classes_match = len(class_names) == num_classes
                        self.log_test("é…ç½®ä¸€è‡´æ€§æ£€æŸ¥", classes_match, 
                                    f"ç±»åˆ«æ•°é‡: {num_classes}, ç±»åˆ«åç§°æ•°é‡: {len(class_names)}")
        
        except Exception as e:
            self.log_test("é…ç½®æ–‡ä»¶åŠ è½½", False, f"å¼‚å¸¸: {e}")
    
    def test_integration(self):
        """é›†æˆæµ‹è¯•"""
        print("\nğŸ§ª æµ‹è¯•6: é›†æˆæµ‹è¯•")
        
        # åˆ›å»ºå®Œæ•´çš„æµ‹è¯•ç¯å¢ƒ
        with tempfile.TemporaryDirectory() as temp_dir:
            timestamp = "20250820_074155"
            train_path = os.path.join(temp_dir, "outputs", "logs", f"train_{timestamp}")
            os.makedirs(train_path, exist_ok=True)
            
            # åˆ›å»ºæ¨¡å‹æ–‡ä»¶
            model_dir = os.path.join(temp_dir, 'models', 'checkpoints', f'train_{timestamp}')
            os.makedirs(model_dir, exist_ok=True)
            model_path = os.path.join(model_dir, 'best_model.pth')
            with open(model_path, 'w') as f:
                f.write("dummy model")
            
            try:
                # æµ‹è¯•å®Œæ•´æµç¨‹
                result_model_path = get_corresponding_model_path(train_path, temp_dir)
                
                if result_model_path == model_path:
                    self.log_test("é›†æˆæµ‹è¯• - å®Œæ•´æµç¨‹", True, "æˆåŠŸæ‰¾åˆ°å¯¹åº”çš„æ¨¡å‹æ–‡ä»¶")
                else:
                    self.log_test("é›†æˆæµ‹è¯• - å®Œæ•´æµç¨‹", False, 
                                f"æœŸæœ›: {model_path}, å®é™…: {result_model_path}")
            
            except Exception as e:
                self.log_test("é›†æˆæµ‹è¯• - å®Œæ•´æµç¨‹", False, f"å¼‚å¸¸: {e}")
    
    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        print("\nğŸ§ª æµ‹è¯•7: é”™è¯¯å¤„ç†")
        
        error_cases = [
            {
                'func': lambda: extract_timestamp_from_path(None),
                'description': "Noneè¾“å…¥å¤„ç†"
            },
            {
                'func': lambda: validate_train_path(None),
                'description': "Noneè·¯å¾„éªŒè¯"
            },
            {
                'func': lambda: find_model_file("", ""),
                'description': "ç©ºå‚æ•°å¤„ç†"
            }
        ]
        
        for case in error_cases:
            try:
                result = case['func']()
                # å¦‚æœæ²¡æœ‰æŠ›å‡ºå¼‚å¸¸ï¼Œæ£€æŸ¥æ˜¯å¦è¿”å›äº†åˆç†çš„é»˜è®¤å€¼
                passed = result is None or result is False
                self.log_test(f"é”™è¯¯å¤„ç† - {case['description']}", passed, 
                            f"è¿”å›å€¼: {result}")
            except Exception as e:
                # å¦‚æœæŠ›å‡ºå¼‚å¸¸ï¼Œè¿™ä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„é”™è¯¯å¤„ç†æ–¹å¼
                self.log_test(f"é”™è¯¯å¤„ç† - {case['description']}", True, 
                            f"æ­£ç¡®æŠ›å‡ºå¼‚å¸¸: {type(e).__name__}")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è¯„ä¼°æ¨¡å—åŠŸèƒ½æµ‹è¯•")
        print("=" * 60)
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        self.test_timestamp_extraction()
        self.test_eval_timestamp_generation()
        self.test_path_validation()
        self.test_model_file_finding()
        self.test_config_loading()
        self.test_integration()
        self.test_error_handling()
        
        # ç»Ÿè®¡ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result['passed'])
        failed_tests = total_tests - passed_tests
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 60)
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"é€šè¿‡: {passed_tests} âœ…")
        print(f"å¤±è´¥: {failed_tests} âŒ")
        print(f"é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")
        
        if failed_tests > 0:
            print("\nâŒ å¤±è´¥çš„æµ‹è¯•:")
            for result in self.test_results:
                if not result['passed']:
                    print(f"  - {result['name']}: {result['message']}")
        
        print("=" * 60)
        
        if failed_tests == 0:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è¯„ä¼°æ¨¡å—åŠŸèƒ½æ­£å¸¸ã€‚")
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
        
        return failed_tests == 0


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    tester = TestEvaluateModule()
    success = tester.run_all_tests()
    
    if success:
        print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print("1. è¯„ä¼°æ¨¡å—åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        print("2. å¯ä»¥å®‰å…¨ä½¿ç”¨ scripts/evaluate/evaluate_model.py")
        print("3. ä¿®æ”¹ SPECIFIC_TRAIN_PATH å˜é‡æ¥æŒ‡å®šè¦è¯„ä¼°çš„è®­ç»ƒè½®æ¬¡")
        print("4. è¯¦ç»†ä½¿ç”¨è¯´æ˜è¯·æŸ¥çœ‹ scripts/evaluate/README.md")
    
    return success


if __name__ == "__main__":
    """
    ç›´æ¥è¿è¡Œé…ç½® - å¯ä»¥åœ¨ PyCharm ä¸­ç›´æ¥ç‚¹å‡»è¿è¡Œ
    æµ‹è¯•è¯„ä¼°æ¨¡å—çš„å„é¡¹åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
    """
    success = main()
    sys.exit(0 if success else 1)
