"""
æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½çš„è„šæœ¬
ç”¨äºéªŒè¯æ–°çš„å¯è§†åŒ–æ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import numpy as np
import matplotlib.pyplot as plt
from src.utils.visualization import (
    plot_enhanced_training_history,
    plot_confusion_matrices, 
    plot_roc_curves,
    plot_pr_curves,
    plot_class_performance_radar
)

def generate_test_data():
    """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
    # æ¨¡æ‹Ÿè®­ç»ƒå†å²æ•°æ®
    epochs = 20
    train_losses = np.random.exponential(0.5, epochs) + 0.1
    val_losses = train_losses + np.random.normal(0, 0.1, epochs)
    train_accuracies = 100 * (1 - train_losses / 2)
    val_accuracies = train_accuracies - np.random.uniform(0, 10, epochs)
    
    # æ¨¡æ‹Ÿè¯„ä¼°ç»“æœ
    n_samples = 1000
    n_classes = 3
    class_names = ["class1_125-175", "class2_180-230", "class3_233-285"]
    
    # ç”Ÿæˆæ¨¡æ‹Ÿçš„é¢„æµ‹å’Œæ ‡ç­¾
    labels = np.random.randint(0, n_classes, n_samples)
    predictions = labels.copy()
    # æ·»åŠ ä¸€äº›é”™è¯¯é¢„æµ‹
    error_indices = np.random.choice(n_samples, size=int(0.2 * n_samples), replace=False)
    predictions[error_indices] = np.random.randint(0, n_classes, len(error_indices))
    
    # ç”Ÿæˆæ¨¡æ‹Ÿçš„æ¦‚ç‡
    probabilities = np.random.dirichlet([1, 1, 1], n_samples)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿçš„åˆ†ç±»æŠ¥å‘Š
    from sklearn.metrics import classification_report, confusion_matrix
    report = classification_report(labels, predictions, target_names=class_names, output_dict=True)
    cm = confusion_matrix(labels, predictions)
    cm_normalized = confusion_matrix(labels, predictions, normalize='true')
    
    evaluation_results = {
        'predictions': predictions,
        'labels': labels,
        'probabilities': probabilities,
        'classification_report': report,
        'confusion_matrix': cm,
        'confusion_matrix_normalized': cm_normalized,
        'class_names': class_names
    }
    
    return train_losses, val_losses, train_accuracies, val_accuracies, evaluation_results

def test_visualization():
    """æµ‹è¯•æ‰€æœ‰å¯è§†åŒ–åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¯è§†åŒ–åŠŸèƒ½...")
    
    # åˆ›å»ºæµ‹è¯•è¾“å‡ºç›®å½•
    test_output_dir = "outputs/plots/test"
    os.makedirs(test_output_dir, exist_ok=True)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    train_losses, val_losses, train_accuracies, val_accuracies, evaluation_results = generate_test_data()
    
    try:
        # 1. æµ‹è¯•è®­ç»ƒå†å²å›¾è¡¨
        print("ğŸ“ˆ æµ‹è¯•è®­ç»ƒå†å²å›¾è¡¨...")
        plot_enhanced_training_history(
            train_losses, val_losses, train_accuracies, val_accuracies,
            os.path.join(test_output_dir, "test_training_history.png")
        )
        
        # 2. æµ‹è¯•æ··æ·†çŸ©é˜µ
        print("ğŸ“Š æµ‹è¯•æ··æ·†çŸ©é˜µ...")
        plot_confusion_matrices(evaluation_results, test_output_dir)
        
        # 3. æµ‹è¯•ROCæ›²çº¿
        print("ğŸ“ˆ æµ‹è¯•ROCæ›²çº¿...")
        plot_roc_curves(evaluation_results, os.path.join(test_output_dir, "test_roc_curves.png"))
        
        # 4. æµ‹è¯•PRæ›²çº¿
        print("ğŸ“ˆ æµ‹è¯•PRæ›²çº¿...")
        plot_pr_curves(evaluation_results, os.path.join(test_output_dir, "test_pr_curves.png"))
        
        # 5. æµ‹è¯•é›·è¾¾å›¾
        print("ğŸ¯ æµ‹è¯•ç±»åˆ«æ€§èƒ½é›·è¾¾å›¾...")
        plot_class_performance_radar(evaluation_results, os.path.join(test_output_dir, "test_class_performance_radar.png"))
        
        print("âœ… æ‰€æœ‰å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•é€šè¿‡!")
        print(f"ğŸ“ æµ‹è¯•å›¾è¡¨ä¿å­˜åœ¨: {test_output_dir}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_visualization()
