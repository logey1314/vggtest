# è®­ç»ƒç»„ä»¶æ¨¡å— (src/training)

æœ¬æ¨¡å—æä¾›ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å™¨ã€æŸå¤±å‡½æ•°çš„å·¥å‚æ¥å£ï¼Œæ”¯æŒçµæ´»çš„è®­ç»ƒç»„ä»¶åˆ‡æ¢å’Œé…ç½®ã€‚

## ğŸ“ æ¨¡å—ç»“æ„

```
src/training/
â”œâ”€â”€ __init__.py              # æ¨¡å—åˆå§‹åŒ–å’Œå¯¼å…¥
â”œâ”€â”€ optimizer_factory.py    # ä¼˜åŒ–å™¨å·¥å‚
â”œâ”€â”€ scheduler_factory.py    # å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥å‚
â”œâ”€â”€ loss_factory.py         # æŸå¤±å‡½æ•°å·¥å‚
â””â”€â”€ README.md               # æœ¬è¯´æ˜æ–‡æ¡£
```

## âš™ï¸ ä¼˜åŒ–å™¨å·¥å‚

### æ”¯æŒçš„ä¼˜åŒ–å™¨

| ä¼˜åŒ–å™¨ | åç§° | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|--------|------|------|----------|
| Adam | Adam | è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œæ”¶æ•›å¿« | å¤§å¤šæ•°ä»»åŠ¡çš„é¦–é€‰ |
| SGD | SGD | ç»å…¸æ¢¯åº¦ä¸‹é™ï¼Œç¨³å®š | éœ€è¦ç²¾ç¡®æ§åˆ¶çš„åœºæ™¯ |
| AdamW | AdamW | Adamçš„æ”¹è¿›ç‰ˆï¼Œæ›´å¥½çš„æƒé‡è¡°å‡ | å¤§æ¨¡å‹è®­ç»ƒ |
| RMSprop | RMSprop | é€‚åˆéå¹³ç¨³ç›®æ ‡ | RNNç­‰å¾ªç¯ç½‘ç»œ |
| Adagrad | Adagrad | è‡ªé€‚åº”æ¢¯åº¦ç®—æ³• | ç¨€ç–æ•°æ® |

### ä½¿ç”¨ç¤ºä¾‹

```python
from src.training import create_optimizer

# åŸºç¡€é…ç½®
optimizer_config = {
    'name': 'Adam',
    'params': {
        'lr': 0.001,
        'weight_decay': 0.0001
    }
}

optimizer = create_optimizer(model, optimizer_config)
```

### é…ç½®æ–‡ä»¶ç¤ºä¾‹

```yaml
training:
  optimizer:
    name: "AdamW"
    params:
      lr: 0.001
      weight_decay: 0.01
      betas: [0.9, 0.999]
      eps: 1e-8
```

## ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥å‚

### æ”¯æŒçš„è°ƒåº¦å™¨

| è°ƒåº¦å™¨ | åç§° | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|--------|------|------|----------|
| StepLR | é˜¶æ¢¯å¼è¡°å‡ | å›ºå®šé—´éš”é™ä½å­¦ä¹ ç‡ | ç®€å•ç¨³å®šçš„è®­ç»ƒ |
| MultiStepLR | å¤šé˜¶æ¢¯è¡°å‡ | æŒ‡å®šè½®æ¬¡é™ä½å­¦ä¹ ç‡ | ç²¾ç¡®æ§åˆ¶å­¦ä¹ ç‡å˜åŒ– |
| CosineAnnealingLR | ä½™å¼¦é€€ç« | å¹³æ»‘çš„å­¦ä¹ ç‡å˜åŒ– | å¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡ |
| ReduceLROnPlateau | è‡ªé€‚åº”è°ƒåº¦ | åŸºäºæŒ‡æ ‡è‡ªåŠ¨è°ƒæ•´ | ä¸ç¡®å®šæœ€ä¼˜è°ƒåº¦ç­–ç•¥æ—¶ |
| ExponentialLR | æŒ‡æ•°è¡°å‡ | æŒ‡æ•°å½¢å¼è¡°å‡ | éœ€è¦å¿«é€Ÿè¡°å‡çš„åœºæ™¯ |

### ä½¿ç”¨ç¤ºä¾‹

```python
from src.training import create_scheduler

scheduler_config = {
    'name': 'CosineAnnealingLR',
    'params': {
        'T_max': 100,  # æˆ– 'auto'
        'eta_min': 1e-6
    }
}

scheduler = create_scheduler(optimizer, scheduler_config, total_epochs=100)
```

### é…ç½®æ–‡ä»¶ç¤ºä¾‹

```yaml
training:
  scheduler:
    name: "CosineAnnealingLR"
    params:
      T_max: "auto"  # è‡ªåŠ¨ä½¿ç”¨æ€»è½®æ•°
      eta_min: 1e-6
```

## ğŸ¯ æŸå¤±å‡½æ•°å·¥å‚

### æ”¯æŒçš„æŸå¤±å‡½æ•°

| æŸå¤±å‡½æ•° | åç§° | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|----------|------|------|----------|
| CrossEntropyLoss | æ ‡å‡†äº¤å‰ç†µ | ç»å…¸åˆ†ç±»æŸå¤± | å¹³è¡¡æ•°æ®é›† |
| WeightedCrossEntropyLoss | åŠ æƒäº¤å‰ç†µ | è‡ªåŠ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ | ä¸å¹³è¡¡æ•°æ®é›† |
| FocalLoss | Focal Loss | ä¸“é—¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ | æåº¦ä¸å¹³è¡¡æ•°æ® |
| LabelSmoothingCrossEntropy | æ ‡ç­¾å¹³æ»‘ | å‡å°‘è¿‡æ‹Ÿåˆ | æé«˜æ³›åŒ–èƒ½åŠ› |

### ä½¿ç”¨ç¤ºä¾‹

```python
from src.training import create_loss_function

loss_config = {
    'name': 'WeightedCrossEntropyLoss',
    'params': {
        'auto_weight': True
    }
}

criterion = create_loss_function(loss_config, train_lines, num_classes)
```

### é…ç½®æ–‡ä»¶ç¤ºä¾‹

```yaml
training:
  loss_function:
    name: "FocalLoss"
    params:
      focal_alpha: 1.0
      focal_gamma: 2.0
```

## ğŸ”§ å·¥å‚APIè¯¦è§£

### OptimizerFactory

```python
from src.training import OptimizerFactory

# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = OptimizerFactory.create_optimizer(model, config)

# è·å–å¯ç”¨ä¼˜åŒ–å™¨
available = OptimizerFactory.get_available_optimizers()

# éªŒè¯é…ç½®
is_valid = OptimizerFactory.validate_config(config)

# æ³¨å†Œè‡ªå®šä¹‰ä¼˜åŒ–å™¨
OptimizerFactory.register_optimizer(name, optimizer_class, display_name, description, default_params)
```

### SchedulerFactory

```python
from src.training import SchedulerFactory

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = SchedulerFactory.create_scheduler(optimizer, config, total_epochs)

# æ£€æŸ¥æ˜¯å¦éœ€è¦æŒ‡æ ‡
needs_metric = SchedulerFactory.requires_metric('ReduceLROnPlateau')

# è·å–å¯ç”¨è°ƒåº¦å™¨
available = SchedulerFactory.get_available_schedulers()
```

### LossFactory

```python
from src.training import LossFactory

# åˆ›å»ºæŸå¤±å‡½æ•°
criterion = LossFactory.create_loss_function(config, train_lines, num_classes)

# è·å–å¯ç”¨æŸå¤±å‡½æ•°
available = LossFactory.get_available_losses()

# æ³¨å†Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°
LossFactory.register_loss(name, loss_class, display_name, description, default_params)
```

## ğŸ¯ æœ€ä½³å®è·µ

### ä¼˜åŒ–å™¨é€‰æ‹©

1. **é€šç”¨åœºæ™¯**: ä½¿ç”¨Adam
   ```yaml
   optimizer:
     name: "Adam"
     params:
       lr: 0.001
       weight_decay: 0.0001
   ```

2. **å¤§æ¨¡å‹è®­ç»ƒ**: ä½¿ç”¨AdamW
   ```yaml
   optimizer:
     name: "AdamW"
     params:
       lr: 0.001
       weight_decay: 0.01
   ```

3. **ç²¾ç¡®æ§åˆ¶**: ä½¿ç”¨SGD
   ```yaml
   optimizer:
     name: "SGD"
     params:
       lr: 0.01
       momentum: 0.9
       nesterov: true
   ```

### è°ƒåº¦å™¨é€‰æ‹©

1. **æ¨èé…ç½®**: ä½™å¼¦é€€ç«
   ```yaml
   scheduler:
     name: "CosineAnnealingLR"
     params:
       T_max: "auto"
       eta_min: 1e-6
   ```

2. **ç®€å•ç¨³å®š**: é˜¶æ¢¯è¡°å‡
   ```yaml
   scheduler:
     name: "StepLR"
     params:
       step_size: 10
       gamma: 0.5
   ```

3. **è‡ªé€‚åº”**: åŸºäºæŒ‡æ ‡è°ƒæ•´
   ```yaml
   scheduler:
     name: "ReduceLROnPlateau"
     params:
       mode: "min"
       factor: 0.5
       patience: 5
   ```

### æŸå¤±å‡½æ•°é€‰æ‹©

1. **å¹³è¡¡æ•°æ®**: æ ‡å‡†äº¤å‰ç†µ
   ```yaml
   loss_function:
     name: "CrossEntropyLoss"
   ```

2. **ä¸å¹³è¡¡æ•°æ®**: åŠ æƒäº¤å‰ç†µ
   ```yaml
   loss_function:
     name: "WeightedCrossEntropyLoss"
     params:
       auto_weight: true
   ```

3. **æåº¦ä¸å¹³è¡¡**: Focal Loss
   ```yaml
   loss_function:
     name: "FocalLoss"
     params:
       focal_alpha: 1.0
       focal_gamma: 2.0
   ```

## ğŸ”„ æ‰©å±•è‡ªå®šä¹‰ç»„ä»¶

### æ·»åŠ è‡ªå®šä¹‰ä¼˜åŒ–å™¨

```python
import torch.optim as optim
from src.training import register_custom_optimizer

class CustomOptimizer(optim.Optimizer):
    def __init__(self, params, lr=0.001, **kwargs):
        defaults = dict(lr=lr, **kwargs)
        super().__init__(params, defaults)
    
    def step(self, closure=None):
        # è‡ªå®šä¹‰ä¼˜åŒ–æ­¥éª¤
        pass

# æ³¨å†Œä¼˜åŒ–å™¨
register_custom_optimizer(
    name="custom",
    optimizer_class=CustomOptimizer,
    display_name="Custom Optimizer",
    description="è‡ªå®šä¹‰ä¼˜åŒ–å™¨",
    default_params={'lr': 0.001}
)
```

### æ·»åŠ è‡ªå®šä¹‰æŸå¤±å‡½æ•°

```python
import torch.nn as nn
from src.training import register_custom_loss

class CustomLoss(nn.Module):
    def __init__(self, alpha=1.0):
        super().__init__()
        self.alpha = alpha
    
    def forward(self, inputs, targets):
        # è‡ªå®šä¹‰æŸå¤±è®¡ç®—
        return loss

# æ³¨å†ŒæŸå¤±å‡½æ•°
register_custom_loss(
    name="custom_loss",
    loss_class=CustomLoss,
    display_name="Custom Loss",
    description="è‡ªå®šä¹‰æŸå¤±å‡½æ•°",
    default_params={'alpha': 1.0}
)
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å‚æ•°å…¼å®¹æ€§**: ç¡®ä¿å‚æ•°ä¸PyTorchå®˜æ–¹APIå…¼å®¹
2. **è®¾å¤‡ç®¡ç†**: æŸå¤±å‡½æ•°éœ€è¦æ‰‹åŠ¨ç§»åŠ¨åˆ°GPU
3. **è°ƒåº¦å™¨ç±»å‹**: ReduceLROnPlateauéœ€è¦åœ¨éªŒè¯åè°ƒç”¨step()
4. **æƒé‡è®¡ç®—**: WeightedCrossEntropyLossä¼šè‡ªåŠ¨è®¡ç®—ç±»åˆ«æƒé‡

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ä¼˜åŒ–å™¨å‚æ•°æ— æ•ˆ
**A**: æ£€æŸ¥å‚æ•°åç§°æ˜¯å¦æ­£ç¡®ï¼Œå‚è€ƒPyTorchå®˜æ–¹æ–‡æ¡£

### Q2: è°ƒåº¦å™¨ä¸ç”Ÿæ•ˆ
**A**: ç¡®ä¿åœ¨æ­£ç¡®çš„æ—¶æœºè°ƒç”¨scheduler.step()

### Q3: æŸå¤±å‡½æ•°æŠ¥é”™
**A**: æ£€æŸ¥è¾“å…¥å¼ é‡çš„å½¢çŠ¶å’Œè®¾å¤‡æ˜¯å¦åŒ¹é…

### Q4: è‡ªå®šä¹‰ç»„ä»¶æ³¨å†Œå¤±è´¥
**A**: ç¡®ä¿ç»„ä»¶ç±»ç»§æ‰¿äº†æ­£ç¡®çš„åŸºç±»
