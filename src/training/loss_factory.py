"""
æŸå¤±å‡½æ•°å·¥å‚
ç»Ÿä¸€çš„æŸå¤±å‡½æ•°åˆ›å»ºæ¥å£ï¼Œæ”¯æŒå¤šç§æŸå¤±å‡½æ•°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional, List
import numpy as np


class FocalLoss(nn.Module):
    """
    Focal Losså®ç°
    ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    """
    
    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class LabelSmoothingLoss(nn.Module):
    """
    æ ‡ç­¾å¹³æ»‘æŸå¤±å‡½æ•°
    å‡å°‘è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
    """
    
    def __init__(self, num_classes, smoothing=0.1, reduction='mean'):
        super(LabelSmoothingLoss, self).__init__()
        self.num_classes = num_classes
        self.smoothing = smoothing
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        log_probs = F.log_softmax(inputs, dim=1)
        targets_one_hot = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)
        targets_smooth = targets_one_hot * (1 - self.smoothing) + self.smoothing / self.num_classes
        loss = -torch.sum(targets_smooth * log_probs, dim=1)
        
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss


class LossFactory:
    """æŸå¤±å‡½æ•°å·¥å‚ç±»"""
    
    # æŸå¤±å‡½æ•°æ³¨å†Œè¡¨
    LOSS_REGISTRY = {
        'crossentropyloss': {
            'class': nn.CrossEntropyLoss,
            'name': 'CrossEntropyLoss',
            'description': 'æ ‡å‡†äº¤å‰ç†µæŸå¤±å‡½æ•°',
            'default_params': {
                'reduction': 'mean',
                'label_smoothing': 0.0
            },
            'supports_weights': True
        },
        'weightedcrossentropyloss': {
            'class': nn.CrossEntropyLoss,
            'name': 'WeightedCrossEntropyLoss',
            'description': 'åŠ æƒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œè‡ªåŠ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡',
            'default_params': {
                'reduction': 'mean',
                'label_smoothing': 0.0
            },
            'supports_weights': True,
            'auto_weight': True
        },
        'focalloss': {
            'class': FocalLoss,
            'name': 'FocalLoss',
            'description': 'Focal Lossï¼Œä¸“é—¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜',
            'default_params': {
                'alpha': 1.0,
                'gamma': 2.0,
                'reduction': 'mean'
            },
            'supports_weights': False
        },
        'labelsmoothingcrossentropy': {
            'class': LabelSmoothingLoss,
            'name': 'LabelSmoothingCrossEntropy',
            'description': 'æ ‡ç­¾å¹³æ»‘äº¤å‰ç†µæŸå¤±å‡½æ•°',
            'default_params': {
                'smoothing': 0.1,
                'reduction': 'mean'
            },
            'supports_weights': False
        }
    }
    
    @classmethod
    def create_loss_function(cls, config: Dict[str, Any], train_lines: List[str] = None, 
                           num_classes: int = None) -> nn.Module:
        """
        æ ¹æ®é…ç½®åˆ›å»ºæŸå¤±å‡½æ•°
        
        Args:
            config (Dict): æŸå¤±å‡½æ•°é…ç½®
            train_lines (List[str]): è®­ç»ƒæ•°æ®è¡Œï¼ˆç”¨äºè®¡ç®—ç±»åˆ«æƒé‡ï¼‰
            num_classes (int): ç±»åˆ«æ•°é‡
            
        Returns:
            nn.Module: åˆ›å»ºçš„æŸå¤±å‡½æ•°å®ä¾‹
            
        Example:
            config = {
                'name': 'FocalLoss',
                'params': {
                    'alpha': 1.0,
                    'gamma': 2.0
                }
            }
            loss_fn = LossFactory.create_loss_function(config)
        """
        # å…¼å®¹æ—§é…ç½®æ ¼å¼
        if isinstance(config, str):
            loss_name = config.lower()
            loss_params = {}
        else:
            loss_name = config.get('name', 'crossentropyloss').lower()
            loss_params = config.get('params', {})
        
        if loss_name not in cls.LOSS_REGISTRY:
            available_losses = list(cls.LOSS_REGISTRY.keys())
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°ç±»å‹: {loss_name}. "
                           f"æ”¯æŒçš„æŸå¤±å‡½æ•°: {available_losses}")
        
        # è·å–æŸå¤±å‡½æ•°ä¿¡æ¯
        loss_info = cls.LOSS_REGISTRY[loss_name]
        loss_class = loss_info['class']
        default_params = loss_info['default_params'].copy()
        
        # åˆå¹¶é»˜è®¤å‚æ•°å’Œç”¨æˆ·å‚æ•°
        final_params = {**default_params, **loss_params}
        
        # å¤„ç†ç‰¹æ®Šå‚æ•°
        final_params = cls._process_special_params(
            loss_name, final_params, loss_info, train_lines, num_classes
        )
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        try:
            loss_function = loss_class(**final_params)
            
            # æ‰“å°æŸå¤±å‡½æ•°ä¿¡æ¯
            cls._print_loss_info(loss_name, final_params, loss_info)
            
            return loss_function
            
        except Exception as e:
            raise RuntimeError(f"åˆ›å»ºæŸå¤±å‡½æ•° {loss_name} å¤±è´¥: {e}")
    
    @classmethod
    def _process_special_params(cls, loss_name: str, params: Dict[str, Any], 
                               loss_info: Dict[str, Any], train_lines: List[str] = None, 
                               num_classes: int = None) -> Dict[str, Any]:
        """å¤„ç†ç‰¹æ®Šå‚æ•°"""
        final_params = params.copy()
        
        # å¤„ç†åŠ æƒäº¤å‰ç†µçš„æƒé‡è®¡ç®—
        if loss_info.get('auto_weight', False) and train_lines and num_classes:
            if params.get('auto_weight', True):
                weights = cls._calculate_class_weights(train_lines, num_classes)
                final_params['weight'] = weights
                print(f"ğŸ”¢ è‡ªåŠ¨è®¡ç®—ç±»åˆ«æƒé‡: {weights.tolist()}")
                # ç§»é™¤auto_weightå‚æ•°ï¼Œå› ä¸ºPyTorchçš„CrossEntropyLossä¸æ¥å—è¿™ä¸ªå‚æ•°
                final_params.pop('auto_weight', None)
        
        # å¤„ç†æ ‡ç­¾å¹³æ»‘æŸå¤±çš„ç±»åˆ«æ•°é‡
        if loss_name == 'labelsmoothingcrossentropy' and num_classes:
            final_params['num_classes'] = num_classes
        
        return final_params
    
    @classmethod
    def _calculate_class_weights(cls, train_lines: List[str], num_classes: int) -> torch.FloatTensor:
        """è®¡ç®—ç±»åˆ«æƒé‡"""
        class_counts = [0] * num_classes
        
        for line in train_lines:
            try:
                class_id = int(line.split(';')[0])
                if 0 <= class_id < num_classes:
                    class_counts[class_id] += 1
            except (ValueError, IndexError):
                continue
        
        total_samples = sum(class_counts)
        if total_samples == 0:
            return torch.ones(num_classes, dtype=torch.float32)
        
        # è®¡ç®—æƒé‡ï¼šæ€»æ ·æœ¬æ•° / (ç±»åˆ«æ•° * è¯¥ç±»åˆ«æ ·æœ¬æ•°)
        class_weights = []
        for count in class_counts:
            if count > 0:
                weight = total_samples / (num_classes * count)
            else:
                weight = 1.0  # å¦‚æœæŸä¸ªç±»åˆ«æ²¡æœ‰æ ·æœ¬ï¼Œè®¾ç½®æƒé‡ä¸º1
            class_weights.append(weight)
        
        return torch.FloatTensor(class_weights)
    
    @classmethod
    def _print_loss_info(cls, loss_name: str, params: Dict[str, Any], loss_info: Dict[str, Any]):
        """æ‰“å°æŸå¤±å‡½æ•°ä¿¡æ¯"""
        print(f"\nğŸ¯ æŸå¤±å‡½æ•°é…ç½®:")
        print(f"   æŸå¤±å‡½æ•°ç±»å‹: {loss_info['name']}")
        
        # æ‰“å°å…³é”®å‚æ•°
        if loss_name == 'crossentropyloss' or loss_name == 'weightedcrossentropyloss':
            print(f"   æ ‡ç­¾å¹³æ»‘: {params.get('label_smoothing', 0.0)}")
            if 'weight' in params:
                print(f"   ä½¿ç”¨ç±»åˆ«æƒé‡: æ˜¯")
            else:
                print(f"   ä½¿ç”¨ç±»åˆ«æƒé‡: å¦")
        elif loss_name == 'focalloss':
            print(f"   Alpha: {params.get('alpha', 1.0)}")
            print(f"   Gamma: {params.get('gamma', 2.0)}")
        elif loss_name == 'labelsmoothingcrossentropy':
            print(f"   å¹³æ»‘å‚æ•°: {params.get('smoothing', 0.1)}")
            print(f"   ç±»åˆ«æ•°é‡: {params.get('num_classes', 'N/A')}")
        
        print(f"   å½’çº¦æ–¹å¼: {params.get('reduction', 'mean')}")
    
    @classmethod
    def get_available_losses(cls) -> Dict[str, Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„æŸå¤±å‡½æ•°ä¿¡æ¯
        
        Returns:
            Dict: æŸå¤±å‡½æ•°ä¿¡æ¯å­—å…¸
        """
        return cls.LOSS_REGISTRY.copy()
    
    @classmethod
    def get_loss_info(cls, loss_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šæŸå¤±å‡½æ•°çš„ä¿¡æ¯
        
        Args:
            loss_name (str): æŸå¤±å‡½æ•°åç§°
            
        Returns:
            Dict: æŸå¤±å‡½æ•°ä¿¡æ¯ï¼Œå¦‚æœæŸå¤±å‡½æ•°ä¸å­˜åœ¨è¿”å›None
        """
        return cls.LOSS_REGISTRY.get(loss_name.lower())
    
    @classmethod
    def register_loss(cls, name: str, loss_class, display_name: str, 
                     description: str, default_params: Dict[str, Any], 
                     supports_weights: bool = False, auto_weight: bool = False):
        """
        æ³¨å†Œæ–°çš„æŸå¤±å‡½æ•°ç±»å‹
        
        Args:
            name (str): æŸå¤±å‡½æ•°åç§°ï¼ˆç”¨äºé…ç½®æ–‡ä»¶ï¼‰
            loss_class: æŸå¤±å‡½æ•°ç±»
            display_name (str): æ˜¾ç¤ºåç§°
            description (str): æŸå¤±å‡½æ•°æè¿°
            default_params (Dict): é»˜è®¤å‚æ•°
            supports_weights (bool): æ˜¯å¦æ”¯æŒç±»åˆ«æƒé‡
            auto_weight (bool): æ˜¯å¦è‡ªåŠ¨è®¡ç®—æƒé‡
        """
        cls.LOSS_REGISTRY[name.lower()] = {
            'class': loss_class,
            'name': display_name,
            'description': description,
            'default_params': default_params,
            'supports_weights': supports_weights,
            'auto_weight': auto_weight
        }
        print(f"âœ… å·²æ³¨å†Œæ–°æŸå¤±å‡½æ•°: {display_name} ({name})")
    
    @classmethod
    def validate_config(cls, config: Dict[str, Any]) -> bool:
        """
        éªŒè¯æŸå¤±å‡½æ•°é…ç½®
        
        Args:
            config (Dict): æŸå¤±å‡½æ•°é…ç½®
            
        Returns:
            bool: é…ç½®æ˜¯å¦æœ‰æ•ˆ
        """
        if isinstance(config, str):
            return config.lower() in cls.LOSS_REGISTRY
        
        if not isinstance(config, dict):
            print("âŒ æŸå¤±å‡½æ•°é…ç½®å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸")
            return False
        
        if 'name' not in config:
            print("âŒ æŸå¤±å‡½æ•°é…ç½®ç¼ºå°‘ 'name' å­—æ®µ")
            return False
        
        loss_name = config['name'].lower()
        if loss_name not in cls.LOSS_REGISTRY:
            available_losses = list(cls.LOSS_REGISTRY.keys())
            print(f"âŒ ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°ç±»å‹: {loss_name}")
            print(f"   æ”¯æŒçš„æŸå¤±å‡½æ•°: {available_losses}")
            return False
        
        return True


# ä¾¿æ·å‡½æ•°
def create_loss_function(config: Dict[str, Any], train_lines: List[str] = None, 
                        num_classes: int = None) -> nn.Module:
    """
    ä¾¿æ·çš„æŸå¤±å‡½æ•°åˆ›å»ºå‡½æ•°
    
    Args:
        config (Dict): æŸå¤±å‡½æ•°é…ç½®
        train_lines (List[str]): è®­ç»ƒæ•°æ®è¡Œ
        num_classes (int): ç±»åˆ«æ•°é‡
        
    Returns:
        nn.Module: åˆ›å»ºçš„æŸå¤±å‡½æ•°
    """
    return LossFactory.create_loss_function(config, train_lines, num_classes)


def get_available_losses() -> Dict[str, Dict[str, Any]]:
    """è·å–æ‰€æœ‰å¯ç”¨æŸå¤±å‡½æ•°"""
    return LossFactory.get_available_losses()


def register_custom_loss(name: str, loss_class, display_name: str, description: str, 
                        default_params: Dict[str, Any], supports_weights: bool = False, 
                        auto_weight: bool = False):
    """æ³¨å†Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°"""
    LossFactory.register_loss(name, loss_class, display_name, description, 
                             default_params, supports_weights, auto_weight)
