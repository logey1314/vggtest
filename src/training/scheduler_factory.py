"""
å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥å‚
ç»Ÿä¸€çš„å­¦ä¹ ç‡è°ƒåº¦å™¨åˆ›å»ºæ¥å£ï¼Œæ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥
"""

import torch.optim as optim
from typing import Dict, Any, Optional, Union


class SchedulerFactory:
    """å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥å‚ç±»"""
    
    # è°ƒåº¦å™¨æ³¨å†Œè¡¨
    SCHEDULER_REGISTRY = {
        'steplr': {
            'class': optim.lr_scheduler.StepLR,
            'name': 'StepLR',
            'description': 'é˜¶æ¢¯å¼å­¦ä¹ ç‡è¡°å‡',
            'default_params': {
                'step_size': 7,
                'gamma': 0.5,
                'last_epoch': -1
            },
            'requires_metric': False,
            'supported_params': ['step_size', 'gamma', 'last_epoch']
        },
        'multisteplr': {
            'class': optim.lr_scheduler.MultiStepLR,
            'name': 'MultiStepLR',
            'description': 'å¤šé˜¶æ¢¯å­¦ä¹ ç‡è¡°å‡',
            'default_params': {
                'milestones': [10, 15],
                'gamma': 0.1,
                'last_epoch': -1
            },
            'requires_metric': False,
            'supported_params': ['milestones', 'gamma', 'last_epoch']
        },
        'cosineannealinglr': {
            'class': optim.lr_scheduler.CosineAnnealingLR,
            'name': 'CosineAnnealingLR',
            'description': 'ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦',
            'default_params': {
                'T_max': 20,
                'eta_min': 0,
                'last_epoch': -1
            },
            'requires_metric': False,
            'supported_params': ['T_max', 'eta_min', 'last_epoch']
        },
        'reducelronplateau': {
            'class': optim.lr_scheduler.ReduceLROnPlateau,
            'name': 'ReduceLROnPlateau',
            'description': 'åŸºäºæŒ‡æ ‡çš„è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦',
            'default_params': {
                'mode': 'min',
                'factor': 0.5,
                'patience': 3,
                'threshold': 1e-4,
                'threshold_mode': 'rel',
                'cooldown': 0,
                'min_lr': 0,
                'eps': 1e-8
            },
            'requires_metric': True,
            'supported_params': ['mode', 'factor', 'patience', 'threshold', 'threshold_mode', 'cooldown', 'min_lr', 'eps']
        },
        'exponentiallr': {
            'class': optim.lr_scheduler.ExponentialLR,
            'name': 'ExponentialLR',
            'description': 'æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡è°ƒåº¦',
            'default_params': {
                'gamma': 0.95,
                'last_epoch': -1
            },
            'requires_metric': False,
            'supported_params': ['gamma', 'last_epoch']
        },
        'cosineannealingwarmrestarts': {
            'class': optim.lr_scheduler.CosineAnnealingWarmRestarts,
            'name': 'CosineAnnealingWarmRestarts',
            'description': 'å¸¦çƒ­é‡å¯çš„ä½™å¼¦é€€ç«',
            'default_params': {
                'T_0': 10,
                'T_mult': 1,
                'eta_min': 0,
                'last_epoch': -1
            },
            'requires_metric': False,
            'supported_params': ['T_0', 'T_mult', 'eta_min', 'last_epoch']
        }
    }
    
    @classmethod
    def create_scheduler(cls, optimizer: optim.Optimizer, config: Dict[str, Any], 
                        total_epochs: int = None) -> Optional[optim.lr_scheduler._LRScheduler]:
        """
        æ ¹æ®é…ç½®åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        
        Args:
            optimizer (optim.Optimizer): ä¼˜åŒ–å™¨
            config (Dict): è°ƒåº¦å™¨é…ç½®
            total_epochs (int): æ€»è®­ç»ƒè½®æ•°ï¼ˆç”¨äºè‡ªåŠ¨è®¾ç½®T_maxï¼‰
            
        Returns:
            optim.lr_scheduler._LRScheduler: åˆ›å»ºçš„è°ƒåº¦å™¨å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä½¿ç”¨è°ƒåº¦å™¨
            
        Example:
            config = {
                'name': 'CosineAnnealingLR',
                'params': {
                    'T_max': 'auto',
                    'eta_min': 1e-6
                }
            }
            scheduler = SchedulerFactory.create_scheduler(optimizer, config, total_epochs=100)
        """
        # å…¼å®¹æ—§é…ç½®æ ¼å¼
        if isinstance(config, str):
            if config.lower() == 'none' or config.lower() == 'null':
                return None
            scheduler_name = config.lower()
            scheduler_params = {}
        else:
            scheduler_name = config.get('name', '').lower()
            if scheduler_name == 'none' or scheduler_name == 'null' or scheduler_name == '':
                return None
            scheduler_params = config.get('params', {})
        
        if scheduler_name not in cls.SCHEDULER_REGISTRY:
            available_schedulers = list(cls.SCHEDULER_REGISTRY.keys())
            raise ValueError(f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹: {scheduler_name}. "
                           f"æ”¯æŒçš„è°ƒåº¦å™¨: {available_schedulers}")
        
        # è·å–è°ƒåº¦å™¨ä¿¡æ¯
        scheduler_info = cls.SCHEDULER_REGISTRY[scheduler_name]
        scheduler_class = scheduler_info['class']
        default_params = scheduler_info['default_params'].copy()
        supported_params = scheduler_info['supported_params']

        # å¤„ç†ç‰¹æ®Šå‚æ•°
        processed_params = cls._process_special_params(
            scheduler_name, default_params, scheduler_params, total_epochs
        )

        # åªä¿ç•™è¯¥è°ƒåº¦å™¨æ”¯æŒçš„å‚æ•°
        filtered_params = {k: v for k, v in processed_params.items() if k in supported_params}

        # è½¬æ¢å‚æ•°ç±»å‹
        final_params = cls._convert_param_types(filtered_params)
        
        # åˆ›å»ºè°ƒåº¦å™¨
        try:
            scheduler = scheduler_class(optimizer, **final_params)
            
            # æ‰“å°è°ƒåº¦å™¨ä¿¡æ¯
            cls._print_scheduler_info(scheduler_name, final_params, scheduler_info['requires_metric'], supported_params)
            
            return scheduler
            
        except Exception as e:
            raise RuntimeError(f"åˆ›å»ºè°ƒåº¦å™¨ {scheduler_name} å¤±è´¥: {e}")
    
    @classmethod
    def _process_special_params(cls, scheduler_name: str, default_params: Dict[str, Any], 
                               user_params: Dict[str, Any], total_epochs: int = None) -> Dict[str, Any]:
        """å¤„ç†ç‰¹æ®Šå‚æ•°"""
        final_params = {**default_params, **user_params}
        
        # å¤„ç†CosineAnnealingLRçš„T_maxè‡ªåŠ¨è®¾ç½®
        if scheduler_name == 'cosineannealinglr':
            if final_params.get('T_max') == 'auto' and total_epochs:
                final_params['T_max'] = total_epochs
                print(f"ğŸ”„ è‡ªåŠ¨è®¾ç½® T_max = {total_epochs}")
            elif final_params.get('T_max') == 'auto':
                final_params['T_max'] = 20  # é»˜è®¤å€¼
                print(f"âš ï¸  æœªæä¾›æ€»è½®æ•°ï¼Œä½¿ç”¨é»˜è®¤ T_max = 20")
        
        # å¤„ç†CosineAnnealingWarmRestartsçš„T_0è‡ªåŠ¨è®¾ç½®
        if scheduler_name == 'cosineannealingwarmrestarts':
            if final_params.get('T_0') == 'auto' and total_epochs:
                final_params['T_0'] = max(1, total_epochs // 10)  # æ€»è½®æ•°çš„1/10
                print(f"ğŸ”„ è‡ªåŠ¨è®¾ç½® T_0 = {final_params['T_0']}")
        
        # ç¡®ä¿eta_minæ˜¯æ•°å€¼ç±»å‹
        if 'eta_min' in final_params and isinstance(final_params['eta_min'], str):
            try:
                final_params['eta_min'] = float(final_params['eta_min'])
            except ValueError:
                final_params['eta_min'] = 0
        
        return final_params

    @classmethod
    def _convert_param_types(cls, params: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢å‚æ•°ç±»å‹ï¼Œç¡®ä¿æ•°å€¼å‚æ•°æ˜¯æ­£ç¡®çš„ç±»å‹"""
        converted_params = {}

        for key, value in params.items():
            try:
                if key in ['step_size', 'T_max', 'T_0', 'T_mult', 'patience', 'cooldown', 'last_epoch']:
                    # æ•´æ•°å‚æ•°
                    converted_params[key] = int(value)
                elif key in ['gamma', 'eta_min', 'factor', 'threshold', 'min_lr', 'eps']:
                    # æµ®ç‚¹æ•°å‚æ•°
                    converted_params[key] = float(value)
                elif key == 'milestones':
                    # é‡Œç¨‹ç¢‘åˆ—è¡¨è½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
                    if isinstance(value, (list, tuple)):
                        converted_params[key] = [int(x) for x in value]
                    else:
                        converted_params[key] = value
                elif key in ['mode', 'threshold_mode']:
                    # å­—ç¬¦ä¸²å‚æ•°
                    converted_params[key] = str(value)
                else:
                    # å…¶ä»–å‚æ•°ä¿æŒåŸæ ·
                    converted_params[key] = value

            except (ValueError, TypeError) as e:
                print(f"âš ï¸  è°ƒåº¦å™¨å‚æ•° {key} ç±»å‹è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå€¼: {value} (é”™è¯¯: {e})")
                converted_params[key] = value

        return converted_params

    @classmethod
    def _print_scheduler_info(cls, scheduler_name: str, params: Dict[str, Any], requires_metric: bool, supported_params: list):
        """æ‰“å°è°ƒåº¦å™¨ä¿¡æ¯"""
        scheduler_info = cls.SCHEDULER_REGISTRY[scheduler_name]
        
        print(f"\nğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®:")
        print(f"   è°ƒåº¦å™¨ç±»å‹: {scheduler_info['name']}")
        print(f"   éœ€è¦æŒ‡æ ‡: {'æ˜¯' if requires_metric else 'å¦'}")
        print(f"   ä½¿ç”¨å‚æ•°: {list(params.keys())}")

        # æ‰“å°å…³é”®å‚æ•°
        if scheduler_name == 'steplr':
            if 'step_size' in params:
                print(f"   æ­¥é•¿: {params['step_size']}")
            if 'gamma' in params:
                print(f"   è¡°å‡å› å­: {params['gamma']}")
        elif scheduler_name == 'multisteplr':
            if 'milestones' in params:
                print(f"   é‡Œç¨‹ç¢‘: {params['milestones']}")
            if 'gamma' in params:
                print(f"   è¡°å‡å› å­: {params['gamma']}")
        elif scheduler_name == 'cosineannealinglr':
            if 'T_max' in params:
                print(f"   T_max: {params['T_max']}")
            if 'eta_min' in params:
                print(f"   æœ€å°å­¦ä¹ ç‡: {params['eta_min']}")
        elif scheduler_name == 'reducelronplateau':
            if 'mode' in params:
                print(f"   æ¨¡å¼: {params['mode']}")
            if 'factor' in params:
                print(f"   è¡°å‡å› å­: {params['factor']}")
            if 'patience' in params:
                print(f"   è€å¿ƒå€¼: {params['patience']}")
    
    @classmethod
    def get_available_schedulers(cls) -> Dict[str, Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„è°ƒåº¦å™¨ä¿¡æ¯
        
        Returns:
            Dict: è°ƒåº¦å™¨ä¿¡æ¯å­—å…¸
        """
        return cls.SCHEDULER_REGISTRY.copy()
    
    @classmethod
    def get_scheduler_info(cls, scheduler_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šè°ƒåº¦å™¨çš„ä¿¡æ¯
        
        Args:
            scheduler_name (str): è°ƒåº¦å™¨åç§°
            
        Returns:
            Dict: è°ƒåº¦å™¨ä¿¡æ¯ï¼Œå¦‚æœè°ƒåº¦å™¨ä¸å­˜åœ¨è¿”å›None
        """
        return cls.SCHEDULER_REGISTRY.get(scheduler_name.lower())
    
    @classmethod
    def requires_metric(cls, scheduler_name: str) -> bool:
        """
        æ£€æŸ¥è°ƒåº¦å™¨æ˜¯å¦éœ€è¦æŒ‡æ ‡
        
        Args:
            scheduler_name (str): è°ƒåº¦å™¨åç§°
            
        Returns:
            bool: æ˜¯å¦éœ€è¦æŒ‡æ ‡
        """
        scheduler_info = cls.SCHEDULER_REGISTRY.get(scheduler_name.lower())
        return scheduler_info['requires_metric'] if scheduler_info else False
    
    @classmethod
    def register_scheduler(cls, name: str, scheduler_class, display_name: str, 
                          description: str, default_params: Dict[str, Any], requires_metric: bool = False):
        """
        æ³¨å†Œæ–°çš„è°ƒåº¦å™¨ç±»å‹
        
        Args:
            name (str): è°ƒåº¦å™¨åç§°ï¼ˆç”¨äºé…ç½®æ–‡ä»¶ï¼‰
            scheduler_class: è°ƒåº¦å™¨ç±»
            display_name (str): æ˜¾ç¤ºåç§°
            description (str): è°ƒåº¦å™¨æè¿°
            default_params (Dict): é»˜è®¤å‚æ•°
            requires_metric (bool): æ˜¯å¦éœ€è¦æŒ‡æ ‡
        """
        cls.SCHEDULER_REGISTRY[name.lower()] = {
            'class': scheduler_class,
            'name': display_name,
            'description': description,
            'default_params': default_params,
            'requires_metric': requires_metric
        }
        print(f"âœ… å·²æ³¨å†Œæ–°è°ƒåº¦å™¨: {display_name} ({name})")


# ä¾¿æ·å‡½æ•°
def create_scheduler(optimizer: optim.Optimizer, config: Dict[str, Any], 
                    total_epochs: int = None) -> Optional[optim.lr_scheduler._LRScheduler]:
    """
    ä¾¿æ·çš„è°ƒåº¦å™¨åˆ›å»ºå‡½æ•°
    
    Args:
        optimizer (optim.Optimizer): ä¼˜åŒ–å™¨
        config (Dict): è°ƒåº¦å™¨é…ç½®
        total_epochs (int): æ€»è®­ç»ƒè½®æ•°
        
    Returns:
        optim.lr_scheduler._LRScheduler: åˆ›å»ºçš„è°ƒåº¦å™¨
    """
    return SchedulerFactory.create_scheduler(optimizer, config, total_epochs)


def get_available_schedulers() -> Dict[str, Dict[str, Any]]:
    """è·å–æ‰€æœ‰å¯ç”¨è°ƒåº¦å™¨"""
    return SchedulerFactory.get_available_schedulers()


def requires_metric(scheduler_name: str) -> bool:
    """æ£€æŸ¥è°ƒåº¦å™¨æ˜¯å¦éœ€è¦æŒ‡æ ‡"""
    return SchedulerFactory.requires_metric(scheduler_name)
