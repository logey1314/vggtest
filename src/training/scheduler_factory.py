"""
å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥å‚
ç»Ÿä¸€çš„å­¦ä¹ ç‡è°ƒåº¦å™¨åˆ›å»ºæ¥å£ï¼Œæ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥
"""

import torch.optim as optim
from typing import Dict, Any, Optional, Union


class SchedulerFactory:
    """å­¦ä¹ ç‡è°ƒåº¦å™¨å·¥å‚ç±»"""
    
    # è°ƒåº¦å™¨æ³¨å†Œè¡¨
    SCHEDULER_REGISTRY = {
        'steplr': {
            'class': optim.lr_scheduler.StepLR,
            'name': 'StepLR',
            'description': 'é˜¶æ¢¯å¼å­¦ä¹ ç‡è¡°å‡',
            'default_params': {
                'step_size': 7,
                'gamma': 0.5,
                'last_epoch': -1
            },
            'requires_metric': False
        },
        'multisteplr': {
            'class': optim.lr_scheduler.MultiStepLR,
            'name': 'MultiStepLR',
            'description': 'å¤šé˜¶æ¢¯å­¦ä¹ ç‡è¡°å‡',
            'default_params': {
                'milestones': [10, 15],
                'gamma': 0.1,
                'last_epoch': -1
            },
            'requires_metric': False
        },
        'cosineannealinglr': {
            'class': optim.lr_scheduler.CosineAnnealingLR,
            'name': 'CosineAnnealingLR',
            'description': 'ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦',
            'default_params': {
                'T_max': 20,
                'eta_min': 0,
                'last_epoch': -1
            },
            'requires_metric': False
        },
        'reducelronplateau': {
            'class': optim.lr_scheduler.ReduceLROnPlateau,
            'name': 'ReduceLROnPlateau',
            'description': 'åŸºäºæŒ‡æ ‡çš„è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦',
            'default_params': {
                'mode': 'min',
                'factor': 0.5,
                'patience': 3,
                'threshold': 1e-4,
                'threshold_mode': 'rel',
                'cooldown': 0,
                'min_lr': 0,
                'eps': 1e-8
            },
            'requires_metric': True
        },
        'exponentiallr': {
            'class': optim.lr_scheduler.ExponentialLR,
            'name': 'ExponentialLR',
            'description': 'æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡è°ƒåº¦',
            'default_params': {
                'gamma': 0.95,
                'last_epoch': -1
            },
            'requires_metric': False
        },
        'cosineannealingwarmrestarts': {
            'class': optim.lr_scheduler.CosineAnnealingWarmRestarts,
            'name': 'CosineAnnealingWarmRestarts',
            'description': 'å¸¦çƒ­é‡å¯çš„ä½™å¼¦é€€ç«',
            'default_params': {
                'T_0': 10,
                'T_mult': 1,
                'eta_min': 0,
                'last_epoch': -1
            },
            'requires_metric': False
        }
    }
    
    @classmethod
    def create_scheduler(cls, optimizer: optim.Optimizer, config: Dict[str, Any], 
                        total_epochs: int = None) -> Optional[optim.lr_scheduler._LRScheduler]:
        """
        æ ¹æ®é…ç½®åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        
        Args:
            optimizer (optim.Optimizer): ä¼˜åŒ–å™¨
            config (Dict): è°ƒåº¦å™¨é…ç½®
            total_epochs (int): æ€»è®­ç»ƒè½®æ•°ï¼ˆç”¨äºè‡ªåŠ¨è®¾ç½®T_maxï¼‰
            
        Returns:
            optim.lr_scheduler._LRScheduler: åˆ›å»ºçš„è°ƒåº¦å™¨å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä½¿ç”¨è°ƒåº¦å™¨
            
        Example:
            config = {
                'name': 'CosineAnnealingLR',
                'params': {
                    'T_max': 'auto',
                    'eta_min': 1e-6
                }
            }
            scheduler = SchedulerFactory.create_scheduler(optimizer, config, total_epochs=100)
        """
        # å…¼å®¹æ—§é…ç½®æ ¼å¼
        if isinstance(config, str):
            if config.lower() == 'none' or config.lower() == 'null':
                return None
            scheduler_name = config.lower()
            scheduler_params = {}
        else:
            scheduler_name = config.get('name', '').lower()
            if scheduler_name == 'none' or scheduler_name == 'null' or scheduler_name == '':
                return None
            scheduler_params = config.get('params', {})
        
        if scheduler_name not in cls.SCHEDULER_REGISTRY:
            available_schedulers = list(cls.SCHEDULER_REGISTRY.keys())
            raise ValueError(f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹: {scheduler_name}. "
                           f"æ”¯æŒçš„è°ƒåº¦å™¨: {available_schedulers}")
        
        # è·å–è°ƒåº¦å™¨ä¿¡æ¯
        scheduler_info = cls.SCHEDULER_REGISTRY[scheduler_name]
        scheduler_class = scheduler_info['class']
        default_params = scheduler_info['default_params'].copy()
        
        # å¤„ç†ç‰¹æ®Šå‚æ•°
        final_params = cls._process_special_params(
            scheduler_name, default_params, scheduler_params, total_epochs
        )
        
        # åˆ›å»ºè°ƒåº¦å™¨
        try:
            scheduler = scheduler_class(optimizer, **final_params)
            
            # æ‰“å°è°ƒåº¦å™¨ä¿¡æ¯
            cls._print_scheduler_info(scheduler_name, final_params, scheduler_info['requires_metric'])
            
            return scheduler
            
        except Exception as e:
            raise RuntimeError(f"åˆ›å»ºè°ƒåº¦å™¨ {scheduler_name} å¤±è´¥: {e}")
    
    @classmethod
    def _process_special_params(cls, scheduler_name: str, default_params: Dict[str, Any], 
                               user_params: Dict[str, Any], total_epochs: int = None) -> Dict[str, Any]:
        """å¤„ç†ç‰¹æ®Šå‚æ•°"""
        final_params = {**default_params, **user_params}
        
        # å¤„ç†CosineAnnealingLRçš„T_maxè‡ªåŠ¨è®¾ç½®
        if scheduler_name == 'cosineannealinglr':
            if final_params.get('T_max') == 'auto' and total_epochs:
                final_params['T_max'] = total_epochs
                print(f"ğŸ”„ è‡ªåŠ¨è®¾ç½® T_max = {total_epochs}")
            elif final_params.get('T_max') == 'auto':
                final_params['T_max'] = 20  # é»˜è®¤å€¼
                print(f"âš ï¸  æœªæä¾›æ€»è½®æ•°ï¼Œä½¿ç”¨é»˜è®¤ T_max = 20")
        
        # å¤„ç†CosineAnnealingWarmRestartsçš„T_0è‡ªåŠ¨è®¾ç½®
        if scheduler_name == 'cosineannealingwarmrestarts':
            if final_params.get('T_0') == 'auto' and total_epochs:
                final_params['T_0'] = max(1, total_epochs // 10)  # æ€»è½®æ•°çš„1/10
                print(f"ğŸ”„ è‡ªåŠ¨è®¾ç½® T_0 = {final_params['T_0']}")
        
        # ç¡®ä¿eta_minæ˜¯æ•°å€¼ç±»å‹
        if 'eta_min' in final_params and isinstance(final_params['eta_min'], str):
            try:
                final_params['eta_min'] = float(final_params['eta_min'])
            except ValueError:
                final_params['eta_min'] = 0
        
        return final_params
    
    @classmethod
    def _print_scheduler_info(cls, scheduler_name: str, params: Dict[str, Any], requires_metric: bool):
        """æ‰“å°è°ƒåº¦å™¨ä¿¡æ¯"""
        scheduler_info = cls.SCHEDULER_REGISTRY[scheduler_name]
        
        print(f"\nğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®:")
        print(f"   è°ƒåº¦å™¨ç±»å‹: {scheduler_info['name']}")
        print(f"   éœ€è¦æŒ‡æ ‡: {'æ˜¯' if requires_metric else 'å¦'}")
        
        # æ‰“å°å…³é”®å‚æ•°
        if scheduler_name == 'steplr':
            print(f"   æ­¥é•¿: {params.get('step_size', 'N/A')}")
            print(f"   è¡°å‡å› å­: {params.get('gamma', 'N/A')}")
        elif scheduler_name == 'multisteplr':
            print(f"   é‡Œç¨‹ç¢‘: {params.get('milestones', 'N/A')}")
            print(f"   è¡°å‡å› å­: {params.get('gamma', 'N/A')}")
        elif scheduler_name == 'cosineannealinglr':
            print(f"   T_max: {params.get('T_max', 'N/A')}")
            print(f"   æœ€å°å­¦ä¹ ç‡: {params.get('eta_min', 'N/A')}")
        elif scheduler_name == 'reducelronplateau':
            print(f"   æ¨¡å¼: {params.get('mode', 'N/A')}")
            print(f"   è¡°å‡å› å­: {params.get('factor', 'N/A')}")
            print(f"   è€å¿ƒå€¼: {params.get('patience', 'N/A')}")
    
    @classmethod
    def get_available_schedulers(cls) -> Dict[str, Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„è°ƒåº¦å™¨ä¿¡æ¯
        
        Returns:
            Dict: è°ƒåº¦å™¨ä¿¡æ¯å­—å…¸
        """
        return cls.SCHEDULER_REGISTRY.copy()
    
    @classmethod
    def get_scheduler_info(cls, scheduler_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šè°ƒåº¦å™¨çš„ä¿¡æ¯
        
        Args:
            scheduler_name (str): è°ƒåº¦å™¨åç§°
            
        Returns:
            Dict: è°ƒåº¦å™¨ä¿¡æ¯ï¼Œå¦‚æœè°ƒåº¦å™¨ä¸å­˜åœ¨è¿”å›None
        """
        return cls.SCHEDULER_REGISTRY.get(scheduler_name.lower())
    
    @classmethod
    def requires_metric(cls, scheduler_name: str) -> bool:
        """
        æ£€æŸ¥è°ƒåº¦å™¨æ˜¯å¦éœ€è¦æŒ‡æ ‡
        
        Args:
            scheduler_name (str): è°ƒåº¦å™¨åç§°
            
        Returns:
            bool: æ˜¯å¦éœ€è¦æŒ‡æ ‡
        """
        scheduler_info = cls.SCHEDULER_REGISTRY.get(scheduler_name.lower())
        return scheduler_info['requires_metric'] if scheduler_info else False
    
    @classmethod
    def register_scheduler(cls, name: str, scheduler_class, display_name: str, 
                          description: str, default_params: Dict[str, Any], requires_metric: bool = False):
        """
        æ³¨å†Œæ–°çš„è°ƒåº¦å™¨ç±»å‹
        
        Args:
            name (str): è°ƒåº¦å™¨åç§°ï¼ˆç”¨äºé…ç½®æ–‡ä»¶ï¼‰
            scheduler_class: è°ƒåº¦å™¨ç±»
            display_name (str): æ˜¾ç¤ºåç§°
            description (str): è°ƒåº¦å™¨æè¿°
            default_params (Dict): é»˜è®¤å‚æ•°
            requires_metric (bool): æ˜¯å¦éœ€è¦æŒ‡æ ‡
        """
        cls.SCHEDULER_REGISTRY[name.lower()] = {
            'class': scheduler_class,
            'name': display_name,
            'description': description,
            'default_params': default_params,
            'requires_metric': requires_metric
        }
        print(f"âœ… å·²æ³¨å†Œæ–°è°ƒåº¦å™¨: {display_name} ({name})")


# ä¾¿æ·å‡½æ•°
def create_scheduler(optimizer: optim.Optimizer, config: Dict[str, Any], 
                    total_epochs: int = None) -> Optional[optim.lr_scheduler._LRScheduler]:
    """
    ä¾¿æ·çš„è°ƒåº¦å™¨åˆ›å»ºå‡½æ•°
    
    Args:
        optimizer (optim.Optimizer): ä¼˜åŒ–å™¨
        config (Dict): è°ƒåº¦å™¨é…ç½®
        total_epochs (int): æ€»è®­ç»ƒè½®æ•°
        
    Returns:
        optim.lr_scheduler._LRScheduler: åˆ›å»ºçš„è°ƒåº¦å™¨
    """
    return SchedulerFactory.create_scheduler(optimizer, config, total_epochs)


def get_available_schedulers() -> Dict[str, Dict[str, Any]]:
    """è·å–æ‰€æœ‰å¯ç”¨è°ƒåº¦å™¨"""
    return SchedulerFactory.get_available_schedulers()


def requires_metric(scheduler_name: str) -> bool:
    """æ£€æŸ¥è°ƒåº¦å™¨æ˜¯å¦éœ€è¦æŒ‡æ ‡"""
    return SchedulerFactory.requires_metric(scheduler_name)
