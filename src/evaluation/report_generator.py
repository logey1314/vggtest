"""
è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆå®Œæ•´çš„æ¨¡å‹è¯„ä¼°æŠ¥å‘Š
"""

import os
import json
import datetime
import matplotlib.pyplot as plt
import numpy as np
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.utils.font_manager import setup_matplotlib_font, has_chinese_font, get_text_labels
from .metrics import ModelMetrics
from .confusion_matrix import ConfusionMatrixAnalyzer
from .error_analysis import ErrorAnalyzer


class ReportGenerator:
    """
    è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨
    æ•´åˆæ‰€æœ‰è¯„ä¼°ç»“æœï¼Œç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š
    """
    
    def __init__(self, class_names=None, model_name="VGG16"):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨

        Args:
            class_names (list): ç±»åˆ«åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ¨æ€æ¨æ–­
            model_name (str): æ¨¡å‹åç§°
        """
        self.class_names = class_names  # å…è®¸ä¸ºNoneï¼Œåœ¨ä½¿ç”¨æ—¶åŠ¨æ€æ¨æ–­
        self.num_classes = len(class_names) if class_names else None
            
        self.model_name = model_name

        # è®¾ç½®å­—ä½“
        setup_matplotlib_font()
        self.use_english = not has_chinese_font()

        # åˆå§‹åŒ–å„ä¸ªåˆ†æå™¨ï¼ˆå…è®¸class_namesä¸ºNoneï¼‰
        self.metrics_calculator = ModelMetrics(class_names)
        self.cm_analyzer = ConfusionMatrixAnalyzer(class_names)
        self.error_analyzer = ErrorAnalyzer(class_names)
    
    def generate_comprehensive_report(self, y_true, y_pred, y_prob=None, 
                                    image_paths=None, save_dir="outputs/evaluation"):
        """
        ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            y_true (array): çœŸå®æ ‡ç­¾
            y_pred (array): é¢„æµ‹æ ‡ç­¾
            y_prob (array, optional): é¢„æµ‹æ¦‚ç‡
            image_paths (list, optional): å›¾åƒè·¯å¾„åˆ—è¡¨
            save_dir (str): ä¿å­˜ç›®å½•
            
        Returns:
            dict: å®Œæ•´çš„è¯„ä¼°ç»“æœ
        """
        print("ğŸš€ å¼€å§‹ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)
        os.makedirs(os.path.join(save_dir, "reports"), exist_ok=True)
        os.makedirs(os.path.join(save_dir, "confusion_matrices"), exist_ok=True)
        os.makedirs(os.path.join(save_dir, "error_analysis"), exist_ok=True)
        
        # 1. è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        metrics_results = self.metrics_calculator.calculate_all_metrics(y_true, y_pred, y_prob)
        
        # 2. æ··æ·†çŸ©é˜µåˆ†æ
        print("ğŸ” åˆ†ææ··æ·†çŸ©é˜µ...")
        cm_save_dir = os.path.join(save_dir, "confusion_matrices")
        cm_counts, cm_recall, cm_precision = self.cm_analyzer.plot_multiple_confusion_matrices(
            y_true, y_pred, cm_save_dir
        )
        
        # æ··æ·†æ¨¡å¼åˆ†æ
        confusion_analysis = self.cm_analyzer.analyze_confusion_patterns(y_true, y_pred)
        cm_analysis_path = os.path.join(save_dir, "confusion_matrices", "confusion_analysis.png")
        self.cm_analyzer.plot_confusion_analysis(y_true, y_pred, cm_analysis_path)
        
        # 3. é”™è¯¯æ ·æœ¬åˆ†æ
        print("âŒ åˆ†æé”™è¯¯æ ·æœ¬...")
        error_info = self.error_analyzer.find_error_samples(y_true, y_pred, y_prob, image_paths)
        error_analysis = self.error_analyzer.analyze_error_patterns(error_info)
        
        # é”™è¯¯åˆ†æå¯è§†åŒ–
        error_analysis_path = os.path.join(save_dir, "error_analysis", "error_analysis.png")
        self.error_analyzer.plot_error_analysis(error_info, error_analysis_path)
        
        # é”™è¯¯æ ·æœ¬å¯è§†åŒ–ï¼ˆå¦‚æœæœ‰å›¾åƒè·¯å¾„ï¼‰
        if image_paths:
            error_samples_path = os.path.join(save_dir, "error_analysis", "error_samples.png")
            self.error_analyzer.visualize_error_samples(error_info, save_path=error_samples_path)
        
        # 4. ç”Ÿæˆæ€§èƒ½æ€»è§ˆå›¾
        print("ğŸ“ˆ ç”Ÿæˆæ€§èƒ½æ€»è§ˆ...")
        overview_path = os.path.join(save_dir, "performance_overview.png")
        self.plot_performance_overview(metrics_results, overview_path)
        
        # 5. æ•´åˆæ‰€æœ‰ç»“æœ
        comprehensive_results = {
            'model_info': {
                'model_name': self.model_name,
                'evaluation_date': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'total_samples': len(y_true),
                'class_names': self.class_names
            },
            'metrics': metrics_results,
            'confusion_analysis': confusion_analysis,
            'error_analysis': error_analysis,
            'error_info': {
                'total_errors': error_info['total_errors'],
                'error_rate': error_info['error_rate']
            }
        }
        
        # 6. ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        results_path = os.path.join(save_dir, "reports", "evaluation_results.json")
        self.save_results_to_json(comprehensive_results, results_path)
        
        # 7. ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        text_report = self.generate_text_report(comprehensive_results)
        report_path = os.path.join(save_dir, "reports", "evaluation_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(text_report)
        
        print(f"âœ… ç»¼åˆè¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ æŠ¥å‘Šä¿å­˜ä½ç½®: {save_dir}")
        print(f"ğŸ“„ ä¸»è¦æ–‡ä»¶:")
        print(f"   - è¯„ä¼°ç»“æœ: {results_path}")
        print(f"   - æ–‡æœ¬æŠ¥å‘Š: {report_path}")
        print(f"   - æ€§èƒ½æ€»è§ˆ: {overview_path}")
        
        return comprehensive_results
    
    def plot_performance_overview(self, metrics_results, save_path):
        """
        ç»˜åˆ¶æ€§èƒ½æ€»è§ˆå›¾
        
        Args:
            metrics_results (dict): è¯„ä¼°æŒ‡æ ‡ç»“æœ
            save_path (str): ä¿å­˜è·¯å¾„
        """
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

        # è·å–æ ‡ç­¾æ–‡æœ¬
        labels = get_text_labels('performance', self.use_english)

        # 1. æ•´ä½“æ€§èƒ½æŒ‡æ ‡
        overall_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
        overall_values = [metrics_results[metric] for metric in overall_metrics]

        if self.use_english:
            overall_labels = [labels.get('accuracy', 'Accuracy'),
                            labels.get('precision', 'Precision'),
                            labels.get('recall', 'Recall'),
                            labels.get('f1_score', 'F1-Score')]
        else:
            overall_labels = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1-Score']

        bars1 = ax1.bar(overall_labels, overall_values, color=['#2ECC71', '#3498DB', '#E74C3C', '#F39C12'])
        title1 = f'{self.model_name} {labels.get("overall_performance", "Overall Performance Metrics")}'
        ax1.set_title(title1, fontsize=14, fontweight='bold')
        ax1.set_ylabel(labels.get('score', 'Score'))
        ax1.set_ylim(0, 1)
        for bar, value in zip(bars1, overall_values):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

        # 2. å„ç±»åˆ«ç²¾ç¡®ç‡å¯¹æ¯”
        precision_values = [metrics_results[f'precision_{name}'] for name in self.class_names]
        bars2 = ax2.bar(self.class_names, precision_values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax2.set_title(labels.get('class_precision', 'Class Precision'), fontsize=14, fontweight='bold')
        ax2.set_ylabel(labels.get('precision', 'Precision'))
        ax2.set_ylim(0, 1)
        ax2.tick_params(axis='x', rotation=45)
        for bar, value in zip(bars2, precision_values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{value:.3f}', ha='center', va='bottom')
        
        # 3. å„ç±»åˆ«å¬å›ç‡å¯¹æ¯”
        recall_values = [metrics_results[f'recall_{name}'] for name in self.class_names]
        bars3 = ax3.bar(self.class_names, recall_values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax3.set_title(labels.get('class_recall', 'Class Recall'), fontsize=14, fontweight='bold')
        ax3.set_ylabel(labels.get('recall', 'Recall'))
        ax3.set_ylim(0, 1)
        ax3.tick_params(axis='x', rotation=45)
        for bar, value in zip(bars3, recall_values):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom')

        # 4. å„ç±»åˆ«F1-Scoreå¯¹æ¯”
        f1_values = [metrics_results[f'f1_{name}'] for name in self.class_names]
        bars4 = ax4.bar(self.class_names, f1_values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        ax4.set_title(labels.get('class_f1', 'Class F1-Score'), fontsize=14, fontweight='bold')
        ax4.set_ylabel(labels.get('f1_score', 'F1-Score'))
        ax4.set_ylim(0, 1)
        ax4.tick_params(axis='x', rotation=45)
        for bar, value in zip(bars4, f1_values):
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š æ€§èƒ½æ€»è§ˆå›¾å·²ä¿å­˜: {save_path}")
        plt.show()
    
    def save_results_to_json(self, results, save_path):
        """
        ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            results (dict): è¯„ä¼°ç»“æœ
            save_path (str): ä¿å­˜è·¯å¾„
        """
        # å¤„ç†numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {key: convert_numpy(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                return obj
        
        results_serializable = convert_numpy(results)
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°JSON: {save_path}")
    
    def generate_text_report(self, results):
        """
        ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š
        
        Args:
            results (dict): è¯„ä¼°ç»“æœ
            
        Returns:
            str: æ–‡æœ¬æŠ¥å‘Š
        """
        report = f"""
{'='*80}
ğŸ—ï¸  {results['model_info']['model_name']} æ··å‡åœŸåè½åº¦åˆ†ç±»æ¨¡å‹è¯„ä¼°æŠ¥å‘Š
{'='*80}

ğŸ“… è¯„ä¼°æ—¶é—´: {results['model_info']['evaluation_date']}
ğŸ“Š æ€»æ ·æœ¬æ•°: {results['model_info']['total_samples']}
ğŸ·ï¸  åˆ†ç±»ç±»åˆ«: {', '.join(results['model_info']['class_names'])}

{'='*80}
ğŸ“ˆ æ•´ä½“æ€§èƒ½æŒ‡æ ‡
{'='*80}

å‡†ç¡®ç‡ (Accuracy):           {results['metrics']['accuracy']:.4f}
å®å¹³å‡ç²¾ç¡®ç‡ (Precision):     {results['metrics']['precision_macro']:.4f}
å®å¹³å‡å¬å›ç‡ (Recall):        {results['metrics']['recall_macro']:.4f}
å®å¹³å‡F1-Score:              {results['metrics']['f1_macro']:.4f}

åŠ æƒå¹³å‡ç²¾ç¡®ç‡:               {results['metrics']['precision_weighted']:.4f}
åŠ æƒå¹³å‡å¬å›ç‡:               {results['metrics']['recall_weighted']:.4f}
åŠ æƒå¹³å‡F1-Score:            {results['metrics']['f1_weighted']:.4f}"""

        if 'top2_accuracy' in results['metrics']:
            report += f"\nTop-2å‡†ç¡®ç‡:                 {results['metrics']['top2_accuracy']:.4f}"

        report += f"""

{'='*80}
ğŸ¯ å„ç±»åˆ«è¯¦ç»†æ€§èƒ½
{'='*80}
"""

        for class_name in self.class_names:
            precision = results['metrics'][f'precision_{class_name}']
            recall = results['metrics'][f'recall_{class_name}']
            f1 = results['metrics'][f'f1_{class_name}']
            
            report += f"""
{class_name}:
  ç²¾ç¡®ç‡: {precision:.4f}
  å¬å›ç‡: {recall:.4f}
  F1-Score: {f1:.4f}"""

        report += f"""

{'='*80}
ğŸ” æ··æ·†åˆ†æ
{'='*80}

æ€»æ ·æœ¬æ•°: {results['confusion_analysis']['total_samples']}
æ­£ç¡®é¢„æµ‹: {results['confusion_analysis']['correct_predictions']}
æ•´ä½“å‡†ç¡®ç‡: {results['confusion_analysis']['accuracy']:.4f}

ç›¸é‚»ç±»åˆ«æ··æ·†ç‡: {results['metrics']['adjacent_confusion_rate']:.4f}
ä¸¥é‡é”™è¯¯ç‡: {results['metrics']['severe_error_rate']:.4f}"""

        if 'most_confused_pair' in results['confusion_analysis']:
            pair = results['confusion_analysis']['most_confused_pair']
            count = results['confusion_analysis']['most_confused_count']
            report += f"\næœ€æ˜“æ··æ·†ç±»åˆ«å¯¹: {pair[0]} â†” {pair[1]} ({count}æ¬¡)"

        report += f"""

{'='*80}
âŒ é”™è¯¯æ ·æœ¬åˆ†æ
{'='*80}

æ€»é”™è¯¯æ•°: {results['error_info']['total_errors']}
é”™è¯¯ç‡: {results['error_info']['error_rate']:.4f}"""

        if results['error_info']['total_errors'] > 0:
            report += f"""
ç›¸é‚»ç±»åˆ«é”™è¯¯: {results['error_analysis']['adjacent_errors']}
ä¸¥é‡é”™è¯¯: {results['error_analysis']['severe_errors']}"""

            if 'mean_error_confidence' in results['error_analysis']:
                report += f"""
é”™è¯¯æ ·æœ¬å¹³å‡ç½®ä¿¡åº¦: {results['error_analysis']['mean_error_confidence']:.4f}"""

        report += f"""

{'='*80}
ğŸ“‹ åˆ†ç±»æŠ¥å‘Š
{'='*80}

{results['metrics']['classification_report']}

{'='*80}
âœ… è¯„ä¼°å®Œæˆ
{'='*80}
"""

        return report
