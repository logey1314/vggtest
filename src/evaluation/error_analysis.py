"""
é”™è¯¯æ ·æœ¬åˆ†ææ¨¡å—
åˆ†æå’Œå¯è§†åŒ–æ¨¡å‹çš„é”™è¯¯é¢„æµ‹æ ·æœ¬
"""

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
import torch
import torch.nn.functional as F
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.utils.font_manager import setup_matplotlib_font, has_chinese_font, get_text_labels


class ErrorAnalyzer:
    """
    é”™è¯¯æ ·æœ¬åˆ†æå™¨
    æä¾›é”™è¯¯æ ·æœ¬çš„ç»Ÿè®¡ã€åˆ†æå’Œå¯è§†åŒ–åŠŸèƒ½
    """
    
    def __init__(self, class_names=None):
        """
        åˆå§‹åŒ–é”™è¯¯æ ·æœ¬åˆ†æå™¨

        Args:
            class_names (list): ç±»åˆ«åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ¨æ€æ¨æ–­
        """
        self.class_names = class_names  # å…è®¸ä¸ºNoneï¼Œåœ¨ä½¿ç”¨æ—¶åŠ¨æ€æ¨æ–­
        self.num_classes = len(class_names) if class_names else None

        # è®¾ç½®å­—ä½“
        setup_matplotlib_font()
        self.use_english = not has_chinese_font()

    def _ensure_class_names(self, y_true=None):
        """
        ç¡®ä¿ç±»åˆ«åç§°å·²è®¾ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ ¹æ®æ•°æ®åŠ¨æ€ç”Ÿæˆ

        Args:
            y_true (array): çœŸå®æ ‡ç­¾ï¼Œç”¨äºæ¨æ–­ç±»åˆ«æ•°é‡
        """
        if self.class_names is None:
            if y_true is not None:
                unique_labels = sorted(set(y_true))
                self.num_classes = len(unique_labels)
                self.class_names = [f"Class_{i}" for i in unique_labels]
            else:
                # å¦‚æœæ— æ³•æ¨æ–­ï¼Œä½¿ç”¨é»˜è®¤çš„3åˆ†ç±»
                self.num_classes = 3
                self.class_names = ["Class_0", "Class_1", "Class_2"]
    
    def find_error_samples(self, y_true, y_pred, y_prob=None, image_paths=None):
        """
        æ‰¾å‡ºæ‰€æœ‰é”™è¯¯é¢„æµ‹çš„æ ·æœ¬

        Args:
            y_true (array): çœŸå®æ ‡ç­¾
            y_pred (array): é¢„æµ‹æ ‡ç­¾
            y_prob (array, optional): é¢„æµ‹æ¦‚ç‡
            image_paths (list, optional): å›¾åƒè·¯å¾„åˆ—è¡¨

        Returns:
            dict: é”™è¯¯æ ·æœ¬ä¿¡æ¯
        """
        # ç¡®ä¿ç±»åˆ«åç§°å·²è®¾ç½®
        self._ensure_class_names(y_true)
        # æ‰¾å‡ºé”™è¯¯é¢„æµ‹çš„ç´¢å¼•
        error_indices = np.where(y_true != y_pred)[0]
        
        error_info = {
            'total_errors': len(error_indices),
            'error_rate': len(error_indices) / len(y_true),
            'error_indices': error_indices,
            'true_labels': y_true[error_indices],
            'pred_labels': y_pred[error_indices]
        }
        
        if y_prob is not None:
            error_info['pred_probs'] = y_prob[error_indices]
            error_info['confidence'] = np.max(y_prob[error_indices], axis=1)
        
        if image_paths is not None:
            error_info['image_paths'] = [image_paths[i] for i in error_indices]
        
        return error_info
    
    def categorize_errors(self, error_info):
        """
        å¯¹é”™è¯¯è¿›è¡Œåˆ†ç±»
        
        Args:
            error_info (dict): é”™è¯¯æ ·æœ¬ä¿¡æ¯
            
        Returns:
            dict: åˆ†ç±»åçš„é”™è¯¯ä¿¡æ¯
        """
        true_labels = error_info['true_labels']
        pred_labels = error_info['pred_labels']
        
        categories = {
            'adjacent_errors': [],      # ç›¸é‚»ç±»åˆ«é”™è¯¯
            'severe_errors': [],        # è·¨ç±»åˆ«ä¸¥é‡é”™è¯¯
            'high_confidence_errors': [], # é«˜ç½®ä¿¡åº¦é”™è¯¯
            'low_confidence_errors': []   # ä½ç½®ä¿¡åº¦é”™è¯¯
        }
        
        for i, (true_label, pred_label) in enumerate(zip(true_labels, pred_labels)):
            # ç›¸é‚»ç±»åˆ«é”™è¯¯ vs ä¸¥é‡é”™è¯¯
            # æ”¹è¿›ï¼šæ”¯æŒåŠ¨æ€ç±»åˆ«æ•°é‡çš„ç›¸é‚»åˆ¤æ–­
            label_diff = abs(true_label - pred_label)
            if label_diff == 1:
                categories['adjacent_errors'].append(i)
            elif label_diff > 1:
                categories['severe_errors'].append(i)
            # å¦‚æœlabel_diff == 0ï¼Œè¯´æ˜é¢„æµ‹æ­£ç¡®ï¼Œä¸åº”è¯¥åœ¨é”™è¯¯åˆ—è¡¨ä¸­
            
            # é«˜ä½ç½®ä¿¡åº¦é”™è¯¯ï¼ˆå¦‚æœæœ‰æ¦‚ç‡ä¿¡æ¯ï¼‰
            if 'confidence' in error_info:
                confidence = error_info['confidence'][i]
                if confidence > 0.8:
                    categories['high_confidence_errors'].append(i)
                else:
                    categories['low_confidence_errors'].append(i)
        
        return categories
    
    def analyze_error_patterns(self, error_info):
        """
        åˆ†æé”™è¯¯æ¨¡å¼

        Args:
            error_info (dict): é”™è¯¯æ ·æœ¬ä¿¡æ¯

        Returns:
            dict: é”™è¯¯æ¨¡å¼åˆ†æç»“æœ
        """
        if error_info['total_errors'] == 0:
            return {'message': 'æ²¡æœ‰é”™è¯¯æ ·æœ¬'}

        true_labels = error_info['true_labels']
        pred_labels = error_info['pred_labels']

        # ç¡®ä¿ç±»åˆ«åç§°å·²è®¾ç½®
        self._ensure_class_names(true_labels)
        
        analysis = {
            'total_errors': error_info['total_errors'],
            'error_rate': error_info['error_rate']
        }
        
        # å„ç±»åˆ«çš„é”™è¯¯åˆ†æ
        for class_idx, class_name in enumerate(self.class_names):
            class_errors = np.sum(true_labels == class_idx)
            analysis[f'{class_name}_errors'] = class_errors
            
            if class_errors > 0:
                # è¯¥ç±»åˆ«æœ€å¸¸è¢«è¯¯åˆ†ä¸ºå“ªä¸ªç±»åˆ«
                class_error_mask = (true_labels == class_idx)
                misclassified_as = pred_labels[class_error_mask]
                if len(misclassified_as) > 0:
                    unique, counts = np.unique(misclassified_as, return_counts=True)
                    most_common_idx = unique[np.argmax(counts)]
                    # ç¡®ä¿ç´¢å¼•æ˜¯æ ‡é‡å¹¶ä¸”åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    most_common_idx = int(most_common_idx)
                    if 0 <= most_common_idx < len(self.class_names):
                        analysis[f'{class_name}_most_misclassified_as'] = self.class_names[most_common_idx]
                        analysis[f'{class_name}_most_misclassified_count'] = int(np.max(counts))
                    else:
                        analysis[f'{class_name}_most_misclassified_as'] = f"Unknown_Class_{most_common_idx}"
                        analysis[f'{class_name}_most_misclassified_count'] = int(np.max(counts))
        
        # é”™è¯¯ç±»å‹ç»Ÿè®¡
        categories = self.categorize_errors(error_info)
        analysis['adjacent_errors'] = len(categories['adjacent_errors'])
        analysis['severe_errors'] = len(categories['severe_errors'])
        analysis['adjacent_error_rate'] = len(categories['adjacent_errors']) / error_info['total_errors']
        analysis['severe_error_rate'] = len(categories['severe_errors']) / error_info['total_errors']
        
        # ç½®ä¿¡åº¦åˆ†æï¼ˆå¦‚æœæœ‰ï¼‰
        if 'confidence' in error_info:
            confidence = error_info['confidence']
            analysis['mean_error_confidence'] = np.mean(confidence)
            analysis['std_error_confidence'] = np.std(confidence)
            analysis['high_confidence_errors'] = len(categories['high_confidence_errors'])
            analysis['low_confidence_errors'] = len(categories['low_confidence_errors'])
        
        return analysis
    
    def plot_error_analysis(self, error_info, save_path=None):
        """
        ç»˜åˆ¶é”™è¯¯åˆ†æå›¾è¡¨
        
        Args:
            error_info (dict): é”™è¯¯æ ·æœ¬ä¿¡æ¯
            save_path (str): ä¿å­˜è·¯å¾„
        """
        if error_info['total_errors'] == 0:
            print("æ²¡æœ‰é”™è¯¯æ ·æœ¬ï¼Œæ— éœ€ç»˜åˆ¶åˆ†æå›¾è¡¨")
            return
        
        analysis = self.analyze_error_patterns(error_info)
        categories = self.categorize_errors(error_info)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # è·å–æ ‡ç­¾æ–‡æœ¬
        labels = get_text_labels('error_analysis', self.use_english)

        # 1. å„ç±»åˆ«é”™è¯¯æ•°é‡
        class_errors = [analysis.get(f'{name}_errors', 0) for name in self.class_names]
        bars1 = axes[0, 0].bar(self.class_names, class_errors, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[0, 0].set_title(labels.get('class_errors', 'Class Error Count'), fontsize=14, fontweight='bold')
        axes[0, 0].set_ylabel('Error Count' if self.use_english else 'é”™è¯¯æ•°é‡')
        for bar, count in zip(bars1, class_errors):
            if count > 0:
                axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(class_errors)*0.01, 
                               f'{count}', ha='center', va='bottom')
        
        # 2. é”™è¯¯ç±»å‹åˆ†å¸ƒ
        if self.use_english:
            error_types = [labels.get('adjacent_errors', 'Adjacent Errors'),
                          labels.get('severe_errors', 'Severe Errors')]
        else:
            error_types = ['ç›¸é‚»ç±»åˆ«é”™è¯¯', 'ä¸¥é‡é”™è¯¯']
        error_counts = [analysis['adjacent_errors'], analysis['severe_errors']]
        colors = ['#F39C12', '#E74C3C']

        wedges, texts, autotexts = axes[0, 1].pie(error_counts, labels=error_types,
                                                 colors=colors, autopct='%1.1f%%', startangle=90)
        axes[0, 1].set_title(labels.get('error_types', 'Error Type Distribution'), fontsize=14, fontweight='bold')

        # 3. ç½®ä¿¡åº¦åˆ†å¸ƒï¼ˆå¦‚æœæœ‰ï¼‰
        if 'confidence' in error_info:
            confidence = error_info['confidence']
            axes[1, 0].hist(confidence, bins=20, color='#9B59B6', alpha=0.7, edgecolor='black')
            mean_label = f'{labels.get("mean", "Mean")}: {np.mean(confidence):.3f}'
            axes[1, 0].axvline(np.mean(confidence), color='red', linestyle='--', label=mean_label)
            axes[1, 0].set_title(labels.get('confidence_dist', 'Error Sample Confidence Distribution'), fontsize=14, fontweight='bold')
            axes[1, 0].set_xlabel(labels.get('confidence', 'Confidence'))
            axes[1, 0].set_ylabel(labels.get('frequency', 'Frequency'))
            axes[1, 0].legend()
        else:
            no_conf_text = labels.get('no_confidence', 'No Confidence Info')
            axes[1, 0].text(0.5, 0.5, no_conf_text, ha='center', va='center',
                           transform=axes[1, 0].transAxes, fontsize=16)
            axes[1, 0].set_title(labels.get('confidence_dist', 'Confidence Distribution'), fontsize=14, fontweight='bold')
        
        # 4. é”™è¯¯è½¬ç§»çŸ©é˜µï¼ˆçœŸå®->é¢„æµ‹ï¼‰
        true_labels = error_info['true_labels']
        pred_labels = error_info['pred_labels']
        
        # åˆ›å»ºé”™è¯¯è½¬ç§»çŸ©é˜µ
        error_matrix = np.zeros((len(self.class_names), len(self.class_names)))
        for true_label, pred_label in zip(true_labels, pred_labels):
            error_matrix[true_label, pred_label] += 1
        
        im = axes[1, 1].imshow(error_matrix, cmap='Reds', aspect='auto')
        axes[1, 1].set_title('é”™è¯¯è½¬ç§»çŸ©é˜µ', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[1, 1].set_ylabel('çœŸå®æ ‡ç­¾')
        axes[1, 1].set_xticks(range(len(self.class_names)))
        axes[1, 1].set_yticks(range(len(self.class_names)))
        axes[1, 1].set_xticklabels(self.class_names, rotation=45)
        axes[1, 1].set_yticklabels(self.class_names)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(len(self.class_names)):
            for j in range(len(self.class_names)):
                if error_matrix[i, j] > 0:
                    axes[1, 1].text(j, i, f'{int(error_matrix[i, j])}', 
                                   ha='center', va='center', color='white', fontweight='bold')
        
        plt.colorbar(im, ax=axes[1, 1])
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š é”™è¯¯åˆ†æå›¾è¡¨å·²ä¿å­˜: {save_path}")
        
        plt.show()
        
        return analysis
    
    def visualize_error_samples(self, error_info, max_samples=16, save_path=None):
        """
        å¯è§†åŒ–é”™è¯¯æ ·æœ¬å›¾åƒ
        
        Args:
            error_info (dict): é”™è¯¯æ ·æœ¬ä¿¡æ¯
            max_samples (int): æœ€å¤§æ˜¾ç¤ºæ ·æœ¬æ•°
            save_path (str): ä¿å­˜è·¯å¾„
        """
        if 'image_paths' not in error_info:
            print("æ²¡æœ‰å›¾åƒè·¯å¾„ä¿¡æ¯ï¼Œæ— æ³•å¯è§†åŒ–é”™è¯¯æ ·æœ¬")
            return
        
        if error_info['total_errors'] == 0:
            print("æ²¡æœ‰é”™è¯¯æ ·æœ¬")
            return
        
        # é€‰æ‹©è¦æ˜¾ç¤ºçš„æ ·æœ¬
        num_samples = min(max_samples, error_info['total_errors'])
        sample_indices = np.random.choice(error_info['total_errors'], num_samples, replace=False)
        
        # è®¡ç®—ç½‘æ ¼å¤§å°
        cols = 4
        rows = (num_samples + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))
        if rows == 1:
            axes = axes.reshape(1, -1)
        
        for idx, sample_idx in enumerate(sample_indices):
            row = idx // cols
            col = idx % cols
            
            # åŠ è½½å›¾åƒ
            img_path = error_info['image_paths'][sample_idx]
            try:
                img = Image.open(img_path).convert('RGB')
                axes[row, col].imshow(img)
                axes[row, col].axis('off')
                
                # æ·»åŠ æ ‡é¢˜ä¿¡æ¯
                true_label = error_info['true_labels'][sample_idx]
                pred_label = error_info['pred_labels'][sample_idx]
                
                title = f"çœŸå®: {self.class_names[true_label]}\né¢„æµ‹: {self.class_names[pred_label]}"
                
                if 'confidence' in error_info:
                    confidence = error_info['confidence'][sample_idx]
                    title += f"\nç½®ä¿¡åº¦: {confidence:.3f}"
                
                axes[row, col].set_title(title, fontsize=10, pad=10)
                
            except Exception as e:
                axes[row, col].text(0.5, 0.5, f"æ— æ³•åŠ è½½å›¾åƒ\n{str(e)}", 
                                   ha='center', va='center', transform=axes[row, col].transAxes)
                axes[row, col].set_title(f"é”™è¯¯æ ·æœ¬ {sample_idx}", fontsize=10)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(num_samples, rows * cols):
            row = idx // cols
            col = idx % cols
            axes[row, col].axis('off')
        
        plt.suptitle(f'é”™è¯¯æ ·æœ¬å¯è§†åŒ– (å…±{error_info["total_errors"]}ä¸ªé”™è¯¯ï¼Œæ˜¾ç¤º{num_samples}ä¸ª)', 
                     fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š é”™è¯¯æ ·æœ¬å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def generate_error_report(self, error_info):
        """
        ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Š
        
        Args:
            error_info (dict): é”™è¯¯æ ·æœ¬ä¿¡æ¯
            
        Returns:
            str: é”™è¯¯åˆ†ææŠ¥å‘Š
        """
        if error_info['total_errors'] == 0:
            return "ğŸ‰ æ¨¡å‹é¢„æµ‹å®Œå…¨æ­£ç¡®ï¼Œæ²¡æœ‰é”™è¯¯æ ·æœ¬ï¼"
        
        analysis = self.analyze_error_patterns(error_info)
        
        report = f"""
ğŸ“Š é”™è¯¯æ ·æœ¬åˆ†ææŠ¥å‘Š
{'='*50}

ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:
  æ€»é”™è¯¯æ•°: {analysis['total_errors']}
  é”™è¯¯ç‡: {analysis['error_rate']:.2%}

ğŸ” é”™è¯¯ç±»å‹åˆ†æ:
  ç›¸é‚»ç±»åˆ«é”™è¯¯: {analysis['adjacent_errors']} ({analysis['adjacent_error_rate']:.1%})
  ä¸¥é‡é”™è¯¯: {analysis['severe_errors']} ({analysis['severe_error_rate']:.1%})

ğŸ“‹ å„ç±»åˆ«é”™è¯¯è¯¦æƒ…:"""
        
        for class_name in self.class_names:
            errors = analysis.get(f'{class_name}_errors', 0)
            if errors > 0:
                most_confused = analysis.get(f'{class_name}_most_misclassified_as', 'æœªçŸ¥')
                confused_count = analysis.get(f'{class_name}_most_misclassified_count', 0)
                report += f"""
  {class_name}:
    é”™è¯¯æ•°: {errors}
    æœ€å¸¸è¯¯åˆ†ä¸º: {most_confused} ({confused_count}æ¬¡)"""
        
        if 'mean_error_confidence' in analysis:
            report += f"""

ğŸ¯ ç½®ä¿¡åº¦åˆ†æ:
  é”™è¯¯æ ·æœ¬å¹³å‡ç½®ä¿¡åº¦: {analysis['mean_error_confidence']:.3f}
  ç½®ä¿¡åº¦æ ‡å‡†å·®: {analysis['std_error_confidence']:.3f}
  é«˜ç½®ä¿¡åº¦é”™è¯¯: {analysis['high_confidence_errors']}
  ä½ç½®ä¿¡åº¦é”™è¯¯: {analysis['low_confidence_errors']}"""
        
        return report
