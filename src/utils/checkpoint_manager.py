"""
æ£€æŸ¥ç‚¹ç®¡ç†å·¥å…·
ç”¨äºè®­ç»ƒæ¢å¤åŠŸèƒ½çš„æ£€æŸ¥ç‚¹æ–‡ä»¶ç®¡ç†
"""

import os
import torch
import glob
import datetime
from typing import Optional, Dict, Any, Tuple, List


class CheckpointManager:
    """
    æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    è´Ÿè´£æ£€æŸ¥ç‚¹æ–‡ä»¶çš„æŸ¥æ‰¾ã€éªŒè¯ã€åŠ è½½å’Œä¿å­˜
    """
    
    def __init__(self, project_root: str):
        """
        åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        
        Args:
            project_root (str): é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        """
        self.project_root = project_root
        self.checkpoint_base_dir = os.path.join(project_root, 'models', 'checkpoints')
        self.latest_dir = os.path.join(self.checkpoint_base_dir, 'latest')
    
    def find_latest_checkpoint(self, train_dir: Optional[str] = None) -> Optional[str]:
        """
        æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
        
        Args:
            train_dir (str, optional): æŒ‡å®šè®­ç»ƒç›®å½•åç§°
            
        Returns:
            str: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å›None
        """
        if train_dir:
            # æŸ¥æ‰¾æŒ‡å®šè®­ç»ƒç›®å½•ä¸­çš„æ£€æŸ¥ç‚¹
            target_dir = os.path.join(self.checkpoint_base_dir, train_dir)
            if not os.path.exists(target_dir):
                print(f"âŒ æŒ‡å®šçš„è®­ç»ƒç›®å½•ä¸å­˜åœ¨: {train_dir}")
                return None
            
            checkpoint_files = glob.glob(os.path.join(target_dir, "checkpoint_epoch_*.pth"))
            if not checkpoint_files:
                print(f"âŒ åœ¨ç›®å½• {train_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
                return None
            
            # æŒ‰epochæ•°æ’åºï¼Œè¿”å›æœ€æ–°çš„
            checkpoint_files.sort(key=lambda x: int(x.split('_epoch_')[1].split('.')[0]))
            latest_checkpoint = checkpoint_files[-1]
            print(f"âœ… åœ¨æŒ‡å®šç›®å½•ä¸­æ‰¾åˆ°æ£€æŸ¥ç‚¹: {os.path.basename(latest_checkpoint)}")
            return latest_checkpoint
        
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
            # 1. é¦–å…ˆæ£€æŸ¥latestç›®å½•
            latest_checkpoints = glob.glob(os.path.join(self.latest_dir, "checkpoint_epoch_*.pth"))
            if latest_checkpoints:
                latest_checkpoints.sort(key=lambda x: int(x.split('_epoch_')[1].split('.')[0]))
                latest_checkpoint = latest_checkpoints[-1]
                print(f"âœ… åœ¨latestç›®å½•ä¸­æ‰¾åˆ°æ£€æŸ¥ç‚¹: {os.path.basename(latest_checkpoint)}")
                return latest_checkpoint
            
            # 2. å¦‚æœlatestç›®å½•æ²¡æœ‰ï¼ŒæŸ¥æ‰¾æ‰€æœ‰è®­ç»ƒç›®å½•
            train_dirs = [d for d in os.listdir(self.checkpoint_base_dir) 
                         if d.startswith('train_') and os.path.isdir(os.path.join(self.checkpoint_base_dir, d))]
            
            if not train_dirs:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®­ç»ƒç›®å½•")
                return None
            
            # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œæ‰¾æœ€æ–°çš„
            train_dirs.sort(reverse=True)
            
            for train_dir in train_dirs:
                train_path = os.path.join(self.checkpoint_base_dir, train_dir)
                checkpoint_files = glob.glob(os.path.join(train_path, "checkpoint_epoch_*.pth"))
                
                if checkpoint_files:
                    checkpoint_files.sort(key=lambda x: int(x.split('_epoch_')[1].split('.')[0]))
                    latest_checkpoint = checkpoint_files[-1]
                    print(f"âœ… åœ¨è®­ç»ƒç›®å½• {train_dir} ä¸­æ‰¾åˆ°æ£€æŸ¥ç‚¹: {os.path.basename(latest_checkpoint)}")
                    return latest_checkpoint
            
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ£€æŸ¥ç‚¹æ–‡ä»¶")
            return None
    
    def validate_checkpoint(self, checkpoint_path: str) -> bool:
        """
        éªŒè¯æ£€æŸ¥ç‚¹æ–‡ä»¶çš„å®Œæ•´æ€§
        
        Args:
            checkpoint_path (str): æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: éªŒè¯æ˜¯å¦é€šè¿‡
        """
        if not os.path.exists(checkpoint_path):
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return False
        
        try:
            # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # æ£€æŸ¥å¿…éœ€çš„é”®
            required_keys = ['epoch', 'model_state_dict', 'optimizer_state_dict', 'best_acc']
            missing_keys = [key for key in required_keys if key not in checkpoint]
            
            if missing_keys:
                print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„é”®: {missing_keys}")
                return False
            
            # æ£€æŸ¥epochæ˜¯å¦ä¸ºæ­£æ•´æ•°
            if not isinstance(checkpoint['epoch'], int) or checkpoint['epoch'] <= 0:
                print(f"âŒ æ£€æŸ¥ç‚¹ä¸­çš„epochå€¼æ— æ•ˆ: {checkpoint['epoch']}")
                return False
            
            print(f"âœ… æ£€æŸ¥ç‚¹æ–‡ä»¶éªŒè¯é€šè¿‡")
            print(f"   ğŸ“Š Epoch: {checkpoint['epoch']}")
            print(f"   ğŸ¯ æœ€ä½³ç²¾åº¦: {checkpoint.get('best_acc', 'N/A'):.2f}%")
            print(f"   ğŸ“ˆ éªŒè¯ç²¾åº¦: {checkpoint.get('val_acc', 'N/A'):.2f}%")
            
            return True

        except Exception as e:
            print(f"âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
            return False

    def load_checkpoint(self, checkpoint_path: str, model, optimizer, scheduler=None) -> Tuple[int, float]:
        """
        åŠ è½½æ£€æŸ¥ç‚¹å¹¶æ¢å¤è®­ç»ƒçŠ¶æ€

        Args:
            checkpoint_path (str): æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            model: PyTorchæ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            Tuple[int, float]: (èµ·å§‹epoch, æœ€ä½³ç²¾åº¦)
        """
        print(f"ğŸ“¥ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")

        # éªŒè¯æ£€æŸ¥ç‚¹
        if not self.validate_checkpoint(checkpoint_path):
            raise ValueError("æ£€æŸ¥ç‚¹æ–‡ä»¶éªŒè¯å¤±è´¥")

        # åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®
        checkpoint = torch.load(checkpoint_path, map_location='cpu')

        # æ¢å¤æ¨¡å‹çŠ¶æ€
        model.load_state_dict(checkpoint['model_state_dict'])
        print("âœ… æ¨¡å‹çŠ¶æ€å·²æ¢å¤")

        # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²æ¢å¤")

        # æ¢å¤å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if scheduler is not None and 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å·²æ¢å¤")
        elif scheduler is not None:
            print("âš ï¸  æ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€ï¼Œå°†ä½¿ç”¨é»˜è®¤çŠ¶æ€")

        start_epoch = checkpoint['epoch']
        best_acc = checkpoint['best_acc']

        print(f"ğŸš€ è®­ç»ƒå°†ä»ç¬¬ {start_epoch + 1} è½®å¼€å§‹")
        print(f"ğŸ¯ å½“å‰æœ€ä½³ç²¾åº¦: {best_acc:.2f}%")

        return start_epoch, best_acc

    def list_available_checkpoints(self) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ£€æŸ¥ç‚¹æ–‡ä»¶

        Returns:
            List[Dict]: æ£€æŸ¥ç‚¹ä¿¡æ¯åˆ—è¡¨
        """
        checkpoints = []

        # æ‰«ææ‰€æœ‰è®­ç»ƒç›®å½•
        if not os.path.exists(self.checkpoint_base_dir):
            return checkpoints

        train_dirs = [d for d in os.listdir(self.checkpoint_base_dir)
                     if d.startswith('train_') and os.path.isdir(os.path.join(self.checkpoint_base_dir, d))]

        # ä¹ŸåŒ…æ‹¬latestç›®å½•
        if os.path.exists(self.latest_dir):
            train_dirs.append('latest')

        for train_dir in train_dirs:
            train_path = os.path.join(self.checkpoint_base_dir, train_dir)
            checkpoint_files = glob.glob(os.path.join(train_path, "checkpoint_epoch_*.pth"))

            for checkpoint_file in checkpoint_files:
                try:
                    checkpoint = torch.load(checkpoint_file, map_location='cpu')

                    # æå–epochæ•°
                    epoch = checkpoint.get('epoch', 0)

                    # è·å–æ–‡ä»¶ä¿¡æ¯
                    file_stat = os.stat(checkpoint_file)
                    file_time = datetime.datetime.fromtimestamp(file_stat.st_mtime)

                    checkpoint_info = {
                        'path': checkpoint_file,
                        'train_dir': train_dir,
                        'epoch': epoch,
                        'best_acc': checkpoint.get('best_acc', 0.0),
                        'val_acc': checkpoint.get('val_acc', 0.0),
                        'file_time': file_time,
                        'file_size': file_stat.st_size
                    }

                    checkpoints.append(checkpoint_info)

                except Exception as e:
                    print(f"âš ï¸  æ— æ³•è¯»å–æ£€æŸ¥ç‚¹ {checkpoint_file}: {e}")

        # æŒ‰æ—¶é—´æ’åº
        checkpoints.sort(key=lambda x: x['file_time'], reverse=True)

        return checkpoints

    def print_checkpoint_summary(self):
        """
        æ‰“å°æ£€æŸ¥ç‚¹æ‘˜è¦ä¿¡æ¯
        """
        checkpoints = self.list_available_checkpoints()

        if not checkpoints:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ£€æŸ¥ç‚¹æ–‡ä»¶")
            return

        print(f"\nğŸ“‹ å¯ç”¨æ£€æŸ¥ç‚¹æ‘˜è¦ (å…± {len(checkpoints)} ä¸ª):")
        print("=" * 80)
        print(f"{'åºå·':<4} {'è®­ç»ƒç›®å½•':<20} {'Epoch':<8} {'æœ€ä½³ç²¾åº¦':<10} {'éªŒè¯ç²¾åº¦':<10} {'æ–‡ä»¶æ—¶é—´':<20}")
        print("-" * 80)

        for i, checkpoint in enumerate(checkpoints[:10], 1):  # åªæ˜¾ç¤ºæœ€è¿‘10ä¸ª
            train_dir = checkpoint['train_dir']
            epoch = checkpoint['epoch']
            best_acc = checkpoint['best_acc']
            val_acc = checkpoint['val_acc']
            file_time = checkpoint['file_time'].strftime('%Y-%m-%d %H:%M:%S')

            print(f"{i:<4} {train_dir:<20} {epoch:<8} {best_acc:<10.2f} {val_acc:<10.2f} {file_time:<20}")

        if len(checkpoints) > 10:
            print(f"... è¿˜æœ‰ {len(checkpoints) - 10} ä¸ªæ£€æŸ¥ç‚¹æœªæ˜¾ç¤º")

        print("=" * 80)
