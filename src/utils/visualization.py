"""
è®­ç»ƒå¯è§†åŒ–å·¥å…·æ¨¡å— - é‡æ–°è®¾è®¡ç‰ˆæœ¬
æ”¯æŒè‡ªåŠ¨ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒåˆ†æå›¾è¡¨
"""
import matplotlib.pyplot as plt
import numpy as np
import os
import torch
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, classification_report,
    roc_curve, auc, precision_recall_curve,
    average_precision_score
)
from sklearn.preprocessing import label_binarize
import matplotlib.font_manager as fm
from .font_manager import setup_matplotlib_font, has_chinese_font, get_text_labels

# åˆå§‹åŒ–å­—ä½“è®¾ç½®
setup_matplotlib_font()

def plot_enhanced_training_history(train_losses, val_losses, train_accuracies, val_accuracies, save_path, use_english=False):
    """
    ç»˜åˆ¶å¢å¼ºç‰ˆè®­ç»ƒå†å²å›¾è¡¨ - 2x2å­å›¾å¸ƒå±€

    Args:
        train_losses (list): è®­ç»ƒæŸå¤±åˆ—è¡¨
        val_losses (list): éªŒè¯æŸå¤±åˆ—è¡¨
        train_accuracies (list): è®­ç»ƒå‡†ç¡®ç‡åˆ—è¡¨
        val_accuracies (list): éªŒè¯å‡†ç¡®ç‡åˆ—è¡¨
        save_path (str): ä¿å­˜è·¯å¾„
        use_english (bool): æ˜¯å¦ä½¿ç”¨è‹±æ–‡æ ‡é¢˜ï¼ˆè§£å†³ä¸­æ–‡å­—ä½“é—®é¢˜ï¼‰
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    epochs = range(1, len(train_losses) + 1)

    # è®¾ç½®æ ‡é¢˜æ–‡æœ¬
    if use_english:
        titles = {
            'loss': 'Training Loss vs Validation Loss',
            'acc': 'Training Accuracy vs Validation Accuracy',
            'val_acc': 'Validation Accuracy Progress',
            'gap': 'Overfitting Analysis (Train Acc - Val Acc)',
            'train_loss': 'Training Loss',
            'val_loss': 'Validation Loss',
            'train_acc': 'Training Accuracy',
            'val_acc': 'Validation Accuracy',
            'best': 'Best',
            'gap_label': 'Accuracy Gap (%)'
        }
    else:
        titles = {
            'loss': 'è®­ç»ƒæŸå¤± vs éªŒè¯æŸå¤±',
            'acc': 'è®­ç»ƒå‡†ç¡®ç‡ vs éªŒè¯å‡†ç¡®ç‡',
            'val_acc': 'éªŒè¯å‡†ç¡®ç‡å˜åŒ–',
            'gap': 'è¿‡æ‹Ÿåˆåˆ†æ (è®­ç»ƒå‡†ç¡®ç‡ - éªŒè¯å‡†ç¡®ç‡)',
            'train_loss': 'è®­ç»ƒæŸå¤±',
            'val_loss': 'éªŒè¯æŸå¤±',
            'train_acc': 'è®­ç»ƒå‡†ç¡®ç‡',
            'val_acc': 'éªŒè¯å‡†ç¡®ç‡',
            'best': 'æœ€ä½³',
            'gap_label': 'å‡†ç¡®ç‡å·®è· (%)'
        }

    # 1. æŸå¤±æ›²çº¿å¯¹æ¯”
    ax1.plot(epochs, train_losses, 'b-', label=titles['train_loss'], linewidth=2)
    ax1.plot(epochs, val_losses, 'r-', label=titles['val_loss'], linewidth=2)
    ax1.set_title(titles['loss'], fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. å‡†ç¡®ç‡æ›²çº¿å¯¹æ¯”
    ax2.plot(epochs, train_accuracies, 'b-', label=titles['train_acc'], linewidth=2)
    ax2.plot(epochs, val_accuracies, 'r-', label=titles['val_acc'], linewidth=2)
    ax2.set_title(titles['acc'], fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. éªŒè¯å‡†ç¡®ç‡è¯¦ç»†è§†å›¾ï¼ˆå¸¦æœ€ä½³ç‚¹æ ‡è®°ï¼‰
    ax3.plot(epochs, val_accuracies, 'g-', label=titles['val_acc'], linewidth=2)
    best_acc_idx = np.argmax(val_accuracies)
    best_acc = val_accuracies[best_acc_idx]
    ax3.plot(best_acc_idx + 1, best_acc, 'ro', markersize=8)
    ax3.annotate(f'{titles["best"]}: {best_acc:.2f}%',
                xy=(best_acc_idx + 1, best_acc),
                xytext=(10, 10), textcoords='offset points',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))
    ax3.set_title(titles['val_acc'], fontsize=14, fontweight='bold')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Accuracy (%)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. è¿‡æ‹Ÿåˆåˆ†æï¼ˆè®­ç»ƒä¸éªŒè¯çš„å·®è·ï¼‰
    acc_gap = np.array(train_accuracies) - np.array(val_accuracies)
    ax4.plot(epochs, acc_gap, 'purple', label=titles['gap_label'], linewidth=2)
    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    ax4.fill_between(epochs, acc_gap, 0, alpha=0.3, color='purple')
    ax4.set_title(titles['gap'], fontsize=14, fontweight='bold')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel(titles['gap_label'])
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å¢å¼ºç‰ˆè®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜: {save_path}")
    plt.close()


def evaluate_model_comprehensive(model, dataloader, device, class_names):
    """
    å¯¹æ¨¡å‹è¿›è¡Œç»¼åˆè¯„ä¼°ï¼Œè·å–æ‰€æœ‰éœ€è¦çš„æŒ‡æ ‡æ•°æ®

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        class_names: ç±»åˆ«åç§°åˆ—è¡¨

    Returns:
        dict: åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„å­—å…¸
    """
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    print("ğŸ” æ­£åœ¨è¿›è¡Œæ¨¡å‹è¯„ä¼°...")

    with torch.no_grad():
        for batch_idx, (images, labels, _) in enumerate(dataloader):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # è®¡ç®—åˆ†ç±»æŠ¥å‘Š
    report = classification_report(all_labels, all_preds,
                                 target_names=class_names,
                                 output_dict=True)

    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds)
    cm_normalized = confusion_matrix(all_labels, all_preds, normalize='true')

    print("âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ")

    return {
        'predictions': all_preds,
        'labels': all_labels,
        'probabilities': all_probs,
        'classification_report': report,
        'confusion_matrix': cm,
        'confusion_matrix_normalized': cm_normalized,
        'class_names': class_names
    }


def plot_confusion_matrices(evaluation_results, save_dir):
    """
    ç»˜åˆ¶æ ‡å‡†å’Œå½’ä¸€åŒ–æ··æ·†çŸ©é˜µ

    Args:
        evaluation_results (dict): æ¨¡å‹è¯„ä¼°ç»“æœ
        save_dir (str): ä¿å­˜ç›®å½•
    """
    os.makedirs(save_dir, exist_ok=True)

    cm = evaluation_results['confusion_matrix']
    cm_normalized = evaluation_results['confusion_matrix_normalized']
    class_names = evaluation_results['class_names']

    # 1. æ ‡å‡†æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
    plt.title('æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    plt.tight_layout()

    save_path_standard = os.path.join(save_dir, "confusion_matrix.png")
    plt.savefig(save_path_standard, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š æ ‡å‡†æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path_standard}")
    plt.close()

    # 2. å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'ç™¾åˆ†æ¯”'})
    plt.title('å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    plt.tight_layout()

    save_path_normalized = os.path.join(save_dir, "confusion_matrix_normalized.png")
    plt.savefig(save_path_normalized, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å½’ä¸€åŒ–æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path_normalized}")
    plt.close()


def plot_roc_curves(evaluation_results, save_path):
    """
    ç»˜åˆ¶å¤šç±»åˆ«ROCæ›²çº¿

    Args:
        evaluation_results (dict): æ¨¡å‹è¯„ä¼°ç»“æœ
        save_path (str): ä¿å­˜è·¯å¾„
    """
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    labels = evaluation_results['labels']
    probabilities = evaluation_results['probabilities']
    class_names = evaluation_results['class_names']
    n_classes = len(class_names)

    # å°†æ ‡ç­¾è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ¼å¼
    labels_binarized = label_binarize(labels, classes=range(n_classes))

    plt.figure(figsize=(10, 8))
    colors = ['blue', 'red', 'green', 'orange', 'purple']

    # ä¸ºæ¯ä¸ªç±»åˆ«ç»˜åˆ¶ROCæ›²çº¿
    for i in range(n_classes):
        fpr, tpr, _ = roc_curve(labels_binarized[:, i], probabilities[:, i])
        roc_auc = auc(fpr, tpr)

        plt.plot(fpr, tpr, color=colors[i % len(colors)], linewidth=2,
                label=f'{class_names[i]} (AUC = {roc_auc:.3f})')

    # ç»˜åˆ¶å¯¹è§’çº¿
    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å‡æ­£ç‡ (False Positive Rate)', fontsize=12)
    plt.ylabel('çœŸæ­£ç‡ (True Positive Rate)', fontsize=12)
    plt.title('ROCæ›²çº¿ - å¤šç±»åˆ«åˆ†ç±»', fontsize=16, fontweight='bold', pad=20)
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ROCæ›²çº¿å·²ä¿å­˜: {save_path}")
    plt.close()


def plot_pr_curves(evaluation_results, save_path):
    """
    ç»˜åˆ¶å¤šç±»åˆ«ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿

    Args:
        evaluation_results (dict): æ¨¡å‹è¯„ä¼°ç»“æœ
        save_path (str): ä¿å­˜è·¯å¾„
    """
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    labels = evaluation_results['labels']
    probabilities = evaluation_results['probabilities']
    class_names = evaluation_results['class_names']
    n_classes = len(class_names)

    # å°†æ ‡ç­¾è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ¼å¼
    labels_binarized = label_binarize(labels, classes=range(n_classes))

    plt.figure(figsize=(10, 8))
    colors = ['blue', 'red', 'green', 'orange', 'purple']

    # ä¸ºæ¯ä¸ªç±»åˆ«ç»˜åˆ¶PRæ›²çº¿
    for i in range(n_classes):
        precision, recall, _ = precision_recall_curve(labels_binarized[:, i], probabilities[:, i])
        avg_precision = average_precision_score(labels_binarized[:, i], probabilities[:, i])

        plt.plot(recall, precision, color=colors[i % len(colors)], linewidth=2,
                label=f'{class_names[i]} (AP = {avg_precision:.3f})')

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å¬å›ç‡ (Recall)', fontsize=12)
    plt.ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=12)
    plt.title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ - å¤šç±»åˆ«åˆ†ç±»', fontsize=16, fontweight='bold', pad=20)
    plt.legend(loc="lower left")
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š PRæ›²çº¿å·²ä¿å­˜: {save_path}")
    plt.close()


def plot_class_performance_radar(evaluation_results, save_path):
    """
    ç»˜åˆ¶ç±»åˆ«æ€§èƒ½é›·è¾¾å›¾

    Args:
        evaluation_results (dict): æ¨¡å‹è¯„ä¼°ç»“æœ
        save_path (str): ä¿å­˜è·¯å¾„
    """
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    report = evaluation_results['classification_report']
    class_names = evaluation_results['class_names']

    # æå–æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    metrics = ['precision', 'recall', 'f1-score']
    class_metrics = []

    for class_name in class_names:
        class_data = report[class_name]
        class_metrics.append([
            class_data['precision'],
            class_data['recall'],
            class_data['f1-score']
        ])

    # è®¾ç½®é›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆå›¾å½¢

    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
    colors = ['blue', 'red', 'green', 'orange', 'purple']

    # ä¸ºæ¯ä¸ªç±»åˆ«ç»˜åˆ¶é›·è¾¾å›¾
    for i, (class_name, values) in enumerate(zip(class_names, class_metrics)):
        values += values[:1]  # é—­åˆå›¾å½¢
        ax.plot(angles, values, 'o-', linewidth=2,
                label=class_name, color=colors[i % len(colors)])
        ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])

    # è®¾ç½®æ ‡ç­¾å’Œæ ¼å¼
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(['ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°'], fontsize=12)
    ax.set_ylim(0, 1)
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=10)
    ax.grid(True)

    plt.title('ç±»åˆ«æ€§èƒ½é›·è¾¾å›¾', fontsize=16, fontweight='bold', pad=30)
    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    plt.tight_layout()

    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ç±»åˆ«æ€§èƒ½é›·è¾¾å›¾å·²ä¿å­˜: {save_path}")
    plt.close()


def generate_all_plots(train_losses, val_losses, train_accuracies, val_accuracies,
                      model, dataloader, device, class_names, plot_dir, latest_plot_dir):
    """
    ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨çš„ä¸»å‡½æ•°

    Args:
        train_losses, val_losses, train_accuracies, val_accuracies: è®­ç»ƒå†å²æ•°æ®
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        class_names: ç±»åˆ«åç§°
        plot_dir: æ—¶é—´æˆ³å›¾è¡¨ç›®å½•
        latest_plot_dir: æœ€æ–°å›¾è¡¨ç›®å½•
    """
    print("ğŸ“Š å¼€å§‹ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨...")

    # æ£€æµ‹ä¸­æ–‡å­—ä½“æ”¯æŒ
    use_english = not has_chinese_font()
    if use_english:
        print("âš ï¸  æœªæ£€æµ‹åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡é¢˜")

    # 1. ç”Ÿæˆå¢å¼ºç‰ˆè®­ç»ƒå†å²å›¾è¡¨
    print("ğŸ“ˆ ç”Ÿæˆè®­ç»ƒå†å²å›¾è¡¨...")
    plot_enhanced_training_history(
        train_losses, val_losses, train_accuracies, val_accuracies,
        os.path.join(plot_dir, "training_history.png"), use_english=use_english
    )
    plot_enhanced_training_history(
        train_losses, val_losses, train_accuracies, val_accuracies,
        os.path.join(latest_plot_dir, "training_history.png"), use_english=use_english
    )

    # 2. è¿›è¡Œæ¨¡å‹è¯„ä¼°
    print("ğŸ” è¿›è¡Œç»¼åˆæ¨¡å‹è¯„ä¼°...")
    evaluation_results = evaluate_model_comprehensive(model, dataloader, device, class_names)

    # 3. ç”Ÿæˆæ··æ·†çŸ©é˜µ
    print("ğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
    plot_confusion_matrices(evaluation_results, plot_dir)
    plot_confusion_matrices(evaluation_results, latest_plot_dir)

    # 4. ç”ŸæˆROCæ›²çº¿
    print("ğŸ“ˆ ç”ŸæˆROCæ›²çº¿...")
    plot_roc_curves(evaluation_results, os.path.join(plot_dir, "roc_curves.png"))
    plot_roc_curves(evaluation_results, os.path.join(latest_plot_dir, "roc_curves.png"))

    # 5. ç”ŸæˆPRæ›²çº¿
    print("ğŸ“ˆ ç”ŸæˆPRæ›²çº¿...")
    plot_pr_curves(evaluation_results, os.path.join(plot_dir, "pr_curves.png"))
    plot_pr_curves(evaluation_results, os.path.join(latest_plot_dir, "pr_curves.png"))

    # 6. ç”Ÿæˆç±»åˆ«æ€§èƒ½é›·è¾¾å›¾
    print("ğŸ¯ ç”Ÿæˆç±»åˆ«æ€§èƒ½é›·è¾¾å›¾...")
    plot_class_performance_radar(evaluation_results, os.path.join(plot_dir, "class_performance_radar.png"))
    plot_class_performance_radar(evaluation_results, os.path.join(latest_plot_dir, "class_performance_radar.png"))

    print("âœ… æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“ æ—¶é—´æˆ³ç›®å½•: {plot_dir}")
    print(f"ğŸ“ æœ€æ–°ç›®å½•: {latest_plot_dir}")

    return evaluation_results
